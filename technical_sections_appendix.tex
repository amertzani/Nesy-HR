% This file contains the appendix with all code listings
% Include this in your main document using \input{technical_sections_appendix.tex}

\appendix
\section{Code Listings}

\subsection{Knowledge Graph Implementation}

\begin{lstlisting}[language=Python, caption=Fact Storage with Provenance, label=lst:fact-storage]
# Create RDF URIs for subject, predicate, and object
subject_uri = URIRef("urn:entity:John_Smith")
predicate_uri = URIRef("urn:predicate:has_salary")
object_literal = Literal("75000")

# Add triple to knowledge graph
graph.add((subject_uri, predicate_uri, object_literal))

# Add metadata properties
graph.add((subject_uri, SOURCE_DOCUMENT, Literal("employees.csv")))
graph.add((subject_uri, UPLOADED_AT, Literal("2024-01-15T10:30:00")))
graph.add((subject_uri, AGENT_ID, Literal("worker_001")))
graph.add((subject_uri, CONFIDENCE, Literal(0.95)))
graph.add((subject_uri, IS_INFERRED, Literal(False)))
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=RDF Triple Structure, label=lst:rdf-structure]
from rdflib import Graph, URIRef, Literal, Namespace

# Create knowledge graph
graph = Graph()

# Define namespaces
ENTITY_NS = Namespace("urn:entity:")
PREDICATE_NS = Namespace("urn:predicate:")
METADATA_NS = Namespace("urn:metadata:")

# Store a fact
subject = URIRef(ENTITY_NS + "John_Smith")
predicate = URIRef(PREDICATE_NS + "has_salary")
object_val = Literal("75000")

# Add triple
graph.add((subject, predicate, object_val))

# Add metadata
graph.add((subject, METADATA_NS + "source_document", 
           Literal("employees.csv")))
graph.add((subject, METADATA_NS + "agent_id", 
           Literal("worker_001")))
graph.add((subject, METADATA_NS + "confidence", 
           Literal(0.95)))
graph.add((subject, METADATA_NS + "uploaded_at", 
           Literal("2024-01-15T10:30:00")))
graph.add((subject, METADATA_NS + "is_inferred", 
           Literal(False)))
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Knowledge Graph Persistence, label=lst:kg-persistence]
import pickle
from datetime import datetime

def save_knowledge_graph():
    """Save knowledge graph to disk"""
    with open("knowledge_graph.pkl", "wb") as f:
        pickle.dump(graph, f)
    
    # Also create JSON backup for portability
    backup_data = {
        "timestamp": datetime.now().isoformat(),
        "total_facts": len(graph),
        "facts": []
    }
    
    for s, p, o in graph:
        # Extract fact and metadata
        fact_data = {
            "subject": str(s),
            "predicate": str(p),
            "object": str(o),
            # ... metadata extraction
        }
        backup_data["facts"].append(fact_data)
    
    with open("knowledge_backup.json", "w") as f:
        json.dump(backup_data, f, indent=2)
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Knowledge Graph Query, label=lst:kg-query]
from rdflib import Graph, URIRef, Literal

def retrieve_facts_for_entity(entity_name: str):
    """Retrieve all facts for a specific entity"""
    entity_uri = URIRef(f"urn:entity:{entity_name.replace(' ', '_')}")
    
    # Query for all triples where entity is subject
    facts = []
    for s, p, o in graph.triples((entity_uri, None, None)):
        facts.append({
            "subject": str(s),
            "predicate": str(p),
            "object": str(o),
            "source": get_source_document(s, p, o),
            "agent": get_agent_id(s, p, o),
            "confidence": get_confidence(s, p, o)
        })
    
    return facts
\end{lstlisting}

\subsection{Entity and Fact Processing}

\begin{lstlisting}[language=Python, caption=Entity Normalization Algorithm, label=lst:entity-norm]
def normalize_entity(entity: str) -> str:
    """Normalize entity name for consistent storage and lookup"""
    # Step 1: Convert to lowercase and strip whitespace
    normalized = entity.lower().strip()
    
    # Step 2: Remove special characters (keep alphanumeric and spaces)
    normalized = re.sub(r'[^\w\s]', '', normalized)
    
    # Step 3: Replace multiple spaces with single space
    normalized = re.sub(r'\s+', ' ', normalized)
    
    # Step 4: Check normalization map for known variants
    if normalized in entity_normalization_map:
        normalized = entity_normalization_map[normalized]
    
    return normalized

def learn_normalizations_from_facts():
    """Learn normalization mappings from existing facts"""
    entity_groups = defaultdict(list)
    
    # Group entities by normalized form (case-insensitive)
    for s, p, o in graph:
        s_normalized = str(s).lower().strip()
        entity_groups[s_normalized].append(str(s))
    
    # Use most common variant as canonical
    for normalized, variants in entity_groups.items():
        if len(set(variants)) > 1:
            canonical = max(set(variants), key=variants.count)
            for variant in variants:
                if variant.lower() != canonical.lower():
                    entity_normalization_map[variant.lower()] = canonical.lower()
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Fact Extraction Algorithm, label=lst:fact-extraction]
def extract_facts_from_text(text: str) -> List[Dict]:
    """Extract subject-predicate-object triples from text"""
    facts = []
    sentences = split_into_sentences(text)
    
    # Regex patterns for common relation types
    patterns = [
        (r"(\w+)\s+has\s+(\w+)\s+(\w+)", "has"),
        (r"(\w+)\s+is\s+(\w+)", "is"),
        (r"(\w+)\s+works\s+in\s+(\w+)", "works_in"),
        (r"(\w+)\s+earns\s+(\d+)", "earns"),
    ]
    
    for sentence in sentences:
        for pattern, predicate_type in patterns:
            matches = re.finditer(pattern, sentence, re.IGNORECASE)
            for match in matches:
                subject = normalize_entity(match.group(1))
                object_val = match.group(2) if len(match.groups()) >= 2 else match.group(0)
                
                # Validate entities (filter out stop words, invalid patterns)
                if is_valid_entity(subject) and is_valid_entity(object_val):
                    # Check for duplicates using in-memory index
                    if not fact_exists(subject, predicate_type, object_val):
                        facts.append({
                            "subject": subject,
                            "predicate": predicate_type,
                            "object": object_val,
                            "confidence": compute_confidence(sentence, match)
                        })
    
    return facts

def compute_confidence(sentence: str, match: re.Match) -> float:
    """Compute confidence score for extracted fact"""
    base_confidence = 0.7
    
    # Increase confidence if sentence is declarative
    if sentence.endswith('.'):
        base_confidence += 0.1
    
    # Decrease confidence if sentence contains uncertainty words
    uncertainty_words = ["maybe", "perhaps", "might", "could"]
    if any(word in sentence.lower() for word in uncertainty_words):
        base_confidence -= 0.2
    
    return min(1.0, max(0.0, base_confidence))
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=CSV Fact Extraction Algorithm, label=lst:csv-extraction]
def extract_facts_from_csv_row(row: Dict, columns: List[str]) -> List[Tuple]:
    """Extract facts from a CSV row"""
    facts = []
    employee_name = None
    
    # Identify employee name column (common patterns)
    name_columns = [col for col in columns if 'name' in col.lower()]
    if name_columns:
        employee_name = normalize_entity(row[name_columns[0]])
    
    if not employee_name:
        return facts
    
    # Extract facts for each column
    for col in columns:
        if col in name_columns:
            continue
        
        value = row[col]
        if pd.notna(value) and str(value).strip():
            # Create predicate from column name
            predicate = f"has_{col.lower().replace(' ', '_')}"
            
            # Normalize value
            object_val = str(value).strip()
            
            # Check for duplicates
            if not fact_exists(employee_name, predicate, object_val):
                facts.append((employee_name, predicate, object_val))
    
    return facts
\end{lstlisting}

\subsection{Multi-Agent System}

\begin{lstlisting}[language=Python, caption=Agent Base Class, label=lst:agent-base]
from dataclasses import dataclass
from typing import Dict, List, Optional
from datetime import datetime

@dataclass
class Agent:
    """Base agent class"""
    agent_id: str
    agent_type: str
    status: str  # "active", "processing", "idle"
    created_at: datetime
    metadata: Dict[str, any]
    
    def process(self, input_data: any) -> any:
        """Process input and return result"""
        raise NotImplementedError
    
    def update_status(self, new_status: str):
        """Update agent status"""
        self.status = new_status
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Agent Communication Pattern, label=lst:agent-comm]
class WorkerAgent(Agent):
    """Worker agent for fact extraction"""
    
    def process_chunk(self, chunk: str, document_name: str):
        """Extract facts from document chunk"""
        facts = extract_facts(chunk)  # NLP-based extraction
        
        for fact in facts:
            # Store fact in knowledge graph with metadata
            add_to_graph(
                subject=fact["subject"],
                predicate=fact["predicate"],
                object=fact["object"],
                source_document=document_name,
                agent_id=self.agent_id,
                confidence=fact["confidence"]
            )
        
        return len(facts)

class StatisticsAgent(Agent):
    """Statistics agent for correlation analysis"""
    
    def compute_correlations(self, csv_data: pd.DataFrame):
        """Compute correlations using Pearson correlation coefficient"""
        numeric_cols = csv_data.select_dtypes(include=[np.number]).columns
        
        # Compute pairwise correlations
        correlation_matrix = csv_data[numeric_cols].corr()
        
        # Extract significant correlations (|r| > 0.3)
        for i, col1 in enumerate(numeric_cols):
            for col2 in numeric_cols[i+1:]:
                corr_value = correlation_matrix.loc[col1, col2]
                
                if abs(corr_value) > 0.3:  # Threshold for significance
                    # Store correlation as fact
                    add_to_graph(
                        subject=f"correlation_{col1}_{col2}",
                        predicate="has_correlation_value",
                        object=str(round(corr_value, 3)),
                        source_document=csv_data.name,
                        agent_id=self.agent_id,
                        confidence=1.0
                    )
    
    def compute_descriptive_stats(self, csv_data: pd.DataFrame):
        """Compute descriptive statistics for numeric columns"""
        numeric_cols = csv_data.select_dtypes(include=[np.number]).columns
        
        for col in numeric_cols:
            col_data = csv_data[col].dropna()
            
            if len(col_data) > 0:
                stats = {
                    "mean": col_data.mean(),
                    "median": col_data.median(),
                    "std": col_data.std(),
                    "min": col_data.min(),
                    "max": col_data.max(),
                    "q1": col_data.quantile(0.25),
                    "q3": col_data.quantile(0.75)
                }
                
                # Store each statistic as a fact
                for stat_name, stat_value in stats.items():
                    add_to_graph(
                        subject=col,
                        predicate=f"has_{stat_name}",
                        object=str(round(stat_value, 2)),
                        source_document=csv_data.name,
                        agent_id=self.agent_id,
                        confidence=1.0
                    )
\end{lstlisting}

\subsection{Query Processing}

\begin{lstlisting}[language=Python, caption=Query Routing Algorithm, label=lst:query-routing]
def find_agents_for_query(query: str, query_type: str, 
                         attribute: Optional[str] = None) -> Dict:
    """Multi-stage query routing algorithm"""
    routing_info = {
        "query_type": query_type,
        "target_agents": [],
        "strategy": "all_agents",
        "reason": ""
    }
    
    query_lower = query.lower()
    
    # Stage 1: Employee-specific query detection
    if query_type == "filter" and attribute:
        # Extract employee name using regex patterns
        name_patterns = [
            r'([A-Z][a-z]+,\s*[A-Z][a-z]+)',  # "Smith, John"
            r'(?:of|for)\s+([A-Z][a-z]+,\s*[A-Z][a-z]+)',
        ]
        
        employee_name = None
        for pattern in name_patterns:
            match = re.search(pattern, query)
            if match:
                employee_name = match.group(1) if match.lastindex >= 1 else match.group(0)
                break
        
        if employee_name:
            # Find which agent processed this employee
            matching_agents = find_agent_for_employee(employee_name)
            if matching_agents:
                routing_info["target_agents"] = matching_agents
                routing_info["strategy"] = "specific_agents"
                routing_info["reason"] = f"Found employee in {len(matching_agents)} agent(s)"
                return routing_info
    
    # Stage 2: Statistical query detection
    statistics_keywords = [
        "correlation", "distribution", "min", "max", "minimum", "maximum",
        "average", "mean", "median", "standard deviation", "variance",
        "relationship between", "how are", "connection between"
    ]
    
    if any(keyword in query_lower for keyword in statistics_keywords):
        routing_info["strategy"] = "statistics_agent"
        routing_info["target_agents"] = ["statistics_agent"]
        routing_info["reason"] = "Query requires statistical analysis"
        return routing_info
    
    # Stage 3: Operational query detection
    operational_keywords = [
        "average", "by department", "group by", "aggregate",
        "sum", "count", "total", "per department"
    ]
    
    if any(keyword in query_lower for keyword in operational_keywords):
        routing_info["strategy"] = "operational_agent"
        routing_info["target_agents"] = ["operational_agent"]
        routing_info["reason"] = "Query requires operational insights"
        return routing_info
    
    # Stage 4: Structured query pattern matching
    structured_patterns = [
        r"what is (.+)'s (.+)",      # "What is John's salary?"
        r"who has the (highest|lowest) (.+)",  # "Who has the highest salary?"
        r"find (.+) with (.+)"       # "Find employees with salary > 50000"
    ]
    
    if any(re.search(p, query_lower) for p in structured_patterns):
        routing_info["strategy"] = "structured"
        routing_info["target_agents"] = ["query_processor"]
        routing_info["reason"] = "Structured query pattern detected"
        return routing_info
    
    # Stage 5: Default to conversational (LLM-based)
    routing_info["strategy"] = "conversational"
    routing_info["target_agents"] = ["llm_agent"]
    routing_info["reason"] = "Conversational query - using LLM"
    return routing_info

def find_agent_for_employee(employee_name: str) -> List[str]:
    """Find which document agents processed a specific employee"""
    matching_agents = []
    employee_normalized = employee_name.strip()
    
    # Normalize name parts for matching
    name_parts = [p.strip() for p in employee_normalized.split(',')]
    
    for agent_id, agent in document_agents.items():
        if hasattr(agent, 'employee_names') and agent.employee_names:
            for tracked_name in agent.employee_names:
                # Exact match
                if tracked_name.lower() == employee_normalized.lower():
                    matching_agents.append(agent_id)
                    break
                
                # Partial match (handle name variations)
                tracked_parts = [p.strip() for p in tracked_name.split(',')]
                if len(tracked_parts) >= 2 and len(name_parts) >= 2:
                    if (tracked_parts[0].lower() == name_parts[0].lower() and
                        tracked_parts[1].lower() == name_parts[1].lower()):
                        matching_agents.append(agent_id)
                        break
    
    return matching_agents
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Query Classification Algorithm, label=lst:query-classification]
def classify_query(query: str) -> Dict[str, any]:
    """Classify query type and extract parameters using pattern matching"""
    query_lower = query.lower()
    
    # Pattern 1: Filter queries ("What is X's Y?")
    filter_pattern = r"what is (.+?)(?:'s|')\s+(.+)"
    if match := re.search(filter_pattern, query, re.IGNORECASE):
        entity = match.group(1).strip()
        attribute = match.group(2).strip()
        return {
            "type": "structured",
            "operation": "filter",
            "entity": normalize_entity(entity),
            "attribute": attribute.lower().replace(' ', '_')
        }
    
    # Pattern 2: Min queries ("Who has the lowest X?")
    min_patterns = [
        r"who has the (?:lowest|minimum)\s+(.+)",
        r"which (.+) has the (?:lowest|minimum)\s+(.+)"
    ]
    for pattern in min_patterns:
        if match := re.search(pattern, query_lower):
            attribute = match.group(1) if len(match.groups()) == 1 else match.group(2)
            return {
                "type": "structured",
                "operation": "min",
                "attribute": attribute.strip().replace(' ', '_')
            }
    
    # Pattern 3: Max queries ("Who has the highest X?")
    max_patterns = [
        r"who has the (?:highest|maximum)\s+(.+)",
        r"which (.+) has the (?:highest|maximum)\s+(.+)"
    ]
    for pattern in max_patterns:
        if match := re.search(pattern, query_lower):
            attribute = match.group(1) if len(match.groups()) == 1 else match.group(2)
            return {
                "type": "structured",
                "operation": "max",
                "attribute": attribute.strip().replace(' ', '_')
            }
    
    # Pattern 4: Operational queries (groupby/aggregation)
    operational_patterns = [
        r"(?:average|mean|avg)\s+(.+?)\s+by\s+(.+)",
        r"(?:sum|total|count)\s+of\s+(.+?)\s+by\s+(.+)"
    ]
    for pattern in operational_patterns:
        if match := re.search(pattern, query_lower):
            metric = match.group(1).strip()
            group_by = match.group(2).strip()
            return {
                "type": "operational",
                "operation": "groupby",
                "metric": metric,
                "group_by": group_by
            }
    
    # Pattern 5: Statistical queries
    if any(kw in query_lower for kw in ["correlation", "relationship between"]):
        # Extract two entities/columns
        correlation_pattern = r"(?:correlation|relationship)\s+(?:between|of)\s+(.+?)\s+(?:and|&)\s+(.+)"
        if match := re.search(correlation_pattern, query_lower):
            return {
                "type": "statistical",
                "operation": "correlation",
                "column1": match.group(1).strip(),
                "column2": match.group(2).strip()
            }
    
    # Default: conversational query
    return {
        "type": "conversational",
        "operation": "llm_response",
        "query": query
    }
\end{lstlisting}

\begin{lstlisting}[language=Python, caption=Evidence Assembly Algorithm, label=lst:evidence-assembly]
def assemble_evidence(facts: List[Dict], query: str) -> str:
    """Assemble and rank evidence facts for response"""
    if not facts:
        return ""
    
    # Step 1: Rank facts by relevance to query
    ranked_facts = rank_facts_by_relevance(facts, query)
    
    # Step 2: Select top-k most relevant facts (limit to 5 for readability)
    top_facts = ranked_facts[:5]
    
    # Step 3: Format evidence display
    evidence_lines = ["**Evidence from Knowledge Graph:**"]
    
    for i, fact in enumerate(top_facts, 1):
        subj = fact["subject"]
        pred = fact["predicate"]
        obj = fact["object"]
        source = fact.get("source", "unknown")
        
        # Format triple display
        evidence_line = f"{i}. {subj} → {pred} → {obj}"
        
        # Add source attribution
        if source:
            if isinstance(source, list) and len(source) > 0:
                # Handle multiple sources
                sources_str = ", ".join([s[0] if isinstance(s, tuple) else str(s) 
                                       for s in source[:2]])  # Limit to 2 sources
                evidence_line += f" [Source: {sources_str}]"
            else:
                evidence_line += f" [Source: {source}]"
        
        evidence_lines.append(evidence_line)
    
    return "\n".join(evidence_lines)

def rank_facts_by_relevance(facts: List[Dict], query: str) -> List[Dict]:
    """Rank facts by relevance to query using keyword matching"""
    query_lower = query.lower()
    query_words = set(query_lower.split())
    
    scored_facts = []
    for fact in facts:
        score = 0.0
        
        # Check subject relevance
        subject_lower = str(fact["subject"]).lower()
        subject_words = set(subject_lower.split())
        score += len(query_words.intersection(subject_words)) * 2.0
        
        # Check predicate relevance
        predicate_lower = str(fact["predicate"]).lower()
        if any(word in predicate_lower for word in query_words):
            score += 1.5
        
        # Check object relevance
        object_lower = str(fact["object"]).lower()
        object_words = set(object_lower.split())
        score += len(query_words.intersection(object_words)) * 1.0
        
        # Boost score for exact matches
        if any(word in subject_lower for word in query_words if len(word) > 3):
            score += 1.0
        
        scored_facts.append((score, fact))
    
    # Sort by score (descending) and return facts
    scored_facts.sort(key=lambda x: x[0], reverse=True)
    return [fact for _, fact in scored_facts]
\end{lstlisting}

\subsection{Parallel Processing}

\begin{lstlisting}[language=Python, caption=Parallel Processing Algorithm, label=lst:parallel-processing]
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

class DocumentAgent(Agent):
    """Document agent for coordinating processing"""
    
    def process_document(self, document_path: str):
        """Process document with parallel workers"""
        # Step 1: Split document into chunks
        chunks = self._split_into_chunks(document_path, chunk_size=1000)
        
        # Step 2: Create worker agents (one per chunk)
        workers = [WorkerAgent(f"worker_{self.agent_id}_{i}") 
                  for i in range(len(chunks))]
        
        # Step 3: Process chunks in parallel (max 4 concurrent workers)
        total_facts = 0
        with ThreadPoolExecutor(max_workers=4) as executor:
            # Submit all tasks
            future_to_worker = {
                executor.submit(worker.process_chunk, chunk, document_path): worker
                for worker, chunk in zip(workers, chunks)
            }
            
            # Collect results as they complete
            for future in as_completed(future_to_worker):
                worker = future_to_worker[future]
                try:
                    facts_count = future.result()
                    total_facts += facts_count
                except Exception as e:
                    print(f"Worker {worker.agent_id} failed: {e}")
        
        return total_facts
    
    def _split_into_chunks(self, document_path: str, chunk_size: int = 1000):
        """Split document into processing chunks"""
        chunks = []
        
        if document_path.endswith('.csv'):
            # For CSV: split by rows
            df = pd.read_csv(document_path)
            for i in range(0, len(df), chunk_size):
                chunks.append(df.iloc[i:i+chunk_size])
        else:
            # For text files: split by characters
            with open(document_path, 'r') as f:
                text = f.read()
                for i in range(0, len(text), chunk_size):
                    chunks.append(text[i:i+chunk_size])
        
        return chunks
\end{lstlisting}

\subsection{Frontend Implementation}

\begin{lstlisting}[language=TypeScript, caption=API Client, label=lst:api-client]
import axios from 'axios';

const API_BASE_URL = 'http://localhost:8002/api';

export const apiClient = {
  async sendChatMessage(message: string, history: any[]) {
    const response = await axios.post(
      `${API_BASE_URL}/chat`,
      { message, history }
    );
    return response.data;
  },
  
  async getFacts(filters?: any) {
    const response = await axios.get(
      `${API_BASE_URL}/knowledge/facts`,
      { params: filters }
    );
    return response.data;
  },
  
  async uploadDocument(file: File) {
    const formData = new FormData();
    formData.append('file', file);
    const response = await axios.post(
      `${API_BASE_URL}/knowledge/upload`,
      formData,
      { headers: { 'Content-Type': 'multipart/form-data' } }
    );
    return response.data;
  }
};
\end{lstlisting}

\begin{lstlisting}[language=TypeScript, caption=State Management, label=lst:state-management]
import { QueryClient, QueryClientProvider } from '@tanstack/react-query';
import { createContext, useContext } from 'react';

// Server state (React Query)
const queryClient = new QueryClient({
  defaultOptions: {
    queries: {
      staleTime: 5 * 60 * 1000, // 5 minutes
      cacheTime: 10 * 60 * 1000, // 10 minutes
    },
  },
});

// Local state (Context API)
interface KnowledgeStore {
  facts: Fact[];
  nodes: GraphNode[];
  edges: GraphEdge[];
  refreshFacts: () => Promise<void>;
}

const KnowledgeStoreContext = createContext<KnowledgeStore | null>(null);

export function useKnowledgeStore() {
  const context = useContext(KnowledgeStoreContext);
  if (!context) {
    throw new Error('useKnowledgeStore must be used within provider');
  }
  return context;
}
\end{lstlisting}

\subsection{LLM Integration}

\begin{lstlisting}[language=Python, caption=LLM Prompt Template, label=lst:llm-prompt]
SYSTEM_PROMPT = """You are an intelligent data analyst assistant that helps users understand their data and make evidence-based decisions.

CRITICAL RULES:
1. ALWAYS use EXACT numbers from the context - NEVER estimate, guess, or make up numbers
2. Always provide TRACEABLE answers - cite the specific facts from the knowledge graph that support your answer
3. For structured queries (max/min/filter): provide the exact answer first, then explain the evidence
4. Focus on INSIGHTS and ACTIONABLE INFORMATION, not raw data dumps
5. Synthesize information to provide clear, concise answers
6. Always ground your answers in the provided context facts - if a number isn't in the context, don't use it
7. Include evidence facts in your response format:
   Evidence from Knowledge Graph:
   1. [Subject] → [Predicate] → [Object] [Source: document.csv]
"""

def generate_response(query: str, context_facts: List[Dict]) -> str:
    """Generate response with evidence"""
    context = format_facts_for_llm(context_facts)
    prompt = f"{SYSTEM_PROMPT}\n\nKnowledge Base Context:\n{context}\n\nUser Query: {query}\n\nResponse:"
    
    # Call LLM (Ollama, Groq, or OpenAI)
    response = call_llm(prompt)
    
    # Ensure evidence is included
    if "Evidence from Knowledge Graph" not in response:
        evidence = assemble_evidence(context_facts, query)
        response += f"\n\n{evidence}"
    
    return response
\end{lstlisting}

\subsection{Performance Optimizations}

\begin{lstlisting}[language=Python, caption=In-Memory Fact Index Algorithm, label=lst:fact-index]
# Global in-memory index: set of normalized (subject, predicate, object) tuples
_fact_lookup_set = set()
_fact_index_initialized = False

def fact_exists(subject: str, predicate: str, object_val: str) -> bool:
    """Fast O(1) fact existence check using normalized tuple lookup"""
    # Normalize all components
    normalized = (
        normalize_entity(subject.lower()),
        predicate.lower().strip(),
        normalize_entity(object_val.lower())
    )
    
    # O(1) set membership test
    return normalized in _fact_lookup_set

def add_fact_to_index(subject: str, predicate: str, object_val: str):
    """Add fact to in-memory index (called when fact is added to graph)"""
    normalized = (
        normalize_entity(subject.lower()),
        predicate.lower().strip(),
        normalize_entity(object_val.lower())
    )
    _fact_lookup_set.add(normalized)

def initialize_fact_index():
    """Initialize index from existing knowledge graph (on startup)"""
    global _fact_index_initialized, _fact_lookup_set
    
    if _fact_index_initialized:
        return
    
    # Load all facts from graph into index
    for s, p, o in graph:
        # Skip metadata triples
        if 'source_document' in str(p) or 'agent_id' in str(p):
            continue
        
        subject_str = str(s).split(':')[-1] if ':' in str(s) else str(s)
        predicate_str = str(p).split(':')[-1] if ':' in str(p) else str(p)
        object_str = str(o)
        
        add_fact_to_index(subject_str, predicate_str, object_str)
    
    _fact_index_initialized = True
    print(f"Initialized fact index with {len(_fact_lookup_set)} facts")
\end{lstlisting}

\subsection{Operational Insights}

\begin{lstlisting}[language=Python, caption=Operational Insights Generation Algorithm, label=lst:operational-insights]
def compute_operational_insights(csv_data: pd.DataFrame) -> Dict:
    """Generate operational insights through groupby operations"""
    insights = {}
    
    # Insight 1: Department performance metrics
    if 'Department' in csv_data.columns:
        dept_metrics = csv_data.groupby('Department').agg({
            'PerformanceRating': ['mean', 'count'],
            'Salary': 'mean',
            'Absences': 'mean'
        }).round(2)
        
        # Store each metric as a fact
        for dept, row in dept_metrics.iterrows():
            add_to_graph(
                subject=dept,
                predicate="has_avg_performance",
                object=str(row[('PerformanceRating', 'mean')]),
                source_document=csv_data.name,
                agent_id="operational_agent",
                confidence=1.0
            )
    
    # Insight 2: Manager-based aggregations
    if 'ManagerName' in csv_data.columns:
        manager_stats = csv_data.groupby('ManagerName').agg({
            'EmployeeCount': 'count',
            'AvgSalary': 'mean',
            'AvgPerformance': 'mean'
        })
        
        for manager, stats in manager_stats.iterrows():
            add_to_graph(
                subject=manager,
                predicate="manages_employees",
                object=str(int(stats['EmployeeCount'])),
                source_document=csv_data.name,
                agent_id="operational_agent",
                confidence=1.0
            )
    
    # Insight 3: Recruitment source effectiveness
    if 'RecruitmentSource' in csv_data.columns:
        source_effectiveness = csv_data.groupby('RecruitmentSource').agg({
            'PerformanceRating': 'mean',
            'Tenure': 'mean',
            'EmployeeCount': 'count'
        })
        
        # Rank sources by performance
        top_source = source_effectiveness['PerformanceRating'].idxmax()
        add_to_graph(
            subject="recruitment_analysis",
            predicate="has_best_source",
            object=top_source,
            source_document=csv_data.name,
            agent_id="operational_agent",
            confidence=1.0
        )
    
    return insights
\end{lstlisting}

\subsection{Entity Matching}

\begin{lstlisting}[language=Python, caption=Entity Matching Algorithm, label=lst:entity-matching]
def extract_employee_facts_from_agents(agent_ids: List[str]) -> List[Dict]:
    """Extract employee facts from specific agents efficiently"""
    employees = {}
    
    # Get source documents for target agents
    agent_documents = {}
    for agent_id in agent_ids:
        if agent_id in document_agents:
            agent = document_agents[agent_id]
            doc_name = agent.document_name
            if doc_name:
                agent_documents[doc_name] = agent_id
    
    # Extract facts from graph, filtering by source document
    for s, p, o in graph:
        # Skip metadata triples
        if any(meta in str(p) for meta in ['source_document', 'agent_id', 
                                           'confidence', 'is_inferred']):
            continue
        
        # Check if fact belongs to target agents
        sources = get_fact_source_document(s, p, o)
        fact_belongs_to_agent = any(
            source_doc in agent_documents 
            for source_doc, _ in sources
        )
        
        if not fact_belongs_to_agent:
            continue
        
        # Extract employee name from subject or object
        employee_name = extract_employee_name(s, p, o)
        if employee_name:
            if employee_name not in employees:
                employees[employee_name] = {
                    "name": employee_name,
                    "attributes": {},
                    "facts": []
                }
            
            # Store attribute
            attr_name = str(p).split(':')[-1].lower()
            employees[employee_name]["attributes"][attr_name] = str(o)
            
            # Store fact with source
            employees[employee_name]["facts"].append({
                "subject": str(s),
                "predicate": str(p),
                "object": str(o),
                "source": sources
            })
    
    return list(employees.values())

def extract_employee_name(s, p, o) -> Optional[str]:
    """Extract employee name from triple using pattern matching"""
    # Pattern: "LastName, FirstName"
    name_pattern = r'^[A-Z][a-z]+,\s*[A-Z][a-z]+'
    
    subject_str = str(s).split(':')[-1] if ':' in str(s) else str(s)
    object_str = str(o)
    
    # Check subject
    if re.match(name_pattern, subject_str):
        return subject_str
    
    # Check object
    if re.match(name_pattern, object_str):
        return object_str
    
    # Try to extract from subject string
    match = re.search(name_pattern, subject_str)
    if match:
        return match.group(0)
    
    return None
\end{lstlisting}

