==================================================================================
EVALUATION METRICS SUMMARY - VALUES FOR GRAPHING
==================================================================================

This document contains computed metrics from the offline evaluation report.
All values are presented as Mean ± 95% Confidence Interval.

Total Queries: 32 (as per ALL_TESTED_QUERIES.md)

METRICS BY GROUP
----------------------------------------------------------------------------------

Group: EVIDENCE
  Sample size (n): 7

  Fact Retrieval Accuracy (Canonical Scoring):
    F1 Score:           0.857 ± 0.397
    Precision:          0.857 ± 0.397
    Recall:             0.857 ± 0.397

  Traceability & Reliability:
    Traceability Completeness: 1.000 ± 0.000
    Hallucination Resistance:  1.000 ± 0.000

  Performance:
    Response Latency (seconds): 0.001 ± 0.004
    Accuracy:                  1.000 ± 0.000

----------------------------------------------------------------------------------

Group: OPERATIONAL
  Sample size (n): 22

  Fact Retrieval Accuracy (Canonical Scoring):
    F1 Score:           0.857 ± 0.219
    Precision:          0.857 ± 0.219
    Recall:             0.857 ± 0.219

  Traceability & Reliability:
    Traceability Completeness: 0.636 ± 0.237
    Hallucination Resistance:  0.545 ± 0.246

  Performance:
    Response Latency (seconds): 0.025 ± 0.037
    Accuracy:                  0.455 ± 0.246

----------------------------------------------------------------------------------

Group: STRATEGIC
  Sample size (n): 3

  Traceability & Reliability:
    Traceability Completeness: 0.000 ± 0.000
    Hallucination Resistance:  0.333 ± 1.060

  Performance:
    Response Latency (seconds): 0.000 ± 0.000
    Accuracy:                  0.333 ± 1.060

----------------------------------------------------------------------------------

GRAPH DATA (for plotting)
----------------------------------------------------------------------------------

The following data is formatted for direct use in plotting libraries
(matplotlib, seaborn, plotly, etc.):

Metric                    | operational | strategic | evidence
--------------------------------------------------------------
F1                       | 0.857 | 0.000 | 0.857
Precision                | 0.857 | 0.000 | 0.857
Recall                   | 0.857 | 0.000 | 0.857
Traceability Completeness | 0.636 | 0.000 | 1.000
Hallucination Resistance | 0.545 | 0.333 | 1.000
Latency                  | 0.025 | 0.000 | 0.001
Accuracy                 | 0.455 | 0.333 | 1.000

Error bars (95% CI):
  Use 'ci_lowers' and 'ci_uppers' arrays from graph_data in JSON

==================================================================================
NOTES
==================================================================================

1. Fact Retrieval Metrics:
   - Precision: |R_q ∩ G_q| / |R_q|
   - Recall: |R_q ∩ G_q| / |G_q|
   - F1: 2 * (Precision * Recall) / (Precision + Recall)
   - Uses canonical scoring (recommended)

2. Traceability Completeness:
   - T_q / D_q (fact-level)
   - T_q = facts shown in evidence
   - D_q = required facts (estimated from ground truth)

3. Hallucination Resistance:
   - 1 - (Hallucinated Claims / Total Claims)
   - Estimated from response correctness

4. Reliability:
   - Requires multiple runs of same query
   - Not computed in this evaluation (single run per query)

5. NASA-TLX:
   - Requires user study data
   - Not available in offline evaluation
