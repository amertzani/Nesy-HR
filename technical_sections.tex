\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,calc}

% For appendix
\usepackage{appendix}

\geometry{margin=1in}

% Code listing style
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    frame=single,
    breaklines=true,
    showstringspaces=false,
    tabsize=2
}

% Custom colors
\definecolor{frontendcolor}{RGB}{66, 139, 202}
\definecolor{backendcolor}{RGB}{92, 184, 92}
\definecolor{storagecolor}{RGB}{240, 173, 78}

\title{System Architecture and Technical Implementation\\
\large{A Human-Centered Multi-Agent Knowledge Graph System for Transparent and Ethical HR Decision Support}}
\author{}
\date{}

\begin{document}

\maketitle

\section{System Architecture Overview}

Our system implements a three-layer architecture designed to ensure complete transparency, ethical responsibility, and effective information retrieval for HR decision support. The architecture separates concerns across user interface, multi-agent orchestration, and knowledge graph storage layers, enabling modular design while maintaining end-to-end traceability.

\subsection{Three-Layer Architecture}

The system is organized into three distinct layers, each serving specific purposes while maintaining seamless integration:

\textbf{Layer 1: User Interface Layer (Frontend)}
The frontend is built using React 18 with TypeScript, providing a modern, responsive user interface specifically designed for HR professionals. The interface consists of nine main pages:

\begin{itemize}
    \item \textbf{Chat Interface}: Natural language conversational interface for querying the knowledge base
    \item \textbf{Knowledge Base}: Table view displaying all extracted facts with filtering capabilities (by source, agent, confidence, inferred/original status)
    \item \textbf{Knowledge Graph Visualization}: Interactive force-directed graph showing entity relationships
    \item \textbf{Statistics Dashboard}: Correlation matrices, distribution charts, and descriptive statistics
    \item \textbf{Insights Page}: Operational insights including department performance, absence patterns, and recruitment analysis
    \item \textbf{Agent Architecture View}: Visual representation of the multi-agent system and agent status
    \item \textbf{Upload Page}: Document upload interface supporting CSV, PDF, DOCX, and TXT formats
    \item \textbf{Documents Page}: Document management with metadata and processing status
    \item \textbf{Import/Export Page}: Data import and export functionality
\end{itemize}

The frontend communicates with the backend through a RESTful API, ensuring clean separation of concerns and enabling future scalability.

\textbf{Layer 2: Multi-Agent Orchestration Layer (Backend)}
The backend is implemented using Python 3.9+ with FastAPI, providing a high-performance REST API server. The core of the backend is the multi-agent orchestration system, which coordinates eight specialized agents:

\begin{enumerate}
    \item \textbf{Knowledge Graph Agent}: Central storage manager that maintains the RDF knowledge graph and handles all fact storage and retrieval operations
    \item \textbf{Document Agents}: Created dynamically for each uploaded document, these agents coordinate document processing and track metadata (columns, rows, employee names, upload timestamps)
    \item \textbf{Worker Agents}: Process document chunks in parallel, extracting facts using NLP-based techniques (optional Triplex LLM or regex-based extraction)
    \item \textbf{Statistics Agent}: Computes correlations between numeric columns, calculates descriptive statistics (mean, median, min, max, standard deviation, quartiles), and analyzes value distributions
    \item \textbf{Operational Agent}: Performs groupby operations and generates operational insights such as department performance metrics and absence patterns
    \item \textbf{LLM Agent}: Generates natural language responses using facts retrieved from the knowledge graph, ensuring all responses include evidence facts and source attribution
    \item \textbf{Orchestrator Agent}: Routes queries to appropriate agents based on query type analysis, managing query processing strategy
    \item \textbf{Visualization Agent}: Creates charts and graphs for UI display, including correlation heatmaps and distribution visualizations
\end{enumerate}

All agents communicate through the centralized knowledge graph, ensuring consistent data access and maintaining complete provenance tracking.

\textbf{Layer 3: Knowledge Graph Storage Layer}
The knowledge graph is implemented using RDFLib, providing RDF (Resource Description Framework) triple storage. Each fact is stored as a Subject-Predicate-Object triple with additional metadata properties:

\begin{itemize}
    \item \textbf{Source Document}: Name of the document from which the fact was extracted
    \item \textbf{Upload Timestamp}: When the document was uploaded and processed
    \item \textbf{Agent ID}: Identifier of the agent that extracted the fact
    \item \textbf{Confidence Score}: Quality indicator for the extracted fact (0.0 to 1.0)
    \item \textbf{Inferred Status}: Whether the fact was directly extracted or inferred from other facts
\end{itemize}

The knowledge graph is persisted using pickle format (\texttt{knowledge\_graph.pkl}) for efficient serialization, with JSON backup (\texttt{knowledge\_backup.json}) for recovery and data portability.

\subsection{Design Principles}

The architecture is guided by four core design principles:

\textbf{Transparency}: Every fact stored in the knowledge graph is traceable to its source document. Query responses include evidence facts with complete source attribution, enabling users to verify system reasoning.

\textbf{Modularity}: The multi-agent architecture enables specialized processing for different tasks. Each agent has a well-defined responsibility, making the system maintainable and extensible.

\textbf{Scalability}: Parallel processing capabilities allow the system to handle large documents efficiently. Worker agents process chunks concurrently, and statistics and operational analysis run simultaneously with fact extraction.

\textbf{Human-Centricity}: The entire system is designed with HR professionals in mind. The interface is intuitive, responses are conversational, and all system reasoning is visible to users.

\section{Key Features}

Our system provides five key features that distinguish it from traditional information retrieval systems, particularly in terms of transparency and human-centered design.

\subsection{Complete Traceability}

Unlike black-box AI systems, our approach ensures complete traceability from source documents to query responses. This is achieved through:

\textbf{Source Document Attribution}: Every fact stored in the knowledge graph maintains a reference to its source document. When a query is processed, the system retrieves not only the relevant facts but also their source information, which is displayed to users.

\textbf{Agent Ownership Tracking}: Each fact tracks which agent extracted it, providing visibility into the processing pipeline. This enables users to understand how different parts of the system contributed to a response.

\textbf{Evidence Display}: Query responses include a dedicated evidence section that lists all supporting facts from the knowledge graph. Each evidence fact shows the subject-predicate-object triple along with source document attribution.

\textbf{Provenance Chains}: The system maintains complete provenance chains, enabling users to trace any response back through the knowledge graph to the original source documents. This is visualized in the graph interface, where users can explore entity relationships and their origins.

\subsection{Multi-Agent Specialization}

The system employs eight specialized agents, each optimized for specific tasks:

\textbf{Specialized Processing}: Rather than using a single monolithic system, specialized agents handle different aspects of information processing. The Statistics Agent focuses on correlation and distribution analysis, while the Operational Agent specializes in groupby operations and aggregations.

\textbf{Intelligent Routing}: The Orchestrator Agent analyzes incoming queries and routes them to the most appropriate agent(s). This ensures efficient processing and leverages each agent's specialized capabilities.

\textbf{Parallel Execution}: Multiple agents can operate simultaneously. For example, while Worker Agents extract facts from a document, the Statistics Agent computes correlations and the Operational Agent generates insights, all in parallel.

\textbf{Centralized Communication}: All agents communicate through the knowledge graph, ensuring data consistency and enabling agents to build upon each other's work.

\subsection{Human-Centered Interface}

The user interface is designed specifically for HR professionals, prioritizing usability and transparency:

\textbf{Natural Language Queries}: Users can interact with the system using natural language, without needing to learn query syntax or understand the underlying knowledge graph structure.

\textbf{Visual Knowledge Exploration}: The interactive graph visualization allows users to explore entity relationships visually, making complex data structures accessible.

\textbf{Evidence Visibility}: Every response prominently displays supporting evidence, making system reasoning transparent. Users can click on source documents to verify information.

\textbf{Intuitive Navigation}: The interface provides nine specialized pages, each serving a specific purpose. Clear navigation and visual feedback guide users through the system.

\textbf{Real-Time Feedback}: The system provides loading states, progress indicators, and clear error messages, ensuring users always understand system status.

\subsection{Effective Query Processing}

The system supports four distinct query types, each processed through specialized pathways:

\textbf{Structured Queries}: Direct fact retrieval queries such as "What is John Smith's salary?" are processed by querying the knowledge graph directly, providing fast and accurate responses.

\textbf{Operational Queries}: Groupby and aggregation queries like "Average salary by department?" are routed to the Operational Agent, which generates insights from pre-computed operational facts.

\textbf{Statistical Queries}: Correlation and distribution questions are handled by the Statistics Agent, which provides statistical analysis with supporting evidence.

\textbf{Conversational Queries}: Natural language questions are processed by the LLM Agent, which retrieves relevant facts from the knowledge graph and generates coherent responses with evidence.

The Orchestrator Agent intelligently routes queries to appropriate agents based on query analysis, ensuring optimal processing for each query type.

\subsection{Real-Time Processing}

The system is designed for efficiency and responsiveness:

\textbf{Parallel Document Processing}: When a document is uploaded, Worker Agents process chunks in parallel, significantly reducing processing time for large documents.

\textbf{Simultaneous Analysis}: Statistics and operational analysis run concurrently with fact extraction, ensuring that insights are available as soon as processing completes.

\textbf{Fast Query Response}: Query processing is optimized through intelligent routing and efficient knowledge graph queries, with average response times under 5 seconds.

\section{System Workflow}

The system operates through two main workflows: document processing and query processing. Both workflows are designed to maintain complete traceability and transparency.

\subsection{Document Processing Pipeline}

When a user uploads a document (CSV, PDF, DOCX, or TXT), the system follows a five-step processing pipeline:

\textbf{Step 1: Document Upload and Storage}
The uploaded file is stored on the server, and metadata is extracted including file name, size, upload timestamp, and file type. The system validates the file format and prepares it for processing.

\textbf{Step 2: Document Agent Creation}
A Document Agent is dynamically created for the uploaded document. This agent tracks document-specific metadata such as:
\begin{itemize}
    \item Column names and types (for CSV files)
    \item Number of rows
    \item Employee names identified in the document
    \item Upload timestamp
    \item Processing status
\end{itemize}

The Document Agent coordinates all processing activities for its document.

\textbf{Step 3: Parallel Processing}
The system initiates three parallel processing streams:

\begin{itemize}
    \item \textbf{Worker Agents}: The document is divided into chunks, and Worker Agents are created to process each chunk in parallel. Each Worker Agent extracts facts using NLP-based techniques (optional Triplex LLM model or regex-based extraction). Facts are extracted as subject-predicate-object triples.
    
    \item \textbf{Statistics Agent}: Simultaneously, the Statistics Agent analyzes numeric columns in the document, computing correlations between pairs of columns, calculating descriptive statistics (mean, median, min, max, standard deviation, quartiles), and analyzing value distributions for both categorical and numeric columns.
    
    \item \textbf{Operational Agent}: The Operational Agent performs groupby operations on the data, generating operational insights such as average metrics by department, absence patterns, and recruitment source effectiveness.
\end{itemize}

\textbf{Step 4: Knowledge Graph Storage}
All extracted facts, statistical insights, and operational insights are stored in the centralized knowledge graph. Each fact is stored as an RDF triple with metadata including source document, upload timestamp, agent ID, confidence score, and inference status. The implementation details are shown in Listing~\ref{lst:fact-storage} in Appendix~A. This ensures that every fact maintains complete provenance information.

\textbf{Step 5: Visualization Generation}
The Visualization Agent creates charts and graphs based on the extracted data, including correlation heatmaps, distribution charts, and operational insight visualizations. These are displayed in the Statistics and Insights pages of the user interface.

\subsection{Query Processing Pipeline}

When a user submits a query through the chat interface, the system follows a six-step processing pipeline:

\textbf{Step 1: User Query Input}
The user types a natural language query in the chat interface. The query is sent to the backend via REST API.

\textbf{Step 2: Orchestrator Analysis}
The Orchestrator Agent analyzes the query to determine its type:

\begin{itemize}
    \item \textbf{Structured Queries}: Contain specific patterns like "What is X's Y?" or "Who has the highest Z?" These are identified through pattern matching.
    
    \item \textbf{Operational Queries}: Contain keywords like "average", "by department", "group by", indicating aggregation needs.
    
    \item \textbf{Statistical Queries}: Contain keywords like "correlation", "distribution", "relationship between", indicating statistical analysis needs.
    
    \item \textbf{Conversational Queries}: Natural language questions that don't match specific patterns, requiring LLM-based processing.
\end{itemize}

\textbf{Step 3: Agent Routing}
Based on the query analysis, the Orchestrator routes the query to appropriate agent(s):

\begin{itemize}
    \item Structured queries → Query Processor (direct knowledge graph queries)
    \item Operational queries → Operational Agent (pre-computed insights)
    \item Statistical queries → Statistics Agent (correlation/distribution analysis)
    \item Conversational queries → LLM Agent (natural language generation)
\end{itemize}

The Orchestrator may route to multiple agents if a query requires combined processing.

\textbf{Step 4: Knowledge Graph Retrieval}
The selected agent(s) query the knowledge graph for relevant facts. The query process:

\begin{enumerate}
    \item Identifies relevant entities mentioned in the query
    \item Retrieves facts involving those entities
    \item Filters by source document if the query is document-specific
    \item Retrieves evidence facts with complete provenance information
    \item Assembles context for response generation
\end{enumerate}

For example, a query about "John Smith's salary" would:
\begin{enumerate}
    \item Identify entity "John Smith"
    \item Query for triples where subject is "John Smith" and predicate contains "salary"
    \item Retrieve the fact: (John Smith, has\_salary, 75000)
    \item Retrieve metadata: source document, agent ID, confidence score
\end{enumerate}

\textbf{Step 5: Response Generation}
The LLM Agent generates a natural language response using the retrieved facts. The response generation process:

\begin{enumerate}
    \item Assembles context from retrieved facts
    \item Formats facts for LLM input
    \item Generates response using LLM (Ollama, Groq API, or OpenAI API)
    \item Ensures response includes evidence facts
    \item Formats response for user-friendly display
\end{enumerate}

The LLM is prompted to:
\begin{itemize}
    \item Use exact values from the knowledge graph (no estimation)
    \item Cite specific facts that support the answer
    \item Provide source document attribution
    \item Explain the reasoning when appropriate
\end{itemize}

\textbf{Step 6: Response Display}
The response is displayed in the chat interface with:

\begin{itemize}
    \item \textbf{Direct Answer}: The main response to the user's query
    \item \textbf{Evidence Section}: List of supporting facts from the knowledge graph, each showing:
    \begin{itemize}
        \item Subject → Predicate → Object triple
        \item Source document name
        \item Upload timestamp (if relevant)
    \end{itemize}
    \item \textbf{Routing Information}: Which agent(s) processed the query (for transparency)
    \item \textbf{Source Links}: Clickable links to view source documents or explore facts in the knowledge base
\end{itemize}

Users can click on evidence facts to view them in the Knowledge Base page, or click on source documents to see all facts from that document.

\section{Implementation}\label{sec:implementation}

This section presents the core algorithmic components and theoretical foundations underlying the system's implementation, focusing on the computational mechanisms that enable transparency and traceability.

\subsection{Knowledge Graph Storage and Provenance}

The knowledge graph uses RDFLib's in-memory \texttt{Graph} structure, where facts are stored as RDF triples with \texttt{URIRef} objects for subjects/predicates and \texttt{Literal} objects for values. The theoretical choice of RDF as the representation formalism provides standardised semantics while enabling explicit provenance tracking through reification.

Each fact is assigned a unique identifier computed as \texttt{fact\_id = f"{subject}|{predicate}|{normalized\_object}"}, which is URI-encoded and stored separately. Provenance metadata (source document, timestamp, agent ID, confidence) is attached through additional RDF triples linking fact IDs to metadata values, as shown in Listing~\ref{lst:fact-storage}. This design enables multiple sources per fact and maintains complete auditability.

The graph persistence uses dual-format storage: binary \texttt{pickle} serialisation for efficient loading and JSON backup for portability. The fact ID computation normalises object values to ensure consistent identification across extraction runs, preventing duplicate facts from different processing stages.

\subsection{Entity Normalisation and Consistency}

Entity normalisation addresses the representational consistency problem: semantically identical entities with different surface forms (e.g., ``John Smith'' vs. ``john  smith'') must map to a canonical representation. The algorithm applies sequential transformations: lowercasing, special character removal, and space normalisation, followed by lookup in a learned mapping dictionary.

The system implements an unsupervised learning mechanism that groups entities by normalised form and selects the most frequent variant as canonical. This learned mapping is persisted and updated incrementally, enabling the system to improve entity consistency over time without manual intervention. The algorithm, shown in Listing~\ref{lst:entity-norm}, ensures that entity queries retrieve all relevant facts regardless of surface form variations.

\subsection{Fact Extraction and Duplicate Prevention}

Fact extraction uses a hybrid approach: regex pattern matching for structured relational phrases combined with optional LLM-based extraction for complex sentences. For CSV data, each row is processed as a structured record where column names become predicates and values become objects, creating facts of the form \texttt{(employee\_name, has\_\{column\}, value)}.

Duplicate prevention is critical for parallel processing correctness. The system maintains an in-memory hash set \texttt{\_fact\_lookup\_set} containing normalised fact tuples, enabling O(1) existence checks before graph insertion. This index is initialised at startup (O(n) complexity) and maintained incrementally, ensuring that concurrent worker threads can safely check for duplicates without graph traversal. The theoretical guarantee is that duplicate facts are prevented even under high concurrency, as the set membership test is atomic.

\subsection{Parallel Processing and Concurrency}

Parallel document processing uses Python's \texttt{ThreadPoolExecutor} with an adaptive chunking strategy. The chunk size calculation \texttt{chunk\_size = max(25, min(200, 15000 // c))} balances load distribution against thread overhead, targeting approximately 15,000 data points per chunk. This adaptive approach ensures efficient parallelisation regardless of dataset dimensions.

The theoretical foundation is that RDFLib's graph operations are thread-safe, enabling concurrent writes without explicit synchronisation. Worker threads use \texttt{threading.Lock()} only for updating shared counters and agent registries, not for graph access. The parallel processing implementation, shown in Listing~\ref{lst:parallel-processing}, demonstrates how the system achieves linear speedup for large documents while maintaining data consistency.

\subsection{Query Routing and Classification}

Query routing uses a hierarchical pattern-matching algorithm that processes queries through priority-ordered stages: employee-specific detection, statistical keyword matching, operational query detection, structured pattern matching, and conversational fallback. The theoretical motivation is efficiency: employee-specific queries can be answered by querying only relevant document agents, reducing graph traversal overhead.

The routing algorithm, detailed in Listing~\ref{lst:query-routing}, extracts query parameters (entities, attributes, operations) using regex patterns and keyword lists. It returns routing decisions with explicit reasoning, maintaining transparency about why each query was routed to specific agents. This design enables users to understand the system's decision-making process.

\subsection{Evidence Assembly}

Evidence assembly ranks facts by relevance to the query using a scoring function that considers keyword overlap, entity name matches, and predicate alignment. The top-\(k\) facts (typically \(k=5\)) are selected and formatted with source attribution. The algorithm, shown in Listing~\ref{lst:evidence-assembly}, ensures that every system response includes inspectable evidence, enabling users to verify reasoning.

The theoretical foundation is that explainability requires not just evidence presence, but evidence relevance. By ranking facts by semantic overlap with the query, the system presents the most informative supporting evidence first, reducing cognitive load on users while maintaining complete traceability.

\subsection{Statistical Analysis}

Statistical analysis pre-computes correlations and descriptive statistics during document ingestion, storing results as facts in the knowledge graph. This design choice enables statistical queries to be answered through graph lookups rather than recomputation, providing fast response times while maintaining consistency with the symbolic knowledge layer.

Correlations are computed using Pearson's coefficient with a configurable threshold (\(|\rho| > 0.3\)), and descriptive statistics (mean, median, std, quartiles) are computed column-wise. Each statistic is encoded as a separate fact, enabling the system to answer queries like ``What is the average salary?'' through direct graph retrieval rather than statistical computation.

\section{Conclusion}

This technical implementation provides a complete, transparent, and effective system for HR decision support. The three-layer architecture ensures separation of concerns while maintaining end-to-end traceability. The multi-agent system enables specialized processing for different query types, and the knowledge graph provides a robust foundation for fact storage and retrieval with complete provenance tracking.

The algorithmic components work together to provide:
\begin{itemize}
    \item \textbf{Efficient Fact Extraction}: Hybrid regex and LLM-based extraction with entity normalization
    \item \textbf{Intelligent Query Routing}: Multi-stage algorithm combining pattern matching and keyword detection
    \item \textbf{Fast Fact Lookup}: O(1) in-memory indexing for duplicate prevention
    \item \textbf{Relevant Evidence Assembly}: Ranking algorithm for selecting most relevant facts
    \item \textbf{Parallel Processing}: Thread-based parallelization for scalable document processing
    \item \textbf{Statistical Analysis}: Correlation and descriptive statistics computation
    \item \textbf{Operational Insights}: Groupby-based aggregation for HR metrics
\end{itemize}

The system's key strengths are:
\begin{itemize}
    \item \textbf{Complete Transparency}: Every fact is traceable to its source through provenance tracking
    \item \textbf{Modular Design}: Specialized agents enable efficient processing
    \item \textbf{Human-Centered Interface}: Intuitive design for HR professionals
    \item \textbf{Effective Retrieval}: High accuracy and coverage for HR queries
    \item \textbf{Scalable Architecture}: Parallel processing handles large datasets
\end{itemize}

The implementation is reproducible, with all algorithms clearly documented and using standard technologies. The system demonstrates how generative AI can be designed for transparency, ethical responsibility, and effective information retrieval in HR contexts.

\input{technical_sections_appendix}

\end{document}

