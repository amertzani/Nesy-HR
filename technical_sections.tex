\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{geometry}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{tikz}
\usetikzlibrary{shapes,arrows,positioning,calc}

% For appendix
\usepackage{appendix}

\geometry{margin=1in}

% Code listing style
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    frame=single,
    breaklines=true,
    showstringspaces=false,
    tabsize=2
}

% Custom colors
\definecolor{frontendcolor}{RGB}{66, 139, 202}
\definecolor{backendcolor}{RGB}{92, 184, 92}
\definecolor{storagecolor}{RGB}{240, 173, 78}

\title{System Architecture and Technical Implementation\\
\large{A Human-Centered Multi-Agent Knowledge Graph System for Transparent and Ethical HR Decision Support}}
\author{}
\date{}

\begin{document}

\maketitle

\section{System Architecture Overview}

Our system implements a three-layer architecture designed to ensure complete transparency, ethical responsibility, and effective information retrieval for HR decision support. The architecture separates concerns across user interface, multi-agent orchestration, and knowledge graph storage layers, enabling modular design while maintaining end-to-end traceability.

\subsection{Three-Layer Architecture}

The system is organized into three distinct layers, each serving specific purposes while maintaining seamless integration:

\textbf{Layer 1: User Interface Layer (Frontend)}
The frontend is built using React 18 with TypeScript, providing a modern, responsive user interface specifically designed for HR professionals. The interface consists of nine main pages:

\begin{itemize}
    \item \textbf{Chat Interface}: Natural language conversational interface for querying the knowledge base
    \item \textbf{Knowledge Base}: Table view displaying all extracted facts with filtering capabilities (by source, agent, confidence, inferred/original status)
    \item \textbf{Knowledge Graph Visualization}: Interactive force-directed graph showing entity relationships
    \item \textbf{Statistics Dashboard}: Correlation matrices, distribution charts, and descriptive statistics
    \item \textbf{Insights Page}: Operational insights including department performance, absence patterns, and recruitment analysis
    \item \textbf{Agent Architecture View}: Visual representation of the multi-agent system and agent status
    \item \textbf{Upload Page}: Document upload interface supporting CSV, PDF, DOCX, and TXT formats
    \item \textbf{Documents Page}: Document management with metadata and processing status
    \item \textbf{Import/Export Page}: Data import and export functionality
\end{itemize}

The frontend communicates with the backend through a RESTful API, ensuring clean separation of concerns and enabling future scalability.

\textbf{Layer 2: Multi-Agent Orchestration Layer (Backend)}
The backend is implemented using Python 3.9+ with FastAPI, providing a high-performance REST API server. The core of the backend is the multi-agent orchestration system, which coordinates eight specialized agents:

\begin{enumerate}
    \item \textbf{Knowledge Graph Agent}: Central storage manager that maintains the RDF knowledge graph and handles all fact storage and retrieval operations
    \item \textbf{Document Agents}: Created dynamically for each uploaded document, these agents coordinate document processing and track metadata (columns, rows, employee names, upload timestamps)
    \item \textbf{Worker Agents}: Process document chunks in parallel, extracting facts using NLP-based techniques (optional Triplex LLM or regex-based extraction)
    \item \textbf{Statistics Agent}: Computes correlations between numeric columns, calculates descriptive statistics (mean, median, min, max, standard deviation, quartiles), and analyzes value distributions
    \item \textbf{Operational Agent}: Performs groupby operations and generates operational insights such as department performance metrics and absence patterns
    \item \textbf{LLM Agent}: Generates natural language responses using facts retrieved from the knowledge graph, ensuring all responses include evidence facts and source attribution
    \item \textbf{Orchestrator Agent}: Routes queries to appropriate agents based on query type analysis, managing query processing strategy
    \item \textbf{Visualization Agent}: Creates charts and graphs for UI display, including correlation heatmaps and distribution visualizations
\end{enumerate}

All agents communicate through the centralized knowledge graph, ensuring consistent data access and maintaining complete provenance tracking.

\textbf{Layer 3: Knowledge Graph Storage Layer}
The knowledge graph is implemented using RDFLib, providing RDF (Resource Description Framework) triple storage. Each fact is stored as a Subject-Predicate-Object triple with additional metadata properties:

\begin{itemize}
    \item \textbf{Source Document}: Name of the document from which the fact was extracted
    \item \textbf{Upload Timestamp}: When the document was uploaded and processed
    \item \textbf{Agent ID}: Identifier of the agent that extracted the fact
    \item \textbf{Confidence Score}: Quality indicator for the extracted fact (0.0 to 1.0)
    \item \textbf{Inferred Status}: Whether the fact was directly extracted or inferred from other facts
\end{itemize}

The knowledge graph is persisted using pickle format (\texttt{knowledge\_graph.pkl}) for efficient serialization, with JSON backup (\texttt{knowledge\_backup.json}) for recovery and data portability.

\subsection{Design Principles}

The architecture is guided by four core design principles:

\textbf{Transparency}: Every fact stored in the knowledge graph is traceable to its source document. Query responses include evidence facts with complete source attribution, enabling users to verify system reasoning.

\textbf{Modularity}: The multi-agent architecture enables specialized processing for different tasks. Each agent has a well-defined responsibility, making the system maintainable and extensible.

\textbf{Scalability}: Parallel processing capabilities allow the system to handle large documents efficiently. Worker agents process chunks concurrently, and statistics and operational analysis run simultaneously with fact extraction.

\textbf{Human-Centricity}: The entire system is designed with HR professionals in mind. The interface is intuitive, responses are conversational, and all system reasoning is visible to users.

\section{Key Features}

Our system provides five key features that distinguish it from traditional information retrieval systems, particularly in terms of transparency and human-centered design.

\subsection{Complete Traceability}

Unlike black-box AI systems, our approach ensures complete traceability from source documents to query responses. This is achieved through:

\textbf{Source Document Attribution}: Every fact stored in the knowledge graph maintains a reference to its source document. When a query is processed, the system retrieves not only the relevant facts but also their source information, which is displayed to users.

\textbf{Agent Ownership Tracking}: Each fact tracks which agent extracted it, providing visibility into the processing pipeline. This enables users to understand how different parts of the system contributed to a response.

\textbf{Evidence Display}: Query responses include a dedicated evidence section that lists all supporting facts from the knowledge graph. Each evidence fact shows the subject-predicate-object triple along with source document attribution.

\textbf{Provenance Chains}: The system maintains complete provenance chains, enabling users to trace any response back through the knowledge graph to the original source documents. This is visualized in the graph interface, where users can explore entity relationships and their origins.

\subsection{Multi-Agent Specialization}

The system employs eight specialized agents, each optimized for specific tasks:

\textbf{Specialized Processing}: Rather than using a single monolithic system, specialized agents handle different aspects of information processing. The Statistics Agent focuses on correlation and distribution analysis, while the Operational Agent specializes in groupby operations and aggregations.

\textbf{Intelligent Routing}: The Orchestrator Agent analyzes incoming queries and routes them to the most appropriate agent(s). This ensures efficient processing and leverages each agent's specialized capabilities.

\textbf{Parallel Execution}: Multiple agents can operate simultaneously. For example, while Worker Agents extract facts from a document, the Statistics Agent computes correlations and the Operational Agent generates insights, all in parallel.

\textbf{Centralized Communication}: All agents communicate through the knowledge graph, ensuring data consistency and enabling agents to build upon each other's work.

\subsection{Human-Centered Interface}

The user interface is designed specifically for HR professionals, prioritizing usability and transparency:

\textbf{Natural Language Queries}: Users can interact with the system using natural language, without needing to learn query syntax or understand the underlying knowledge graph structure.

\textbf{Visual Knowledge Exploration}: The interactive graph visualization allows users to explore entity relationships visually, making complex data structures accessible.

\textbf{Evidence Visibility}: Every response prominently displays supporting evidence, making system reasoning transparent. Users can click on source documents to verify information.

\textbf{Intuitive Navigation}: The interface provides nine specialized pages, each serving a specific purpose. Clear navigation and visual feedback guide users through the system.

\textbf{Real-Time Feedback}: The system provides loading states, progress indicators, and clear error messages, ensuring users always understand system status.

\subsection{Effective Query Processing}

The system supports four distinct query types, each processed through specialized pathways:

\textbf{Structured Queries}: Direct fact retrieval queries such as "What is John Smith's salary?" are processed by querying the knowledge graph directly, providing fast and accurate responses.

\textbf{Operational Queries}: Groupby and aggregation queries like "Average salary by department?" are routed to the Operational Agent, which generates insights from pre-computed operational facts.

\textbf{Statistical Queries}: Correlation and distribution questions are handled by the Statistics Agent, which provides statistical analysis with supporting evidence.

\textbf{Conversational Queries}: Natural language questions are processed by the LLM Agent, which retrieves relevant facts from the knowledge graph and generates coherent responses with evidence.

The Orchestrator Agent intelligently routes queries to appropriate agents based on query analysis, ensuring optimal processing for each query type.

\subsection{Real-Time Processing}

The system is designed for efficiency and responsiveness:

\textbf{Parallel Document Processing}: When a document is uploaded, Worker Agents process chunks in parallel, significantly reducing processing time for large documents.

\textbf{Simultaneous Analysis}: Statistics and operational analysis run concurrently with fact extraction, ensuring that insights are available as soon as processing completes.

\textbf{Fast Query Response}: Query processing is optimized through intelligent routing and efficient knowledge graph queries, with average response times under 5 seconds.

\section{System Workflow}

The system operates through two main workflows: document processing and query processing. Both workflows are designed to maintain complete traceability and transparency.

\subsection{Document Processing Pipeline}

When a user uploads a document (CSV, PDF, DOCX, or TXT), the system follows a five-step processing pipeline:

\textbf{Step 1: Document Upload and Storage}
The uploaded file is stored on the server, and metadata is extracted including file name, size, upload timestamp, and file type. The system validates the file format and prepares it for processing.

\textbf{Step 2: Document Agent Creation}
A Document Agent is dynamically created for the uploaded document. This agent tracks document-specific metadata such as:
\begin{itemize}
    \item Column names and types (for CSV files)
    \item Number of rows
    \item Employee names identified in the document
    \item Upload timestamp
    \item Processing status
\end{itemize}

The Document Agent coordinates all processing activities for its document.

\textbf{Step 3: Parallel Processing}
The system initiates three parallel processing streams:

\begin{itemize}
    \item \textbf{Worker Agents}: The document is divided into chunks, and Worker Agents are created to process each chunk in parallel. Each Worker Agent extracts facts using NLP-based techniques (optional Triplex LLM model or regex-based extraction). Facts are extracted as subject-predicate-object triples.
    
    \item \textbf{Statistics Agent}: Simultaneously, the Statistics Agent analyzes numeric columns in the document, computing correlations between pairs of columns, calculating descriptive statistics (mean, median, min, max, standard deviation, quartiles), and analyzing value distributions for both categorical and numeric columns.
    
    \item \textbf{Operational Agent}: The Operational Agent performs groupby operations on the data, generating operational insights such as average metrics by department, absence patterns, and recruitment source effectiveness.
\end{itemize}

\textbf{Step 4: Knowledge Graph Storage}
All extracted facts, statistical insights, and operational insights are stored in the centralized knowledge graph. Each fact is stored as an RDF triple with metadata including source document, upload timestamp, agent ID, confidence score, and inference status. The implementation details are shown in Listing~\ref{lst:fact-storage} in Appendix~A. This ensures that every fact maintains complete provenance information.

\textbf{Step 5: Visualization Generation}
The Visualization Agent creates charts and graphs based on the extracted data, including correlation heatmaps, distribution charts, and operational insight visualizations. These are displayed in the Statistics and Insights pages of the user interface.

\subsection{Query Processing Pipeline}

When a user submits a query through the chat interface, the system follows a six-step processing pipeline:

\textbf{Step 1: User Query Input}
The user types a natural language query in the chat interface. The query is sent to the backend via REST API.

\textbf{Step 2: Orchestrator Analysis}
The Orchestrator Agent analyzes the query to determine its type:

\begin{itemize}
    \item \textbf{Structured Queries}: Contain specific patterns like "What is X's Y?" or "Who has the highest Z?" These are identified through pattern matching.
    
    \item \textbf{Operational Queries}: Contain keywords like "average", "by department", "group by", indicating aggregation needs.
    
    \item \textbf{Statistical Queries}: Contain keywords like "correlation", "distribution", "relationship between", indicating statistical analysis needs.
    
    \item \textbf{Conversational Queries}: Natural language questions that don't match specific patterns, requiring LLM-based processing.
\end{itemize}

\textbf{Step 3: Agent Routing}
Based on the query analysis, the Orchestrator routes the query to appropriate agent(s):

\begin{itemize}
    \item Structured queries → Query Processor (direct knowledge graph queries)
    \item Operational queries → Operational Agent (pre-computed insights)
    \item Statistical queries → Statistics Agent (correlation/distribution analysis)
    \item Conversational queries → LLM Agent (natural language generation)
\end{itemize}

The Orchestrator may route to multiple agents if a query requires combined processing.

\textbf{Step 4: Knowledge Graph Retrieval}
The selected agent(s) query the knowledge graph for relevant facts. The query process:

\begin{enumerate}
    \item Identifies relevant entities mentioned in the query
    \item Retrieves facts involving those entities
    \item Filters by source document if the query is document-specific
    \item Retrieves evidence facts with complete provenance information
    \item Assembles context for response generation
\end{enumerate}

For example, a query about "John Smith's salary" would:
\begin{enumerate}
    \item Identify entity "John Smith"
    \item Query for triples where subject is "John Smith" and predicate contains "salary"
    \item Retrieve the fact: (John Smith, has\_salary, 75000)
    \item Retrieve metadata: source document, agent ID, confidence score
\end{enumerate}

\textbf{Step 5: Response Generation}
The LLM Agent generates a natural language response using the retrieved facts. The response generation process:

\begin{enumerate}
    \item Assembles context from retrieved facts
    \item Formats facts for LLM input
    \item Generates response using LLM (Ollama, Groq API, or OpenAI API)
    \item Ensures response includes evidence facts
    \item Formats response for user-friendly display
\end{enumerate}

The LLM is prompted to:
\begin{itemize}
    \item Use exact values from the knowledge graph (no estimation)
    \item Cite specific facts that support the answer
    \item Provide source document attribution
    \item Explain the reasoning when appropriate
\end{itemize}

\textbf{Step 6: Response Display}
The response is displayed in the chat interface with:

\begin{itemize}
    \item \textbf{Direct Answer}: The main response to the user's query
    \item \textbf{Evidence Section}: List of supporting facts from the knowledge graph, each showing:
    \begin{itemize}
        \item Subject → Predicate → Object triple
        \item Source document name
        \item Upload timestamp (if relevant)
    \end{itemize}
    \item \textbf{Routing Information}: Which agent(s) processed the query (for transparency)
    \item \textbf{Source Links}: Clickable links to view source documents or explore facts in the knowledge base
\end{itemize}

Users can click on evidence facts to view them in the Knowledge Base page, or click on source documents to see all facts from that document.

\section{Technical Implementation}

This section provides detailed technical implementation information for reproducibility and understanding of the system's inner workings.

\subsection{Knowledge Graph Implementation}

\textbf{Technology Stack}
The knowledge graph is implemented using RDFLib (version 6.0+), a Python library for working with RDF. RDFLib provides:
\begin{itemize}
    \item RDF triple storage and retrieval
    \item SPARQL-like query capabilities
    \item Multiple serialization formats (RDF/XML, Turtle, JSON-LD)
    \item Efficient in-memory graph representation
\end{itemize}

\textbf{Data Structure}
Facts are stored as RDF triples using URIRefs for subjects and predicates, and Literals for objects. The complete structure with namespace definitions and metadata properties is shown in Listing~\ref{lst:rdf-structure} in Appendix~A.

\textbf{Persistence Mechanism}
The knowledge graph is persisted using Python's pickle module for efficient binary serialization, with JSON backup for portability. The implementation is detailed in Listing~\ref{lst:kg-persistence} in Appendix~A.

\textbf{Query Mechanism}
Facts are retrieved using RDFLib's query capabilities, specifically the \texttt{triples()} method for efficient pattern matching. The query implementation with source and metadata retrieval is shown in Listing~\ref{lst:kg-query} in Appendix~A.

\textbf{Entity Normalization Algorithm}
To ensure consistent entity representation, the system normalizes entity names using a multi-step algorithm that includes lowercase conversion, special character removal, space normalization, and learning from existing facts. The complete algorithm with normalization map learning is detailed in Listing~\ref{lst:entity-norm} in Appendix~A. The normalization algorithm ensures that entities like "John Smith", "john smith", and "John  Smith" are all mapped to the same canonical form, enabling consistent fact retrieval and preventing duplicate entities in the knowledge graph.

\textbf{Fact Extraction Algorithm}
The system extracts facts from text using a hybrid approach combining regex pattern matching with optional LLM-based extraction. The algorithm processes sentences, matches patterns, validates entities, and computes confidence scores. The complete implementation is shown in Listing~\ref{lst:fact-extraction} in Appendix~A. For CSV files, the system uses a specialized extraction algorithm that processes each row as a structured data record, as detailed in Listing~\ref{lst:csv-extraction} in Appendix~A.

\subsection{Multi-Agent System Implementation}

\textbf{Agent Architecture Pattern}
The system uses an agent-based architecture with a centralized knowledge graph. Each agent is implemented as a Python dataclass with agent ID, type, status, and metadata. The base class structure is shown in Listing~\ref{lst:agent-base} in Appendix~A.

\textbf{Agent Communication}
Agents communicate through the centralized knowledge graph rather than direct inter-agent communication. Worker agents extract facts and store them in the knowledge graph, while the Statistics Agent computes correlations and descriptive statistics. The complete communication pattern with correlation computation is detailed in Listing~\ref{lst:agent-comm} in Appendix~A.

\textbf{Orchestrator Routing Algorithm}
The Orchestrator Agent implements intelligent query routing using a multi-stage algorithm that combines pattern matching, keyword detection, and entity extraction. The algorithm processes queries through five stages: employee-specific detection, statistical query detection, operational query detection, structured pattern matching, and conversational fallback. The complete routing implementation is shown in Listing~\ref{lst:query-routing} in Appendix~A. The routing algorithm uses a priority-based approach: employee-specific queries are handled first (for efficiency), followed by statistical, operational, and structured queries, with conversational queries as the default fallback.

\textbf{Parallel Processing Algorithm}
Worker agents process document chunks in parallel using a thread pool executor. The algorithm splits documents into chunks, creates worker agents, and processes chunks concurrently with error handling. The complete implementation is shown in Listing~\ref{lst:parallel-processing} in Appendix~A. The parallel processing algorithm uses a thread pool to process multiple chunks simultaneously, significantly reducing processing time for large documents. The algorithm ensures thread-safe access to the knowledge graph through RDFLib's built-in thread safety mechanisms.

\subsection{Query Processing Implementation}

\textbf{Query Classification Algorithm}
The system classifies queries using a hierarchical pattern matching algorithm that extracts both query type and parameters. The algorithm processes queries through multiple pattern matching stages for filter, min/max, operational, statistical, and conversational queries. The complete implementation is detailed in Listing~\ref{lst:query-classification} in Appendix~A. The classification algorithm uses regex patterns with capture groups to extract both the query type and specific parameters (entities, attributes, columns), enabling precise query processing.

\textbf{Evidence Assembly Algorithm}
The system assembles evidence facts for responses using a relevance-based ranking algorithm that scores facts based on keyword matching with the query. The algorithm selects the top-k most relevant facts and formats them with source attribution. The complete implementation with ranking logic is shown in Listing~\ref{lst:evidence-assembly} in Appendix~A. The evidence assembly algorithm ensures that the most relevant facts are displayed first, making it easier for users to verify system reasoning.

\subsection{Frontend Implementation}

\textbf{Technology Stack}
The frontend is built using:
\begin{itemize}
    \item \textbf{React 18}: Modern React with hooks and functional components
    \item \textbf{TypeScript}: Type-safe JavaScript for better code quality
    \item \textbf{Vite}: Fast build tool and development server
    \item \textbf{shadcn/ui}: High-quality React component library
    \item \textbf{React Query}: Server state management and caching
    \item \textbf{D3.js}: Graph visualization library
    \item \textbf{React Flow}: Agent network visualization
\end{itemize}

\textbf{API Client}
The frontend communicates with the backend through a centralized API client that handles chat messages, fact retrieval, and document uploads. The implementation is shown in Listing~\ref{lst:api-client} in Appendix~A.

\textbf{State Management}
The frontend uses React Query for server state management with caching, and Context API for local state. The implementation with query client configuration and knowledge store context is detailed in Listing~\ref{lst:state-management} in Appendix~A.

\subsection{LLM Integration}

\textbf{LLM Options}
The system supports multiple LLM backends:

\begin{itemize}
    \item \textbf{Ollama}: Local LLM execution (recommended for privacy)
    \item \textbf{Groq API}: Fast cloud-based LLM (free tier available)
    \item \textbf{OpenAI API}: Commercial option (requires API key)
\end{itemize}

\textbf{Prompt Engineering}
The system uses carefully crafted prompts to ensure traceability, emphasizing exact number usage, evidence citation, and source attribution. The complete prompt template and response generation logic is shown in Listing~\ref{lst:llm-prompt} in Appendix~A.

\subsection{Performance Optimizations}

\textbf{In-Memory Indexing Algorithm}
For fast fact lookup, the system maintains an in-memory hash-based index that provides O(1) lookup time. The index stores normalized fact tuples and is initialized on startup. The complete implementation with initialization logic is detailed in Listing~\ref{lst:fact-index} in Appendix~A. The indexing algorithm provides constant-time O(1) fact existence checks, which is critical for preventing duplicate facts during parallel processing. The index is initialized on system startup and maintained incrementally as new facts are added.

\textbf{Caching}
The system implements caching for frequently accessed data:

\begin{itemize}
    \item Query results are cached for 5 minutes
    \item Statistics and operational insights are pre-computed and cached
    \item Graph visualization data is cached until new facts are added
\end{itemize}

\subsection{Operational Insights Generation}

The Operational Agent generates insights using pandas groupby operations to compute aggregations such as department performance metrics, manager-based statistics, and recruitment source effectiveness. These insights are stored as facts in the knowledge graph for efficient querying. The complete algorithm is detailed in Listing~\ref{lst:operational-insights} in Appendix~A.

\subsection{Entity Matching}

For employee-specific queries, the system uses an entity matching algorithm that efficiently filters facts by source document, enabling fast retrieval of employee information without querying the entire knowledge graph. The algorithm extracts employee names using pattern matching and filters facts by agent ownership. The implementation is shown in Listing~\ref{lst:entity-matching} in Appendix~A.

\section{Conclusion}

This technical implementation provides a complete, transparent, and effective system for HR decision support. The three-layer architecture ensures separation of concerns while maintaining end-to-end traceability. The multi-agent system enables specialized processing for different query types, and the knowledge graph provides a robust foundation for fact storage and retrieval with complete provenance tracking.

The algorithmic components work together to provide:
\begin{itemize}
    \item \textbf{Efficient Fact Extraction}: Hybrid regex and LLM-based extraction with entity normalization
    \item \textbf{Intelligent Query Routing}: Multi-stage algorithm combining pattern matching and keyword detection
    \item \textbf{Fast Fact Lookup}: O(1) in-memory indexing for duplicate prevention
    \item \textbf{Relevant Evidence Assembly}: Ranking algorithm for selecting most relevant facts
    \item \textbf{Parallel Processing}: Thread-based parallelization for scalable document processing
    \item \textbf{Statistical Analysis}: Correlation and descriptive statistics computation
    \item \textbf{Operational Insights}: Groupby-based aggregation for HR metrics
\end{itemize}

The system's key strengths are:
\begin{itemize}
    \item \textbf{Complete Transparency}: Every fact is traceable to its source through provenance tracking
    \item \textbf{Modular Design}: Specialized agents enable efficient processing
    \item \textbf{Human-Centered Interface}: Intuitive design for HR professionals
    \item \textbf{Effective Retrieval}: High accuracy and coverage for HR queries
    \item \textbf{Scalable Architecture}: Parallel processing handles large datasets
\end{itemize}

The implementation is reproducible, with all algorithms clearly documented and using standard technologies. The system demonstrates how generative AI can be designed for transparency, ethical responsibility, and effective information retrieval in HR contexts.

\input{technical_sections_appendix}

\end{document}

