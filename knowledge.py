"""
Knowledge Graph Management Module
==================================

This module is the CORE of the knowledge extraction and management system.
It handles:
- RDF knowledge graph storage and retrieval
- Knowledge extraction from text (NLP-based with optional Triplex LLM)
- Fact management (add, delete, check duplicates)
- Graph visualization data generation
- Entity normalization and cleaning

CRITICAL FOR KNOWLEDGE EXTRACTION IMPROVEMENTS:
The main function to improve is: add_to_graph(text) - Line ~1100
This function extracts subject-predicate-object triples from raw text.

Triplex Integration (Optional LLM-based extraction):
- Enable by setting environment variable: USE_TRIPLEX=true
- Requires: transformers, torch, accelerate (see requirements.txt)
- Uses SciPhi/Triplex model (4B parameters) for high-quality extraction
- Falls back to regex-based extraction if Triplex is unavailable or disabled
- Model is loaded lazily on first use (may take time on first extraction)

Data Storage:
- knowledge_graph.pkl: Pickled RDFLib Graph object (binary)
- knowledge_backup.json: JSON backup of all facts
- entity_normalization.json: Entity normalization mappings

Author: Research Brain Team
Last Updated: 2025-01-15
"""

import os
import json
import pickle
from datetime import datetime
from typing import Optional
import rdflib
import re
import networkx as nx
from collections import defaultdict

# Optional: Try to import transformers for Triplex model
try:
    from transformers import AutoModelForCausalLM, AutoTokenizer
    import torch
    TRIPLEX_AVAILABLE = True
    TRIPLEX_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
except ImportError:
    TRIPLEX_AVAILABLE = False
    TRIPLEX_DEVICE = "cpu"
    print("‚ö†Ô∏è  Transformers/Torch not available. Triplex extraction disabled. Using regex-based extraction.")

# spaCy is disabled due to Windows compilation issues
# Using improved regex-based extraction instead
SPACY_AVAILABLE = False
SPACY_NLP = None

# ============================================================================
# CONFIGURATION
# ============================================================================

# Storage file paths
KNOWLEDGE_FILE = "knowledge_graph.pkl"  # Main persistent storage (RDF graph)
BACKUP_FILE = "knowledge_backup.json"   # JSON backup for recovery
NORMALIZATION_FILE = "entity_normalization.json"  # Entity normalization mappings

# Triplex model configuration
USE_TRIPLEX = os.getenv("USE_TRIPLEX", "false").lower() == "true"  # Enable via environment variable
TRIPLEX_MODEL_NAME = "sciphi/triplex"
TRIPLEX_MODEL = None
TRIPLEX_TOKENIZER = None

# spaCy NER configuration (disabled - using improved regex instead)
USE_SPACY = False

# ============================================================================
# GLOBAL STATE
# ============================================================================

# Global RDF graph - in-memory representation of all knowledge
# Structure: RDF triples (subject, predicate, object)
graph = rdflib.Graph()

# Mapping of fact IDs to triples for editing operations
fact_index = {}

# In-memory set-based index for fast fact existence checks
# Key: (normalized_subject, normalized_predicate, normalized_object) -> True
_fact_lookup_set = set()
_fact_index_initialized = False

def save_knowledge_graph():
    try:
        with open(KNOWLEDGE_FILE, 'wb') as f:
            pickle.dump(graph, f)
        backup_data = {
            "timestamp": datetime.now().isoformat(),
            "total_facts": len(graph),
            "facts": []
        }
        for i, (s, p, o) in enumerate(graph):
            backup_data["facts"].append({
                "id": i+1,
                "subject": str(s),
                "predicate": str(p),
                "object": str(o)
            })
        with open(BACKUP_FILE, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, indent=2, ensure_ascii=False)
        return f" Saved {len(graph)} facts to storage"
    except Exception as e:
        return f" Error saving knowledge: {e}"

def load_knowledge_graph():
    global graph, _fact_index_initialized, _fact_lookup_set
    _fact_index_initialized = False  # Reset index so it rebuilds after load
    # Load normalization map on startup
    load_normalization_map()
    try:
        if os.path.exists(KNOWLEDGE_FILE):
            with open(KNOWLEDGE_FILE, 'rb') as f:
                loaded_graph = pickle.load(f)
            
            original_count = len(loaded_graph)
            
            # Clean up any invalid URIs (with spaces) that might cause issues
            # This fixes old facts that were saved with spaces in URIs
            # Only clean if there are actually invalid URIs - don't remove valid facts!
            cleaned_graph = rdflib.Graph()
            invalid_count = 0
            
            # Use loaded_graph, not the global graph (which might be empty)
            for s, p, o in loaded_graph:
                s_str = str(s)
                p_str = str(p)
                
                # Check if URI has spaces (invalid) - look for "urn:" followed by space
                # Valid URIs shouldn't have spaces after "urn:"
                has_invalid_uri = False
                try:
                    # Check if subject has spaces after urn:
                    if 'urn:' in s_str:
                        subject_part = s_str.split('urn:')[-1]
                        if ' ' in subject_part:
                            has_invalid_uri = True
                    # Check if predicate has spaces after urn:
                    if 'urn:' in p_str:
                        predicate_part = p_str.split('urn:')[-1]
                        if ' ' in predicate_part:
                            has_invalid_uri = True
                except:
                    # If we can't parse, assume valid
                    pass
                
                if has_invalid_uri:
                    # Invalid URI - recreate with proper encoding
                    invalid_count += 1
                    from urllib.parse import quote
                    s_clean = s_str.split(':')[-1] if ':' in s_str else s_str
                    p_clean = p_str.split(':')[-1] if ':' in p_str else p_str
                    s_clean = s_clean.strip().replace(' ', '_')
                    p_clean = p_clean.strip().replace(' ', '_')
                    s_new = rdflib.URIRef(f"urn:{quote(s_clean, safe='')}")
                    p_new = rdflib.URIRef(f"urn:{quote(p_clean, safe='')}")
                    cleaned_graph.add((s_new, p_new, o))
                else:
                    # Valid URI - keep as is
                    cleaned_graph.add((s, p, o))
            
            # Verify cleaned_graph has facts
            cleaned_count = len(cleaned_graph)
            
            if cleaned_count == 0 and original_count > 0:
                print(f"‚ö†Ô∏è  CRITICAL ERROR: cleaned_graph is empty but original had {original_count} facts!")
                print("‚ö†Ô∏è  Using original graph without cleaning")
                # Clear the global graph and add all facts from loaded_graph
                # RDFLib Graph doesn't have clear(), so remove all triples
                graph.remove((None, None, None))
                for triple in loaded_graph:
                    graph.add(triple)
            else:
                # Always use cleaned_graph (it has all facts, cleaned or not)
                # Clear the global graph and add all facts from cleaned_graph
                # This ensures we update the actual graph object, not replace the reference
                # RDFLib Graph doesn't have clear(), so remove all triples
                graph.remove((None, None, None))
                for triple in cleaned_graph:
                    graph.add(triple)
            
            # Only update if we actually fixed something
            if invalid_count > 0:
                print(f"‚ö†Ô∏è  Fixed {invalid_count} facts with invalid URIs")
                save_knowledge_graph()  # Save the cleaned version
            else:
                # No invalid URIs found - graph already has all facts
                print(f"‚úÖ All {original_count} facts have valid URIs")
            
            final_count = len(graph)
            
            if final_count != original_count:
                print(f"‚ö†Ô∏è  ERROR: Graph count changed from {original_count} to {final_count}!")
                # This shouldn't happen - restore original
                print("‚ö†Ô∏è  Restoring original graph...")
                # Clear current graph and restore from loaded
                graph.remove((None, None, None))
                for triple in loaded_graph:
                    graph.add(triple)
                return f"üìÇ Loaded {len(graph)} facts from storage (restored original)"
            
            # Verify the graph actually has facts
            if len(graph) == 0 and original_count > 0:
                print(f"‚ö†Ô∏è  CRITICAL ERROR: Graph is empty after loading {original_count} facts!")
                # Restore from file - use remove/add pattern instead of assignment
                graph.remove((None, None, None))
                for triple in loaded_graph:
                    graph.add(triple)
                print(f"‚úÖ Restored {len(graph)} facts from loaded_graph")
                return f"üìÇ Loaded {len(graph)} facts from storage (restored after empty graph error)"
            
            return f"üìÇ Loaded {len(graph)} facts from storage"
        else:
            return "üìÇ No existing knowledge file found, starting fresh"
    except Exception as e:
        return f" Error loading knowledge: {e}"

def create_comprehensive_backup():
    try:
        backup_data = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "total_facts": len(graph),
                "backup_type": "comprehensive_knowledge_base",
                "graph_size": len(graph)
            },
            "facts": []
        }
        for i, (s, p, o) in enumerate(graph):
            subject = str(s).split(':')[-1] if ':' in str(s) else str(s)
            predicate = str(p).split(':')[-1] if ':' in str(p) else str(p)
            object_val = str(o)
            backup_data["facts"].append({
                "id": i + 1,
                "subject": subject,
                "predicate": predicate,
                "object": object_val,
                "full_subject": str(s),
                "full_predicate": str(p),
                "full_object": str(o)
            })
        with open(BACKUP_FILE, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, indent=2, ensure_ascii=False)
    except Exception:
        create_error_backup("unknown")

def create_error_backup(error_message):
    try:
        backup_data = {
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "total_facts": 0,
                "backup_type": "error_backup",
                "error": error_message
            },
            "facts": []
        }
        with open(BACKUP_FILE, 'w', encoding='utf-8') as f:
            json.dump(backup_data, f, indent=2, ensure_ascii=False)
    except Exception:
        pass

def extract_entities(text):
    entities = []
    capitalized_words = re.findall(r'\b[A-Z][a-z]+\s+[A-Z][a-z]+(?:\s+[A-Z][a-z]+)?\b', text)
    entities.extend(capitalized_words)
    org_patterns = [
        r'([A-Z][a-zA-Z\s]+)\s+(Inc|Ltd|LLC|Corp|Corporation|Company|Co\.|Ltd\.)',
        r'([A-Z][a-zA-Z\s]+)\s+(University|Institute|Lab|Laboratory)',
    ]
    for pattern in org_patterns:
        matches = re.findall(pattern, text)
        entities.extend([m[0].strip() for m in matches])
    location_keywords = ['in ', 'at ', 'near ', 'from ']
    for keyword in location_keywords:
        pattern = f'{keyword}([A-Z][a-z]+(?:\\s+[A-Z][a-z]+)?)'
        matches = re.findall(pattern, text)
        entities.extend(matches)
    dates = re.findall(r'\b\d{1,2}[/-]\d{1,2}[/-]\d{2,4}\b|\b\d{4}\b', text)
    entities.extend(dates)
    entities = list(set([e.strip() for e in entities if len(e.strip()) > 3]))
    return entities[:50]

def extract_regular_triples_improved(text, entities):
    triples = []
    sentences = re.split(r'[.!?\n]+', text)
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) < 15:
            continue
        improved_patterns = [
            (r'([A-Z][a-zA-Z\s]+(?:,\s+[A-Z][a-zA-Z\s]+)*)\s+(is|are|was|were|becomes|represents|means|refers to|denotes)\s+(.+)', 'relates to'),
            (r'([A-Z][a-zA-Z\s]+)\s+(uses|employs|utilizes|applies)\s+(.+)', 'uses'),
            (r'([A-Z][a-zA-Z\s]+)\s+(develops|created|designed|implemented)\s+(.+)', 'creates'),
            (r'([A-Z][a-zA-Z\s]+)\s+(requires|needs|demands)\s+(.+)', 'requires'),
            (r'([A-Z][a-zA-Z\s]+)\s+(enables|allows|permits)\s+(.+)', 'enables'),
            (r'([A-Z][a-zA-Z\s]+)\s+(affects|impacts|influences|affects)\s+(.+)', 'affects'),
            (r'([A-Z][a-zA-Z\s]+)\s+(found|discovered|identified|observed|detected)\s+(.+)', 'discovered'),
            (r'([A-Z][a-zA-Z\s]+)\s+(studies|analyzes|examines|investigates)\s+(.+)', 'studies'),
            (r'([A-Z][a-zA-Z\s]+)\s+(proposes|suggests|recommends)\s+(.+)', 'proposes'),
            (r'([A-Z][a-zA-Z\s]+)\s+(results in|leads to|causes)\s+(.+)', 'causes'),
            (r'([A-Z][a-zA-Z\s]+)\s+(works with|collaborates with|partnered with)\s+(.+)', 'works with'),
            (r'([A-Z][a-zA-Z\s]+)\s+(located in|based in|situated in)\s+(.+)', 'located in'),
        ]
        for pattern, predicate in improved_patterns:
            match = re.search(pattern, sentence, re.IGNORECASE)
            if match:
                groups = match.groups()
                subject = groups[0].strip() if len(groups) > 0 else ''
                object_val = groups[-1].strip() if len(groups) > 1 else ''
                subject = re.sub(r'^(the|a|an)\s+', '', subject, flags=re.IGNORECASE).strip()
                object_val = re.sub(r'^(the|a|an)\s+', '', object_val, flags=re.IGNORECASE).strip()
                if subject and object_val and len(subject) > 3 and len(object_val) > 3:
                    triples.append((subject, predicate, object_val))
                    break
        clause_patterns = [
            r'([A-Z][a-zA-Z\s]+)\s+which\s+(.+)',
            r'([A-Z][a-zA-Z\s]+)\s+that\s+(.+)',
            r'([A-Z][a-zA-Z\s]+)\s+who\s+(.+)',
        ]
        for pattern in clause_patterns:
            match = re.search(pattern, sentence)
            if match:
                subject = match.group(1).strip()
                description = match.group(2).strip()
                if subject and description and len(subject) > 3 and len(description) > 3:
                    triples.append((subject, 'has property', description[:150]))
    return triples

def extract_structured_triples(text):
    triples = []
    lines = text.split('\n')
    patterns = [
        (r'date\s*:?\s*([0-9\/\-\.]+)', 'date', 'is'),
        (r'time\s*:?\s*([0-9:]+)', 'time', 'is'),
        (r'created\s*:?\s*([0-9\/\-\.]+)', 'created_date', 'is'),
        (r'modified\s*:?\s*([0-9\/\-\.]+)', 'modified_date', 'is'),
        (r'id\s*:?\s*([A-Z0-9\-]+)', 'id', 'is'),
        (r'number\s*:?\s*([A-Z0-9\-]+)', 'number', 'is'),
        (r'code\s*:?\s*([A-Z0-9\-]+)', 'code', 'is'),
        (r'reference\s*:?\s*([A-Z0-9\-]+)', 'reference', 'is'),
        (r'name\s*:?\s*([A-Za-z\s&.,]+)', 'name', 'is'),
        (r'title\s*:?\s*([A-Za-z\s&.,]+)', 'title', 'is'),
        (r'company\s*:?\s*([A-Za-z\s&.,]+)', 'company', 'is'),
        (r'organization\s*:?\s*([A-Za-z\s&.,]+)', 'organization', 'is'),
        (r'email\s*:?\s*([A-Za-z0-9@\.\-]+)', 'email', 'is'),
        (r'phone\s*:?\s*([0-9\s\-\+\(\)]+)', 'phone', 'is'),
        (r'address\s*:?\s*([A-Za-z0-9\s\-\.,]+)', 'address', 'is'),
        (r'description\s*:?\s*([A-Za-z0-9\s\-\.,]+)', 'description', 'is'),
        (r'type\s*:?\s*([A-Za-z0-9\s\-\.,]+)', 'type', 'is'),
        (r'category\s*:?\s*([A-Za-z0-9\s\-\.,]+)', 'category', 'is'),
        (r'status\s*:?\s*([A-Za-z0-9\s\-\.,]+)', 'status', 'is'),
        (r'location\s*:?\s*([A-Za-z0-9\s\-\.,]+)', 'location', 'is'),
        (r'department\s*:?\s*([A-Za-z0-9\s\-\.,]+)', 'department', 'is'),
        (r'section\s*:?\s*([A-Za-z0-9\s\-\.,]+)', 'section', 'is'),
        (r'amount\s*:?\s*\$?([0-9,]+\.?[0-9]*)', 'amount', 'is'),
        (r'total\s*:?\s*\$?([0-9,]+\.?[0-9]*)', 'total', 'is'),
        (r'price\s*:?\s*\$?([0-9,]+\.?[0-9]*)', 'price', 'is'),
        (r'cost\s*:?\s*\$?([0-9,]+\.?[0-9]*)', 'cost', 'is'),
    ]
    for line in lines:
        line = line.strip()
        if len(line) < 5:
            continue
        for pattern, subject, predicate in patterns:
            match = re.search(pattern, line, re.IGNORECASE)
            if match:
                value = match.group(1).strip()
                if value and len(value) > 1:
                    triples.append((subject, predicate, value))
                    break
    kv_patterns = [
        r'([A-Za-z\s]+):\s*([A-Za-z0-9\s\$\-\.\/,]+)',
        r'([A-Za-z\s]+)\s*=\s*([A-Za-z0-9\s\$\-\.\/,]+)',
        r'([A-Za-z\s]+)\s*-\s*([A-Za-z0-9\s\$\-\.\/,]+)',
    ]
    for line in lines:
        for pattern in kv_patterns:
            match = re.search(pattern, line)
            if match:
                key = match.group(1).strip().lower().replace(' ', '_')
                value = match.group(2).strip()
                if len(key) > 2 and len(value) > 1:
                    triples.append((key, 'is', value))
    return triples

def extract_regular_triples(text):
    triples = []
    sentences = re.split(r"[.?!\n]", text)
    patterns = [
        r"\s+(is|are|was|were)\s+",
        r"\s+(has|have|had)\s+",
        r"\s+(uses|used|using)\s+",
        r"\s+(creates|created|creating)\s+",
        r"\s+(develops|developed|developing)\s+",
        r"\s+(leads|led|leading)\s+",
        r"\s+(affects|affected|affecting)\s+",
        r"\s+(contains|contained|containing)\s+",
        r"\s+(includes|included|including)\s+",
        r"\s+(requires|required|requiring)\s+",
        r"\s+(causes|caused|causing)\s+",
        r"\s+(results|resulted|resulting)\s+",
        r"\s+(enables|enabled|enabling)\s+",
        r"\s+(provides|provided|providing)\s+",
        r"\s+(supports|supported|supporting)\s+",
        r"\s+(located|situated|found)\s+",
        r"\s+(connects|links|relates)\s+",
        r"\s+(depends|relies|based)\s+",
        r"\s+(represents|symbolizes|stands)\s+",
        r"\s+(describes|explains|defines)\s+",
        r"\s+(refers|referring|referenced)\s+",
        r"\s+(concerns|concerning|concerned)\s+",
        r"\s+(relates|relating|related)\s+",
    ]
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) < 10:
            continue
        for pattern in patterns:
            parts = re.split(pattern, sentence, maxsplit=1)
            if len(parts) == 3:
                subj, pred, obj = parts
                subj = re.sub(r'^(the|a|an)\s+', '', subj.strip(), flags=re.IGNORECASE)
                obj = re.sub(r'^(the|a|an)\s+', '', obj.strip(), flags=re.IGNORECASE)
                if subj and pred and obj and len(subj) > 2 and len(obj) > 2:
                    triples.append((subj, pred.strip(), obj))
                    break
    return triples

# ============================================================================
# ENTITY NORMALIZATION
# ============================================================================

# Entity normalization mappings (case-insensitive)
# Format: {variant: canonical_form}
entity_normalization_map = {}

def load_normalization_map():
    """Load entity normalization mappings from file"""
    global entity_normalization_map
    if os.path.exists(NORMALIZATION_FILE):
        try:
            with open(NORMALIZATION_FILE, 'r', encoding='utf-8') as f:
                entity_normalization_map = json.load(f)
        except:
            entity_normalization_map = {}
    else:
        # Initialize with common mappings
        entity_normalization_map = {
            # Abbreviations - Academic/Research
            'hw': 'homework',
            'homework': 'homework',
            'wp': 'work package',
            'wpl': 'work package',
            'work package': 'work package',
            'workplan': 'work plan',
            'work plan': 'work plan',
            'detailed work plan': 'work plan',
            'kom': 'kick-off meeting',
            'kom notes': 'kick-off meeting notes',
            'kg': 'knowledge graph',
            'knowledge graph': 'knowledge graph',
            
            # Geographic aliases
            'uk': 'united kingdom',
            'united kingdom': 'united kingdom',
            'great britain': 'united kingdom',
            'gb': 'united kingdom',
            'britain': 'united kingdom',
            'usa': 'united states',
            'us': 'united states',
            'united states': 'united states',
            'united states of america': 'united states',
            
            # Institutional
            'icl': 'imperial college london',
            'imperial college london': 'imperial college london',
            'imperial college': 'imperial college london',
            
            # Meeting variations
            'next meeting': 'meeting',
            'next meeting proposal': 'meeting',
            'meeting': 'meeting',
            'meeting proposal': 'meeting',
            
            # Day names
            'monday': 'monday',
            'mon': 'monday',
            'tuesday': 'tuesday',
            'tue': 'tuesday',
            'wednesday': 'wednesday',
            'wed': 'wednesday',
            'thursday': 'thursday',
            'thu': 'thursday',
            'friday': 'friday',
            'fri': 'friday',
            'saturday': 'saturday',
            'sat': 'saturday',
            'sunday': 'sunday',
            'sun': 'sunday',
        }
        save_normalization_map()

def save_normalization_map():
    """Save entity normalization mappings to file"""
    try:
        with open(NORMALIZATION_FILE, 'w', encoding='utf-8') as f:
            json.dump(entity_normalization_map, f, indent=2, ensure_ascii=False)
    except:
        pass

def normalize_entity(entity):
    """
    Normalize an entity to its canonical form.
    Handles case-insensitive matching, abbreviations, and aliases.
    
    Examples:
    - "homework" -> "homework"
    - "HW" -> "homework"
    - "hw" -> "homework"
    - "WP" -> "work package"
    - "UK" -> "united kingdom"
    - "workplan" -> "work plan"
    """
    if not entity:
        return entity
    
    entity_lower = entity.lower().strip()
    
    # Check normalization map
    if entity_lower in entity_normalization_map:
        return entity_normalization_map[entity_lower]
    
    # Check if any variant matches (case-insensitive)
    for variant, canonical in entity_normalization_map.items():
        if entity_lower == variant.lower() or entity_lower.startswith(variant.lower() + ' ') or entity_lower.endswith(' ' + variant.lower()):
            return canonical
    
    # If no normalization found, return cleaned entity
    return entity

def add_normalization_mapping(variant, canonical):
    """Add a new normalization mapping"""
    global entity_normalization_map
    variant_lower = variant.lower().strip()
    canonical_lower = canonical.lower().strip()
    
    if variant_lower != canonical_lower:
        entity_normalization_map[variant_lower] = canonical_lower
        save_normalization_map()

def learn_normalizations_from_facts():
    """
    Learn normalization mappings from existing facts in the graph.
    Identifies entities that are similar (case-insensitive, abbreviations, etc.)
    and suggests normalizations.
    """
    global graph, entity_normalization_map
    
    # Group entities by normalized form (case-insensitive)
    entity_groups = defaultdict(list)
    
    for s, p, o in graph:
        # Extract and normalize entities
        s_str = str(s).split(':')[-1] if ':' in str(s) else str(s)
        o_str = str(o)
        
        # Normalize to lowercase for grouping
        s_normalized = s_str.lower().strip()
        o_normalized = o_str.lower().strip()
        
        # Group similar entities (exact match after normalization)
        entity_groups[s_normalized].append(s_str)
        entity_groups[o_normalized].append(o_str)
    
    # Find potential normalizations (entities that differ only by case or minor variations)
    for normalized, variants in entity_groups.items():
        unique_variants = list(set(variants))
        if len(unique_variants) > 1:
            # Use the most common variant as canonical
            canonical = max(set(variants), key=variants.count)
            canonical_lower = canonical.lower().strip()
            
            # Add mappings for all variants
            for variant in unique_variants:
                variant_lower = variant.lower().strip()
                if variant_lower != canonical_lower and variant_lower not in entity_normalization_map:
                    entity_normalization_map[variant_lower] = canonical_lower
    
    save_normalization_map()

def extract_core_entity(entity, context=""):
    """
    Extract the core entity from a longer phrase, moving all background/contextual
    information to the details field.
    
    Examples:
    - "3rd Monday of each month 10-11 CET" -> "monday" (with details: "3rd of each month 10-11 CET")
    - "Detailed Work Plan with Involved Partners till Month 18" -> "work plan" (with details: "Detailed with Involved Partners till Month 18")
    - "next meeting proposal" -> "meeting" (with details: "next proposal")
    - "Any discrepancies apart from the ones discovered in KoM notes" -> "discrepancies" (with details: "Any apart from the ones discovered in KoM notes")
    """
    if not entity:
        return entity, None
    
    entity = entity.strip()
    original = entity
    
    # Store all extracted background information here
    details_parts = []
    core_entity = entity
    
    # Extract time ranges like "10-11 CET" or "10-11"
    time_range_pattern = r'\b(\d{1,2}-\d{1,2}(?:\s+(?:CET|UTC|GMT|EST|PST|PDT|EDT|CDT|MDT))?)\b'
    time_matches = re.finditer(time_range_pattern, entity, re.IGNORECASE)
    for match in time_matches:
        details_parts.append(match.group(0))
        core_entity = core_entity.replace(match.group(0), '').strip()
    
    # Extract month references like "till Month 18" or "month 18"
    month_pattern = r'\b(till\s+month\s+\d+|month\s+\d+|each\s+month|of\s+each\s+month)\b'
    month_matches = re.finditer(month_pattern, entity, re.IGNORECASE)
    for match in month_matches:
        details_parts.append(match.group(0))
        core_entity = core_entity.replace(match.group(0), '').strip()
    
    # Extract ordinal day patterns like "3rd Monday" (but keep the day name for core entity)
    ordinal_day_pattern = r'\b(\d{1,2}(?:st|nd|rd|th)?)\s+(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday|Mon|Tue|Wed|Thu|Fri|Sat|Sun)\b'
    ordinal_match = re.search(ordinal_day_pattern, entity, re.IGNORECASE)
    if ordinal_match:
        ordinal_part = ordinal_match.group(1)
        details_parts.append(ordinal_part)
        core_entity = core_entity.replace(ordinal_part, '').strip()
    
    # Extract core noun phrases
    # Remove descriptive adjectives and qualifiers
    descriptive_patterns = [
        r'^(detailed|comprehensive|extensive|complete|full|partial|brief|short|long)\s+',
        r'\s+(with|including|containing|having|featuring)\s+.+$',
        r'\s+(till|until|up to|through|by)\s+.+$',
        r'\s+(proposal|suggestion|recommendation|idea|concept|plan|draft|version)\s*$',
    ]
    
    for pattern in descriptive_patterns:
        matches = re.finditer(pattern, core_entity, re.IGNORECASE)
        for match in matches:
            details_parts.append(match.group(0).strip())
            core_entity = re.sub(pattern, '', core_entity, flags=re.IGNORECASE).strip()
    
    # Extract day names (e.g., "3rd Monday of each month 10-11 CET" -> "monday")
    # Check original entity first for day names
    day_pattern = r'\b(\d{1,2}(?:st|nd|rd|th)?\s+)?(Monday|Tuesday|Wednesday|Thursday|Friday|Saturday|Sunday|Mon|Tue|Wed|Thu|Fri|Sat|Sun)\b'
    day_match = re.search(day_pattern, original, re.IGNORECASE)
    if day_match:
        day_name = day_match.group(2).lower() if day_match.group(2) else None
        if day_name:
            # Normalize day names
            day_map = {'mon': 'monday', 'tue': 'tuesday', 'wed': 'wednesday', 
                      'thu': 'thursday', 'fri': 'friday', 'sat': 'saturday', 'sun': 'sunday'}
            day_name = day_map.get(day_name, day_name)
            # If we found a day in a complex phrase, extract it as core entity
            # Example: "3rd Monday of each month 10-11 CET" -> "monday"
            if len(original.split()) > 3 or len(details_parts) > 0:
                core_entity = day_name
                if not details_parts:
                    details_parts = [original]
            elif day_name not in core_entity.lower():
                core_entity = day_name
                if not details_parts:
                    details_parts = [original]
    
    # Extract work plan / work package (e.g., "Detailed Work Plan with Involved Partners till Month 18" -> "work plan")
    if 'work plan' in core_entity.lower() or 'workplan' in core_entity.lower() or 'work package' in core_entity.lower():
        core_entity = 'work plan'
        if original.lower() != 'work plan' and not details_parts:
            details_parts = [original]
    
    # Extract meeting (e.g., "next meeting proposal" -> "meeting")
    if 'meeting' in core_entity.lower():
        core_entity = 'meeting'
        if original.lower() != 'meeting' and not details_parts:
            details_parts = [original]
    
    # Clean up core entity
    core_entity = clean_entity(core_entity)
    
    # Normalize the core entity
    core_entity = normalize_entity(core_entity)
    
    # Build details - include all background information
    details = None
    if details_parts:
        # Join all extracted details
        details = ' '.join(details_parts).strip()
    elif original.lower() != core_entity.lower():
        # If we modified the entity, store the original as details
        details = original
    
    # If we have context (full sentence), include it for better understanding
    # This helps preserve the full meaning even when we extract core entities
    if context and context.strip() and context != original:
        if details:
            # Append context if it's not already included
            if context[:100] not in details:
                details = f"{details} | Context: {context[:200]}"
        else:
            details = f"Context: {context[:200]}"
    
    return core_entity, details

def clean_entity(entity):
    """
    Clean an entity by removing articles, qualifiers, and common prefixes.
    Examples:
    - "Any discrepancies apart from the ones" -> "discrepancies"
    - "in KoM notes" -> "KoM notes"
    - "the Machine Learning" -> "Machine Learning"
    """
    if not entity:
        return entity
    
    entity = entity.strip()
    
    # Remove common prefixes and qualifiers
    prefixes_to_remove = [
        r'^(any|all|some|each|every|both|either|neither)\s+',
        r'^(the|a|an)\s+',
        r'^(this|that|these|those)\s+',
        r'^(other|another|same|different)\s+',
    ]
    
    for prefix in prefixes_to_remove:
        entity = re.sub(prefix, '', entity, flags=re.IGNORECASE)
    
    # Remove trailing qualifiers like "apart from the ones", "except for", etc.
    qualifiers_to_remove = [
        r'\s+apart\s+from\s+the\s+ones?.*$',
        r'\s+except\s+for.*$',
        r'\s+other\s+than.*$',
        r'\s+besides.*$',
    ]
    
    for qualifier in qualifiers_to_remove:
        entity = re.sub(qualifier, '', entity, flags=re.IGNORECASE)
    
    # Remove leading prepositions if they're not part of a proper noun
    # But keep "in" if it's part of a proper noun like "in KoM notes" -> "KoM notes"
    entity = re.sub(r'^(in|at|on|from|to|for|with|by)\s+', '', entity, flags=re.IGNORECASE)
    
    return entity.strip()

# ============================================================================
# TRIPLEX MODEL INTEGRATION
# ============================================================================

def load_triplex_model():
    """Load Triplex model for knowledge extraction (lazy loading)"""
    global TRIPLEX_MODEL, TRIPLEX_TOKENIZER
    
    if not TRIPLEX_AVAILABLE:
        return False
    
    if TRIPLEX_MODEL is not None and TRIPLEX_TOKENIZER is not None:
        return True
    
    try:
        print(f"üîÑ Loading Triplex model ({TRIPLEX_MODEL_NAME})...")
        print(f"   Device: {TRIPLEX_DEVICE}")
        
        TRIPLEX_TOKENIZER = AutoTokenizer.from_pretrained(TRIPLEX_MODEL_NAME, trust_remote_code=True)
        TRIPLEX_MODEL = AutoModelForCausalLM.from_pretrained(
            TRIPLEX_MODEL_NAME, 
            trust_remote_code=True,
            torch_dtype=torch.float16 if TRIPLEX_DEVICE == "cuda" else torch.float32,
            device_map="auto" if TRIPLEX_DEVICE == "cuda" else None
        )
        
        if TRIPLEX_DEVICE == "cpu":
            TRIPLEX_MODEL = TRIPLEX_MODEL.to(TRIPLEX_DEVICE)
        
        TRIPLEX_MODEL.eval()
        print(f"‚úÖ Triplex model loaded successfully on {TRIPLEX_DEVICE}")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è  Failed to load Triplex model: {e}")
        print("   Falling back to regex-based extraction")
        return False

def extract_with_triplex(text, entity_types=None, predicates=None):
    """
    Extract knowledge graph triplets using Triplex model.
    
    Args:
        text: Input text to extract from
        entity_types: Optional list of entity types to focus on
        predicates: Optional list of predicates to focus on
    
    Returns:
        List of tuples: (subject, predicate, object, details)
    """
    global TRIPLEX_MODEL, TRIPLEX_TOKENIZER
    
    if not TRIPLEX_AVAILABLE or not USE_TRIPLEX:
        return []
    
    # Lazy load model
    if not load_triplex_model():
        return []
    
    try:
        # Default entity types and predicates if not provided
        if entity_types is None:
            entity_types = [
                "PERSON", "ORGANIZATION", "LOCATION", "DATE", "CONCEPT", 
                "PROJECT", "DOCUMENT", "MEETING", "TASK", "DELIVERABLE"
            ]
        
        if predicates is None:
            predicates = [
                "is", "has", "uses", "creates", "requires", "enables", 
                "affects", "discovered", "studies", "proposes", "causes",
                "works with", "located in", "based in", "part of", "related to"
            ]
        
        # Format input according to Triplex format
        input_format = """Perform Named Entity Recognition (NER) and extract knowledge graph triplets from the text. NER identifies named entities of given entity types, and triple extraction identifies relationships between entities using specified predicates.
      
        **Entity Types:**
        {entity_types}
        
        **Predicates:**
        {predicates}
        
        **Text:**
        {text}
        """
        
        message = input_format.format(
            entity_types=json.dumps({"entity_types": entity_types}),
            predicates=json.dumps({"predicates": predicates}),
            text=text[:2000]  # Limit text length for performance
        )
        
        messages = [{'role': 'user', 'content': message}]
        input_ids = TRIPLEX_TOKENIZER.apply_chat_template(
            messages, 
            add_generation_prompt=True, 
            return_tensors="pt"
        ).to(TRIPLEX_DEVICE)
        
        # Generate with reasonable limits
        with torch.no_grad():
            # Fix for compatibility: disable cache to avoid DynamicCache issues
            try:
                output_ids = TRIPLEX_MODEL.generate(
                    input_ids,
                    max_new_tokens=512,  # Use max_new_tokens instead of max_length
                    do_sample=False,
                    pad_token_id=TRIPLEX_TOKENIZER.eos_token_id,
                    eos_token_id=TRIPLEX_TOKENIZER.eos_token_id,
                    use_cache=False  # Disable cache to avoid compatibility issues
                )
            except Exception as gen_error:
                print(f"‚ö†Ô∏è  Generation error (first attempt): {gen_error}")
                # Fallback: try with attention_mask
                try:
                    attention_mask = torch.ones_like(input_ids)
                    output_ids = TRIPLEX_MODEL.generate(
                        input_ids,
                        attention_mask=attention_mask,
                        max_new_tokens=512,
                        do_sample=False,
                        pad_token_id=TRIPLEX_TOKENIZER.eos_token_id,
                        eos_token_id=TRIPLEX_TOKENIZER.eos_token_id,
                        use_cache=False
                    )
                except Exception as gen_error2:
                    print(f"‚ö†Ô∏è  Generation error (fallback): {gen_error2}")
                    raise gen_error2
        
        output = TRIPLEX_TOKENIZER.decode(output_ids[0], skip_special_tokens=True)
        
        # Parse output to extract triplets
        # Triplex typically returns JSON or structured text
        triples = []
        
        # Try to parse as JSON first
        try:
            # Extract JSON from output if present
            json_match = re.search(r'\{.*\}', output, re.DOTALL)
            if json_match:
                parsed = json.loads(json_match.group(0))
                if isinstance(parsed, dict) and 'triplets' in parsed:
                    for triple in parsed['triplets']:
                        if 'subject' in triple and 'predicate' in triple and 'object' in triple:
                            s = triple['subject']
                            p = triple['predicate']
                            o = triple['object']
                            # Extract core entities and normalize
                            core_s, s_details = extract_core_entity(s, text)
                            core_o, o_details = extract_core_entity(o, text)
                            core_s = normalize_entity(core_s)
                            core_o = normalize_entity(core_o)
                            
                            # Combine details
                            details_parts = []
                            if s_details:
                                details_parts.append(f"Subject: {s_details}")
                            if o_details:
                                details_parts.append(f"Object: {o_details}")
                            if not details_parts:
                                details_parts.append(text[:200])  # Use original text as context
                            
                            details = ' | '.join(details_parts) if details_parts else None
                            triples.append((core_s, p, core_o, details))
        except:
            pass
        
        # Fallback: Try to extract triplets from text format
        # Pattern: (subject, predicate, object) or subject | predicate | object
        triplet_patterns = [
            r'\(([^,]+),\s*([^,]+),\s*([^)]+)\)',
            r'([^\|]+)\s*\|\s*([^\|]+)\s*\|\s*([^\|]+)',
            r'Subject:\s*([^\n]+)\s+Predicate:\s*([^\n]+)\s+Object:\s*([^\n]+)',
        ]
        
        for pattern in triplet_patterns:
            matches = re.finditer(pattern, output)
            for match in matches:
                s = match.group(1).strip()
                p = match.group(2).strip()
                o = match.group(3).strip()
                
                # Extract core entities and normalize
                core_s, s_details = extract_core_entity(s, text)
                core_o, o_details = extract_core_entity(o, text)
                core_s = normalize_entity(core_s)
                core_o = normalize_entity(core_o)
                
                # Combine details
                details_parts = []
                if s_details:
                    details_parts.append(f"Subject: {s_details}")
                if o_details:
                    details_parts.append(f"Object: {o_details}")
                if not details_parts:
                    details_parts.append(text[:200])
                
                details = ' | '.join(details_parts) if details_parts else None
                triples.append((core_s, p, core_o, details))
        
        return triples
        
    except Exception as e:
        print(f"‚ö†Ô∏è  Error in Triplex extraction: {e}")
        import traceback
        traceback.print_exc()
        return []

def extract_with_improved_patterns(text):
    """
    Extract knowledge graph triplets using improved regex patterns.
    This is an enhanced version that uses better linguistic patterns
    to identify subject-predicate-object relationships.
    
    Returns:
        List of tuples: (subject, predicate, object, details)
    """
    triples = []
    sentences = re.split(r'[.!?\n]+', text)
    
    # Common verb patterns for relationships
    verb_patterns = [
        # "X works at Y", "X is Y", "X has Y"
        (r'([A-Z][a-zA-Z\s]+(?:,\s+[A-Z][a-zA-Z\s]+)*)\s+(works?\s+at|is\s+at|located\s+at|based\s+at)\s+(.+)', 'works_at'),
        (r'([A-Z][a-zA-Z\s]+(?:,\s+[A-Z][a-zA-Z\s]+)*)\s+(is|are|was|were|becomes|represents|means|refers\s+to|denotes)\s+(.+)', 'is'),
        (r'([A-Z][a-zA-Z\s]+(?:,\s+[A-Z][a-zA-Z\s]+)*)\s+(has|have|had|contains|includes)\s+(.+)', 'has'),
        
        # "X started in Y", "X created Y", "X developed Y"
        (r'([A-Z][a-zA-Z\s]+(?:,\s+[A-Z][a-zA-Z\s]+)*)\s+(started|began|commenced)\s+(in|on|at)\s+(.+)', 'started_in'),
        (r'([A-Z][a-zA-Z\s]+(?:,\s+[A-Z][a-zA-Z\s]+)*)\s+(created|developed|designed|built|implemented)\s+(.+)', 'creates'),
        
        # "X uses Y", "X requires Y", "X needs Y"
        (r'([A-Z][a-zA-Z\s]+(?:,\s+[A-Z][a-zA-Z\s]+)*)\s+(uses?|employs?|utilizes?|applies?)\s+(.+)', 'uses'),
        (r'([A-Z][a-zA-Z\s]+(?:,\s+[A-Z][a-zA-Z\s]+)*)\s+(requires?|needs?|demands?)\s+(.+)', 'requires'),
        
        # "X enables Y", "X allows Y", "X permits Y"
        (r'([A-Z][a-zA-Z\s]+(?:,\s+[A-Z][a-zA-Z\s]+)*)\s+(enables?|allows?|permits?)\s+(.+)', 'enables'),
        
        # "X affects Y", "X impacts Y", "X influences Y"
        (r'([A-Z][a-zA-Z\s]+(?:,\s+[A-Z][a-zA-Z\s]+)*)\s+(affects?|impacts?|influences?)\s+(.+)', 'affects'),
        
        # "X discovered Y", "X found Y", "X identified Y"
        (r'([A-Z][a-zA-Z\s]+(?:,\s+[A-Z][a-zA-Z\s]+)*)\s+(discovered?|found|identified?|detected?)\s+(.+)', 'discovered'),
        
        # "X is on Y", "X happens on Y", "X occurs on Y"
        (r'([A-Z][a-zA-Z\s]+(?:,\s+[A-Z][a-zA-Z\s]+)*)\s+(is|are|happens?|occurs?|takes?\s+place)\s+(on|in|at)\s+(.+)', 'occurs_on'),
        
        # "X is to Y" (purpose/goal)
        (r'([A-Z][a-zA-Z\s]+(?:,\s+[A-Z][a-zA-Z\s]+)*)\s+(is|are)\s+to\s+(.+)', 'purpose_is'),
    ]
    
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) < 10:
            continue
        
        # Try each pattern
        for pattern, pred_type in verb_patterns:
            match = re.search(pattern, sentence, re.IGNORECASE)
            if match:
                groups = match.groups()
                if len(groups) >= 2:
                    subject = groups[0].strip()
                    if len(groups) == 2:
                        obj = groups[1].strip()
                        predicate = pred_type
                    else:
                        # Pattern has preposition, skip it for predicate
                        obj = groups[-1].strip()
                        predicate = pred_type
                    
                    # Extract core entities and normalize
                    # Pass full sentence as context to preserve all background info
                    core_s, s_details = extract_core_entity(subject, sentence)
                    core_o, o_details = extract_core_entity(obj, sentence)
                    core_s = normalize_entity(core_s)
                    core_o = normalize_entity(core_o)
                    
                    # Only add if we have valid entities
                    if core_s and core_o and len(core_s) > 2 and len(core_o) > 2:
                        # Combine details - preserve all background information
                        details_parts = []
                        
                        # Add subject details if we extracted background info from subject
                        if s_details:
                            if "Context:" in s_details:
                                # Context already includes full sentence, use it
                                details_parts.append(s_details)
                            else:
                                details_parts.append(f"Subject context: {s_details}")
                        
                        # Add object details if we extracted background info from object
                        if o_details:
                            if "Context:" in o_details:
                                # Context already includes full sentence, use it
                                if not details_parts or "Context:" not in details_parts[0]:
                                    details_parts.append(o_details)
                            else:
                                details_parts.append(f"Object context: {o_details}")
                        
                        # If no details were extracted but original differs from core, store original sentence
                        if not details_parts:
                            if subject != core_s or obj != core_o:
                                details_parts.append(f"Original: {sentence.strip()}")
                            else:
                                # Store full sentence as context for reference
                                details_parts.append(f"Full context: {sentence.strip()}")
                        
                        details = ' | '.join(details_parts) if details_parts else None
                        triples.append((core_s, predicate, core_o, details))
                        break  # Found a match, move to next sentence
    
    return triples

def extract_triples_with_context(text):
    """
    Extract triples with cleaner entities and store original context in details.
    Returns list of tuples: (subject, predicate, object, details)
    """
    triples_with_context = []
    sentences = re.split(r'[.!?\n]+', text)
    
    for sentence in sentences:
        sentence = sentence.strip()
        if len(sentence) < 15:
            continue
        
        # Try improved patterns first (case-insensitive to handle lowercase starts)
        improved_patterns = [
            (r'([A-Za-z][a-zA-Z\s]+(?:,\s+[A-Za-z][a-zA-Z\s]+)*)\s+(is|are|was|were|becomes|represents|means|refers to|denotes)\s+(.+)', 'relates to'),
            (r'([A-Za-z][a-zA-Z\s]+)\s+(uses|employs|utilizes|applies)\s+(.+)', 'uses'),
            (r'([A-Za-z][a-zA-Z\s]+)\s+(develops|created|designed|implemented)\s+(.+)', 'creates'),
            (r'([A-Za-z][a-zA-Z\s]+)\s+(requires|needs|demands)\s+(.+)', 'requires'),
            (r'([A-Za-z][a-zA-Z\s]+)\s+(enables|allows|permits)\s+(.+)', 'enables'),
            (r'([A-Za-z][a-zA-Z\s]+)\s+(affects|impacts|influences)\s+(.+)', 'affects'),
            (r'([A-Za-z][a-zA-Z\s]+)\s+(found|discovered|identified|observed|detected)\s+(.+)', 'discovered'),
            (r'([A-Za-z][a-zA-Z\s]+)\s+(studies|analyzes|examines|investigates)\s+(.+)', 'studies'),
            (r'([A-Za-z][a-zA-Z\s]+)\s+(proposes|suggests|recommends)\s+(.+)', 'proposes'),
            (r'([A-Za-z][a-zA-Z\s]+)\s+(results in|leads to|causes)\s+(.+)', 'causes'),
            (r'([A-Za-z][a-zA-Z\s]+)\s+(works with|collaborates with|partnered with)\s+(.+)', 'works with'),
            (r'([A-Za-z][a-zA-Z\s]+)\s+(located in|based in|situated in)\s+(.+)', 'located in'),
        ]
        
        for pattern, predicate in improved_patterns:
            match = re.search(pattern, sentence, re.IGNORECASE)
            if match:
                groups = match.groups()
                original_subject = groups[0].strip() if len(groups) > 0 else ''
                original_object = groups[-1].strip() if len(groups) > 1 else ''
                
                # Extract core entities (this handles dates, times, descriptive text)
                core_subject, subject_details = extract_core_entity(original_subject, sentence)
                core_object, object_details = extract_core_entity(original_object, sentence)
                
                # Normalize entities (handles abbreviations, aliases, case-insensitive)
                core_subject = normalize_entity(core_subject)
                core_object = normalize_entity(core_object)
                
                # Only proceed if core entities are meaningful
                if core_subject and core_object and len(core_subject) > 2 and len(core_object) > 2:
                    # Combine details - preserve all background information
                    details_parts = []
                    
                    # Add subject details if we extracted background info from subject
                    if subject_details:
                        if "Context:" in subject_details:
                            # Context already includes full sentence, use it
                            details_parts.append(subject_details)
                        else:
                            details_parts.append(f"Subject context: {subject_details}")
                    
                    # Add object details if we extracted background info from object
                    if object_details:
                        if "Context:" in object_details:
                            # Context already includes full sentence, use it
                            if not details_parts or "Context:" not in details_parts[0]:
                                details_parts.append(object_details)
                        else:
                            details_parts.append(f"Object context: {object_details}")
                    
                    # If no details were extracted but original differs from core, store original sentence
                    if not details_parts:
                        if original_subject != core_subject or original_object != core_object:
                            details_parts.append(f"Original: {sentence.strip()}")
                        else:
                            # Store full sentence as context for reference
                            details_parts.append(f"Full context: {sentence.strip()}")
                    
                    details = ' | '.join(details_parts) if details_parts else None
                    
                    triples_with_context.append((core_subject, predicate, core_object, details))
                    break
        
        # Also try regular patterns
        patterns = [
            r"\s+(is|are|was|were)\s+",
            r"\s+(has|have|had)\s+",
            r"\s+(uses|used|using)\s+",
            r"\s+(creates|created|creating)\s+",
            r"\s+(develops|developed|developing)\s+",
            r"\s+(leads|led|leading)\s+",
            r"\s+(affects|affected|affecting)\s+",
            r"\s+(contains|contained|containing)\s+",
            r"\s+(includes|included|including)\s+",
            r"\s+(requires|required|requiring)\s+",
            r"\s+(causes|caused|causing)\s+",
            r"\s+(results|resulted|resulting)\s+",
            r"\s+(enables|enabled|enabling)\s+",
            r"\s+(provides|provided|providing)\s+",
            r"\s+(supports|supported|supporting)\s+",
            r"\s+(located|situated|found)\s+",
            r"\s+(connects|links|relates)\s+",
            r"\s+(depends|relies|based)\s+",
            r"\s+(represents|symbolizes|stands)\s+",
            r"\s+(describes|explains|defines)\s+",
            r"\s+(refers|referring|referenced)\s+",
            r"\s+(concerns|concerning|concerned)\s+",
            r"\s+(relates|relating|related)\s+",
            r"\s+(discovered|discovering)\s+",
        ]
        
        for pattern in patterns:
            parts = re.split(pattern, sentence, maxsplit=1)
            if len(parts) == 3:
                original_subj, pred, original_obj = parts
                original_subj = original_subj.strip()
                original_obj = original_obj.strip()
                
                # Extract core entities (this handles dates, times, descriptive text)
                # Pass full sentence as context to preserve all background info
                core_subj, subject_details = extract_core_entity(original_subj, sentence)
                core_obj, object_details = extract_core_entity(original_obj, sentence)
                
                # Normalize entities (handles abbreviations, aliases, case-insensitive)
                core_subj = normalize_entity(core_subj)
                core_obj = normalize_entity(core_obj)
                
                if core_subj and core_obj and len(core_subj) > 2 and len(core_obj) > 2:
                    # Combine details - preserve all background information
                    details_parts = []
                    
                    # Add subject details if we extracted background info from subject
                    if subject_details:
                        if "Context:" in subject_details:
                            # Context already includes full sentence, use it
                            details_parts.append(subject_details)
                        else:
                            details_parts.append(f"Subject context: {subject_details}")
                    
                    # Add object details if we extracted background info from object
                    if object_details:
                        if "Context:" in object_details:
                            # Context already includes full sentence, use it
                            if not details_parts or "Context:" not in details_parts[0]:
                                details_parts.append(object_details)
                        else:
                            details_parts.append(f"Object context: {object_details}")
                    
                    # If no details were extracted but original differs from core, store original sentence
                    if not details_parts:
                        if original_subj != core_subj or original_obj != core_obj:
                            details_parts.append(f"Original: {sentence.strip()}")
                        else:
                            # Store full sentence as context for reference
                            details_parts.append(f"Full context: {sentence.strip()}")
                    
                    details = ' | '.join(details_parts) if details_parts else None
                    
                    triples_with_context.append((core_subj, pred.strip(), core_obj, details))
                    break
    
    return triples_with_context

def extract_triples(text):
    triples = []
    entities = extract_entities(text)
    for entity in entities:
        triples.append((entity, 'type', 'entity'))
    triples.extend(extract_structured_triples(text))
    triples.extend(extract_regular_triples_improved(text, entities))
    triples.extend(extract_regular_triples(text))
    unique_triples = []
    for s, p, o in triples:
        if s and p and o and len(s) > 2 and len(p) > 1 and len(o) > 2:
            s = s.strip()[:100]
            p = p.strip()[:50]
            o = o.strip()[:200]
            if (s, p, o) not in unique_triples:
                unique_triples.append((s, p, o))
    return unique_triples

def _initialize_fact_lookup_index():
    """Initialize in-memory fact lookup index for fast existence checks"""
    global _fact_lookup_set, _fact_index_initialized, graph
    from urllib.parse import unquote
    
    if _fact_index_initialized:
        return
    
    print(f"üîÑ Initializing fact lookup index from {len(graph)} triples...")
    _fact_lookup_set.clear()
    
    for s, p, o in graph:
        # Skip metadata triples
        predicate_str = str(p)
        if ('fact_subject' in predicate_str or 'fact_predicate' in predicate_str or 
            'fact_object' in predicate_str or 'has_details' in predicate_str or 
            'source_document' in predicate_str or 'uploaded_at' in predicate_str or
            'agent_id' in predicate_str):
            continue
        
        # Normalize and add to index
        s_str = str(s)
        if 'urn:' in s_str:
            s_decoded = unquote(s_str.replace('urn:', '')).replace('_', ' ').lower().strip()
        else:
            s_decoded = str(s).lower().strip().replace('_', ' ')
        s_decoded = normalize_entity(s_decoded)
        
        p_str = str(p)
        if 'urn:' in p_str:
            p_decoded = unquote(p_str.replace('urn:', '')).replace('_', ' ').lower().strip()
        else:
            p_decoded = str(p).lower().strip().replace('_', ' ')
        
        o_decoded = normalize_entity(str(o).lower().strip())
        
        _fact_lookup_set.add((s_decoded, p_decoded, o_decoded))
    
    _fact_index_initialized = True
    print(f"‚úÖ Fact lookup index initialized with {len(_fact_lookup_set)} facts")

def fact_exists(subject: str, predicate: str, object_val: str) -> bool:
    """
    Check if a fact (subject, predicate, object) already exists in the graph.
    Uses in-memory index for O(1) lookup instead of O(n) iteration.
    
    Args:
        subject: The subject of the fact
        predicate: The predicate of the fact
        object_val: The object of the fact
    
    Returns:
        True if the fact already exists (case-insensitive), False otherwise
    """
    global _fact_lookup_set, _fact_index_initialized
    
    # Initialize index if needed
    if not _fact_index_initialized:
        _initialize_fact_lookup_index()
    
    # Normalize the input (case-insensitive + entity normalization)
    subject_normalized = normalize_entity(str(subject).strip().lower().replace('_', ' '))
    predicate_normalized = str(predicate).strip().lower().replace('_', ' ')
    object_normalized = normalize_entity(str(object_val).strip().lower())
    
    # Fast O(1) lookup using set
    return (subject_normalized, predicate_normalized, object_normalized) in _fact_lookup_set

def add_to_graph(text, source_document: str = "manual", uploaded_at: str = None, agent_id: Optional[str] = None):
    """
    Extract knowledge from text and add to graph.
    Uses the new 7-step pipeline by default, with fallback to legacy methods.
    
    Args:
        text: Text to extract knowledge from
        source_document: Name of the source document (default: "manual")
        uploaded_at: ISO format timestamp when the fact was added (default: current time)
        agent_id: ID of the worker agent that extracted this fact (optional)
    
    Returns:
        str: Status message with extraction method and counts
    """
    global graph
    import rdflib
    from urllib.parse import quote
    from datetime import datetime
    
    # Set default timestamp if not provided
    if uploaded_at is None:
        uploaded_at = datetime.now().isoformat()
    
    # Try new pipeline first
    try:
        from kg_pipeline import extract_knowledge_pipeline
        print(f"üîÑ Attempting pipeline extraction from text (length: {len(text)} chars)...")
        pipeline_triples = extract_knowledge_pipeline(text, source_document, uploaded_at)
        print(f"üìä Pipeline returned {len(pipeline_triples) if pipeline_triples else 0} triples")
        
        if pipeline_triples:
            print(f"‚úÖ Pipeline extracted {len(pipeline_triples)} triples")
            # Process pipeline triples
            added_count = 0
            skipped_count = 0
            error_count = 0
            
            # Progress logging for large batches
            progress_interval = max(100, len(pipeline_triples) // 10)  # Log every 10% or every 100 triples
            if len(pipeline_triples) > 500:
                print(f"   ‚è≥ Processing {len(pipeline_triples)} triples (this may take a moment for large files)...")
            
            for idx, triple in enumerate(pipeline_triples):
                # Progress logging
                if idx > 0 and idx % progress_interval == 0:
                    print(f"   üìä Processing progress: {idx}/{len(pipeline_triples)} triples ({added_count} added, {skipped_count} skipped)")
                try:
                    # Handle formats: 
                    # - 8 elements: (subject, predicate, object, details, confidence, type, source, uploaded)
                    # - 6 elements: (subject, predicate, object, details, confidence, is_inferred) - legacy
                    # - 5 elements: (subject, predicate, object, details, confidence) or (s, p, o, d, is_inferred)
                    # - 4 elements: (subject, predicate, object, details) - legacy
                    if len(triple) == 8:
                        subject, predicate, object_val, details, confidence, fact_type, source_doc, uploaded = triple
                        is_inferred = (fact_type == "inferred")
                        # Use source and uploaded from tuple if provided
                        if source_doc:
                            source_document = source_doc
                        if uploaded:
                            uploaded_at = uploaded
                    elif len(triple) == 6:
                        # Could be (s, p, o, d, confidence, is_inferred) or (s, p, o, d, confidence, type)
                        # Check if 6th element is boolean (is_inferred) or string (type)
                        if isinstance(triple[5], bool):
                            subject, predicate, object_val, details, confidence, is_inferred = triple
                        else:
                            # It's type string
                            subject, predicate, object_val, details, confidence, fact_type = triple
                            is_inferred = (fact_type == "inferred")
                    elif len(triple) == 5:
                        # Could be (s, p, o, d, confidence) or (s, p, o, d, is_inferred)
                        # Check if 5th element is boolean (is_inferred) or float (confidence)
                        if isinstance(triple[4], bool):
                            subject, predicate, object_val, details, is_inferred = triple
                            confidence = 0.7  # Default confidence
                        else:
                            subject, predicate, object_val, details, confidence = triple
                            is_inferred = False
                    else:
                        # Legacy format: (subject, predicate, object, details)
                        subject, predicate, object_val, details = triple[:4]
                        is_inferred = False  # Default to not inferred for legacy format
                        confidence = 0.7  # Default confidence for legacy format
                    
                    # Check if fact already exists (exact match)
                    fact_already_exists = fact_exists(subject, predicate, object_val)
                    
                    # ENHANCED: Also check for semantically equivalent facts with different predicates
                    # For example, "Queen is united kingdom" and "Queen is_from united kingdom" are the same
                    equivalent_fact = None
                    equivalent_predicate = None
                    if not fact_already_exists:
                        # Check if there's a semantically equivalent fact with a different predicate
                        # Map of less specific -> more specific predicates
                        predicate_equivalences = {
                            "is": ["is_from", "is_named", "is_in"],  # "is" can be equivalent to these
                        }
                        
                        # Check if current predicate is more specific than an existing fact
                        if predicate in ["is_from", "is_named", "is_in"]:
                            # Check if there's an existing fact with "is" predicate
                            if fact_exists(subject, "is", object_val):
                                # Found equivalent fact with less specific predicate
                                equivalent_fact = (subject, "is", object_val)
                                equivalent_predicate = predicate  # Use the more specific one
                                fact_already_exists = True  # Treat as existing fact
                        
                        # Check if current predicate is less specific than an existing fact
                        elif predicate == "is":
                            # Check if there's an existing fact with a more specific predicate
                            for more_specific in ["is_from", "is_named", "is_in"]:
                                if fact_exists(subject, more_specific, object_val):
                                    # Found equivalent fact with more specific predicate
                                    equivalent_fact = (subject, more_specific, object_val)
                                    equivalent_predicate = more_specific  # Use the more specific one
                                    fact_already_exists = True  # Treat as existing fact
                                    break
                    
                    if not fact_already_exists:
                        # Add to graph (only if fact doesn't exist)
                        subject_clean = str(subject).strip().replace(' ', '_')
                        predicate_clean = str(predicate).strip().replace(' ', '_')
                        object_clean = str(object_val).strip()
                        
                        subject_uri = rdflib.URIRef(f"urn:entity:{quote(subject_clean, safe='')}")
                        predicate_uri = rdflib.URIRef(f"urn:predicate:{quote(predicate_clean, safe='')}")
                        
                        graph.add((subject_uri, predicate_uri, rdflib.Literal(object_clean)))
                        
                        # Store details if provided (only update if not already set)
                        if details:
                            existing_details = get_fact_details(subject, predicate, object_val)
                            if not existing_details:
                                add_fact_details(subject, predicate, object_val, details)
                        
                        # Store confidence score (only update if not already set)
                        existing_confidence = get_fact_confidence(subject, predicate, object_val)
                        if existing_confidence is None or existing_confidence == 0.7:  # Default confidence
                            add_fact_confidence(subject, predicate, object_val, confidence)
                        
                        # Mark as inferred or not inferred based on pipeline flag
                        # Always update to ensure inferred facts are correctly marked
                        add_fact_is_inferred(subject, predicate, object_val, is_inferred=is_inferred)
                        
                        # CRITICAL: Always store source document for new facts
                        # This ensures all facts have source tracking
                        add_fact_source_document(subject, predicate, object_val, source_document, uploaded_at)
                        
                        # Store agent_id if provided
                        if agent_id:
                            add_fact_agent_id(subject, predicate, object_val, agent_id)
                        
                        added_count += 1
                    else:
                        skipped_count += 1
                    # Even if fact exists, update inferred status if this is an inferred fact
                    # This ensures inferred facts are correctly marked
                    if is_inferred:
                        add_fact_is_inferred(subject, predicate, object_val, is_inferred=True)
                    # If we found an equivalent fact with a different predicate, merge them
                    if equivalent_fact and equivalent_predicate:
                        # The equivalent fact exists with a different predicate
                        # We need to merge them: remove the less specific fact and keep the more specific one
                        eq_subject, eq_predicate, eq_object = equivalent_fact
                        
                        # Get all sources from the existing (less specific) fact
                        existing_sources = get_fact_source_document(eq_subject, eq_predicate, eq_object)
                        
                        # Get other metadata from the existing fact
                        existing_details = get_fact_details(eq_subject, eq_predicate, eq_object)
                        existing_confidence = get_fact_confidence(eq_subject, eq_predicate, eq_object)
                        existing_is_inferred = get_fact_is_inferred(eq_subject, eq_predicate, eq_object)
                        
                        # Remove the less specific fact from the graph
                        # Find and remove the main triple
                        eq_subject_clean = str(eq_subject).strip().replace(' ', '_')
                        eq_predicate_clean = str(eq_predicate).strip().replace(' ', '_')
                        eq_object_clean = str(eq_object).strip()
                        
                        eq_subject_uri = rdflib.URIRef(f"urn:entity:{quote(eq_subject_clean, safe='')}")
                        eq_predicate_uri = rdflib.URIRef(f"urn:predicate:{quote(eq_predicate_clean, safe='')}")
                        
                        # Remove the main triple
                        graph.remove((eq_subject_uri, eq_predicate_uri, rdflib.Literal(eq_object_clean)))
                        
                        # Remove all metadata triples for the old fact
                        eq_fact_id = f"{eq_subject}|{eq_predicate}|{eq_object}"
                        eq_fact_id_clean = eq_fact_id.strip().replace(' ', '_')
                        eq_fact_id_uri = rdflib.URIRef(f"urn:fact:{quote(eq_fact_id_clean, safe='')}")
                        
                        triples_to_remove = []
                        for s, p, o in graph:
                            if str(s) == str(eq_fact_id_uri):
                                triples_to_remove.append((s, p, o))
                        for triple in triples_to_remove:
                            graph.remove(triple)
                        
                        # Add the more specific fact
                        subject_clean = str(subject).strip().replace(' ', '_')
                        predicate_clean = str(equivalent_predicate).strip().replace(' ', '_')
                        object_clean = str(object_val).strip()
                        
                        subject_uri = rdflib.URIRef(f"urn:entity:{quote(subject_clean, safe='')}")
                        predicate_uri = rdflib.URIRef(f"urn:predicate:{quote(predicate_clean, safe='')}")
                        
                        graph.add((subject_uri, predicate_uri, rdflib.Literal(object_clean)))
                        
                        # Transfer metadata from old fact to new fact
                        if existing_details:
                            add_fact_details(subject, equivalent_predicate, object_val, existing_details)
                        elif details:
                            add_fact_details(subject, equivalent_predicate, object_val, details)
                        
                        # Use the higher confidence
                        final_confidence = max(existing_confidence, confidence)
                        add_fact_confidence(subject, equivalent_predicate, object_val, final_confidence)
                        
                        # Transfer inferred status - prefer existing if it's True (inferred), otherwise use new
                        if existing_is_inferred:
                            add_fact_is_inferred(subject, equivalent_predicate, object_val, is_inferred=True)
                        else:
                            add_fact_is_inferred(subject, equivalent_predicate, object_val, is_inferred=is_inferred)
                        
                        # Transfer all sources from the old fact to the new fact
                        for old_source, old_timestamp in existing_sources:
                            add_fact_source_document(subject, equivalent_predicate, object_val, old_source, old_timestamp)
                        
                        # Add the new source
                        add_fact_source_document(subject, equivalent_predicate, object_val, source_document, uploaded_at)
                        
                        # Store agent_id if provided
                        if agent_id:
                            add_fact_agent_id(subject, equivalent_predicate, object_val, agent_id)
                        
                        # Update counts
                        skipped_count -= 1  # Don't count as skipped since we merged
                        added_count += 1  # Count as added since we created the merged fact
                    else:
                        # Regular duplicate - just add the source
                        # But also update inferred status if this is an inferred fact
                        if is_inferred:
                            add_fact_is_inferred(subject, predicate, object_val, is_inferred=True)
                        add_fact_source_document(subject, predicate, object_val, source_document, uploaded_at)
            
                        # Store agent_id if provided (even for duplicates)
                        if agent_id:
                            add_fact_agent_id(subject, predicate, object_val, agent_id)
                except Exception as triple_error:
                    error_count += 1
                    if error_count <= 5:  # Only log first 5 errors to avoid spam
                        print(f"‚ö†Ô∏è  Error processing triple {idx + 1}/{len(pipeline_triples)}: {triple_error}")
                        print(f"   Triple: {triple[:100] if isinstance(triple, (list, tuple)) else str(triple)[:100]}")
                        if error_count == 5:
                            print(f"   ... (suppressing further error messages)")
                    continue
            
            if error_count > 0:
                print(f"‚ö†Ô∏è  Processed {len(pipeline_triples)} triples: {added_count} added, {skipped_count} skipped, {error_count} errors")
            else:
                print(f"‚úÖ Processed {len(pipeline_triples)} triples: {added_count} added, {skipped_count} skipped")
            
            print(f"üíæ Saving knowledge graph to disk...")
            save_knowledge_graph()
            print(f"‚úÖ Knowledge graph saved successfully")
            
            if skipped_count > 0:
                return f"pipeline\n Added {added_count} new triples, skipped {skipped_count} duplicates. Total facts stored: {len(graph)}.\n Saved"
            return f"pipeline\n Added {added_count} new triples. Total facts stored: {len(graph)}.\n Saved"
    except ImportError:
        print("‚ö†Ô∏è  Pipeline module not found, using legacy extraction")
    except Exception as e:
        print(f"‚ö†Ô∏è  Pipeline extraction failed: {e}, falling back to legacy methods")
        import traceback
        traceback.print_exc()
    
    # Fallback to legacy extraction methods
    # Track which extraction method was used
    extraction_method = "regex"
    improved_count = 0
    triplex_count = 0
    regex_count = 0
    
    # OPTIMIZED: Combine fast extraction methods for comprehensive coverage
    # Strategy: Run both improved patterns AND context extraction (both fast, catch different patterns)
    # Only skip Triplex (slow) if we already have good results
    
    improved_triples = []
    triplex_triples = []
    new_triples_with_context = []
    new_triples = []
    
    # Always run improved pattern extraction (fast, catches capitalized entities)
    try:
        improved_triples = extract_with_improved_patterns(text)
        if improved_triples:
            improved_count = len(improved_triples)
            print(f"‚úÖ Improved patterns extracted {improved_count} triples")
        else:
            improved_triples = []
    except Exception as e:
        print(f"‚ö†Ô∏è  Improved pattern extraction failed: {e}")
        improved_triples = []
    
    # Always run context extraction (fast, catches lowercase/case-insensitive patterns)
    # This complements improved patterns by finding different types of relationships
    try:
        new_triples_with_context = extract_triples_with_context(text)
        if new_triples_with_context:
            print(f"‚úÖ Context extraction found {len(new_triples_with_context)} triples")
        else:
            new_triples_with_context = []
    except Exception as e:
        print(f"‚ö†Ô∏è  Context extraction failed: {e}")
        new_triples_with_context = []
    
    # Only try Triplex if we didn't find much from fast methods AND Triplex is enabled
    # Triplex is slow, so skip if we already have good results
    total_fast_triples = len(improved_triples) + len(new_triples_with_context)
    if total_fast_triples < 5 and USE_TRIPLEX and TRIPLEX_AVAILABLE:
        try:
            triplex_triples = extract_with_triplex(text)
            if triplex_triples and len(triplex_triples) > 0:
                triplex_count = len(triplex_triples)
                extraction_method = "triplex"
                print(f"‚úÖ TRIPLEX: Extracted {triplex_count} triples using LLM")
            else:
                triplex_triples = []
        except Exception as e:
            print(f"‚ö†Ô∏è  Triplex extraction failed: {e}")
            triplex_triples = []
    
    # Final fallback: regular triples only if nothing else worked
    if not improved_triples and not triplex_triples and not new_triples_with_context:
        try:
            new_triples = extract_triples(text)
            if new_triples:
                print(f"‚úÖ Fallback extraction found {len(new_triples)} triples")
        except Exception as e:
            print(f"‚ö†Ô∏è  Fallback extraction failed: {e}")
            new_triples = []
    
    # Determine extraction method for reporting
    if triplex_triples:
        extraction_method = "triplex"
    elif improved_triples and new_triples_with_context:
        extraction_method = "improved_patterns+context"
    elif improved_triples:
        extraction_method = "improved_patterns"
    elif new_triples_with_context:
        extraction_method = "context"
    else:
        extraction_method = "regex"
    
    added_count = 0
    skipped_count = 0
    
    # Track which facts we've added to avoid duplicates
    added_facts = set()
    
    # Process improved pattern triples first (if available) - enhanced regex
    for s, p, o, details in improved_triples:
        fact_key = (s.lower().strip(), p.lower().strip(), o.lower().strip())
        
        # Check if fact already exists before adding
        fact_already_exists = fact_exists(s, p, o) or fact_key in added_facts
        
        if not fact_already_exists:
            # Properly encode URIs like create_fact_endpoint does
            subject_clean = str(s).strip().replace(' ', '_')
            predicate_clean = str(p).strip().replace(' ', '_')
            object_value = str(o).strip()
            
            # Create URIs (encode spaces to avoid RDFLib warnings)
            subject_uri = rdflib.URIRef(f"urn:{quote(subject_clean, safe='')}")
            predicate_uri = rdflib.URIRef(f"urn:{quote(predicate_clean, safe='')}")
            object_literal = rdflib.Literal(object_value)
            
            graph.add((subject_uri, predicate_uri, object_literal))
            
            # Store details if provided (original context)
            if details and details.strip():
                add_fact_details(s, p, o, details)
            
            added_facts.add(fact_key)
            added_count += 1
        else:
            skipped_count += 1
        
        # ENHANCED: Always add source document, even if fact already exists
        # This allows tracking multiple sources for the same fact
        add_fact_source_document(s, p, o, source_document, uploaded_at)
        
        # Store agent_id if provided
        if agent_id:
            add_fact_agent_id(s, p, o, agent_id)
    
    # Process Triplex triples (if available) - these are highest quality
    for s, p, o, details in triplex_triples:
        fact_key = (s.lower().strip(), p.lower().strip(), o.lower().strip())
        
        # Check if fact already exists before adding
        fact_already_exists = fact_exists(s, p, o) or fact_key in added_facts
        
        if not fact_already_exists:
            # Properly encode URIs like create_fact_endpoint does
            subject_clean = str(s).strip().replace(' ', '_')
            predicate_clean = str(p).strip().replace(' ', '_')
            object_value = str(o).strip()
            
            # Create URIs (encode spaces to avoid RDFLib warnings)
            subject_uri = rdflib.URIRef(f"urn:{quote(subject_clean, safe='')}")
            predicate_uri = rdflib.URIRef(f"urn:{quote(predicate_clean, safe='')}")
            object_literal = rdflib.Literal(object_value)
            
            graph.add((subject_uri, predicate_uri, object_literal))
            
            # Store details if provided (original context)
            if details and details.strip():
                add_fact_details(s, p, o, details)
            
            added_facts.add(fact_key)
            added_count += 1
        else:
            skipped_count += 1
        
        # ENHANCED: Always add source document, even if fact already exists
        # This allows tracking multiple sources for the same fact
        add_fact_source_document(s, p, o, source_document, uploaded_at)
        
        # Store agent_id if provided
        if agent_id:
            add_fact_agent_id(s, p, o, agent_id)
    
    # Process triples with context (regex-based with entity cleaning)
    for s, p, o, details in new_triples_with_context:
        fact_key = (s.lower().strip(), p.lower().strip(), o.lower().strip())
        
        # Check if fact already exists before adding
        if fact_exists(s, p, o) or fact_key in added_facts:
            skipped_count += 1
            continue
        
        # Properly encode URIs like create_fact_endpoint does
        # Replace spaces with underscores and URL-encode to avoid RDFLib warnings
        subject_clean = str(s).strip().replace(' ', '_')
        predicate_clean = str(p).strip().replace(' ', '_')
        object_value = str(o).strip()
        
        # Create URIs (encode spaces to avoid RDFLib warnings)
        subject_uri = rdflib.URIRef(f"urn:{quote(subject_clean, safe='')}")
        predicate_uri = rdflib.URIRef(f"urn:{quote(predicate_clean, safe='')}")
        object_literal = rdflib.Literal(object_value)
        
        graph.add((subject_uri, predicate_uri, object_literal))
        
        # Store details if provided (original context)
        if details and details.strip():
            add_fact_details(s, p, o, details)
        
        # Store source document and timestamp
        add_fact_source_document(s, p, o, source_document, uploaded_at)
        
        # Store agent_id if provided
        if agent_id:
            add_fact_agent_id(s, p, o, agent_id)
        
        added_facts.add(fact_key)
        added_count += 1
    
    # Process regular triples as fallback, but clean entities and store context
    regex_triples_processed = 0
    for s, p, o in new_triples:
        fact_key = (s.lower().strip(), p.lower().strip(), o.lower().strip())
        
        # Skip if we already added this fact from context extraction
        if fact_key in added_facts:
            continue
        
        # Check if fact already exists before adding
        fact_already_exists = fact_exists(s, p, o)
        
        if not fact_already_exists:
            # Extract core entities even for fallback triples (handles dates, times, descriptive text)
            original_subject = s
            original_object = o
            core_subject, subject_details = extract_core_entity(str(s), "")
            core_object, object_details = extract_core_entity(str(o), "")
            
            # Normalize entities (handles abbreviations, aliases, case-insensitive)
            core_subject = normalize_entity(core_subject)
            core_object = normalize_entity(core_object)
            
            # Use core entities if they're different and meaningful
            if core_subject and core_object and len(core_subject) > 2 and len(core_object) > 2:
                # Combine details from subject and object
                details_parts = []
                if subject_details:
                    details_parts.append(f"Subject: {subject_details}")
                if object_details:
                    details_parts.append(f"Object: {object_details}")
                if not details_parts and (original_subject != core_subject or original_object != core_object):
                    details_parts.append(f"{original_subject} {p} {original_object}")
                
                details = ' | '.join(details_parts) if details_parts else None
                s, o = core_subject, core_object
            else:
                # If extraction didn't work well, normalize and use original
                s = normalize_entity(str(s))
                o = normalize_entity(str(o))
                details = None
            
            # Properly encode URIs like create_fact_endpoint does
            subject_clean = str(s).strip().replace(' ', '_')
            predicate_clean = str(p).strip().replace(' ', '_')
            object_value = str(o).strip()
            
            # Create URIs (encode spaces to avoid RDFLib warnings)
            subject_uri = rdflib.URIRef(f"urn:{quote(subject_clean, safe='')}")
            predicate_uri = rdflib.URIRef(f"urn:{quote(predicate_clean, safe='')}")
            object_literal = rdflib.Literal(object_value)
            
            graph.add((subject_uri, predicate_uri, object_literal))
            
            # Store details if we have original context
            if details and details.strip():
                add_fact_details(s, p, o, details)
            
            added_facts.add(fact_key)
            added_count += 1
        else:
            skipped_count += 1
        
        # ENHANCED: Always add source document, even if fact already exists
        # This allows tracking multiple sources for the same fact
        add_fact_source_document(s, p, o, source_document, uploaded_at)
        
        # Store agent_id if provided
        if agent_id:
            add_fact_agent_id(s, p, o, agent_id)
        regex_triples_processed += 1
    
    # Count triples by method (only count what was actually added)
    triplex_added = sum(1 for s, p, o, _ in triplex_triples if (s.lower().strip(), p.lower().strip(), o.lower().strip()) in added_facts)
    improved_added = sum(1 for s, p, o, _ in improved_triples if (s.lower().strip(), p.lower().strip(), o.lower().strip()) in added_facts)
    context_added = sum(1 for s, p, o, _ in new_triples_with_context if (s.lower().strip(), p.lower().strip(), o.lower().strip()) in added_facts)
    regex_count = added_count - improved_added - triplex_added - context_added
    
    # Save graph to disk
    save_knowledge_graph()
    
    # Verify save worked
    import os
    if os.path.exists("knowledge_graph.pkl"):
        file_size = os.path.getsize("knowledge_graph.pkl")
        print(f"‚úÖ Graph saved - file size: {file_size} bytes, facts in memory: {len(graph)}")
    
    # Build status message with extraction method info
    method_info = f"[{extraction_method.upper()}]"
    if extraction_method == "triplex":
        method_info = f"ü§ñ [TRIPLEX LLM] - {triplex_added} triples from LLM"
    elif extraction_method == "improved_patterns+context":
        method_info = f"‚úÖ [IMPROVED PATTERNS + CONTEXT] - {improved_added} from patterns, {context_added} from context"
    elif extraction_method == "improved_patterns":
        method_info = f"‚ú® [IMPROVED PATTERNS] - {improved_added} triples from enhanced extraction"
    elif extraction_method == "context":
        method_info = f"‚úÖ [CONTEXT EXTRACTION] - {context_added} triples"
    elif "improved" in extraction_method.lower() and "regex" in extraction_method.lower():
        method_info = f"‚ú® [IMPROVED/REGEX] - {improved_added} from patterns, {regex_count} from regex"
    elif "triplex" in extraction_method.lower():
        method_info = f"‚ö†Ô∏è  [FALLBACK] - {extraction_method}"
    else:
        method_info = f"üìù [REGEX] - {regex_count} triples from regex patterns"
    
    if skipped_count > 0:
        return f"{method_info}\n Added {added_count} new triples, skipped {skipped_count} duplicates. Total facts stored: {len(graph)}.\n Saved"
    return f"{method_info}\n Added {added_count} new triples. Total facts stored: {len(graph)}.\n Saved"

def retrieve_context(question, limit=None):
    """
    Retrieve relevant context from knowledge graph for answering questions.
    Uses improved semantic matching and includes details for better context.
    
    Args:
        question: The question to retrieve context for
        limit: Maximum number of facts to return (default: 100 for large graphs, unlimited for small)
    """
    from urllib.parse import unquote
    from knowledge import get_fact_details
    
    # Determine limit EARLY to prevent processing all facts
    if limit is None:
        graph_size = len(graph) if graph else 0
        if graph_size > 5000:  # Large document (1000+ rows typically = 5000+ facts)
            limit = 500  # Limit to top 100 most relevant facts
        elif graph_size > 2000:  # Medium document
            limit = 500
        else:  # Small document
            limit = 500  # Still limit to prevent issues
    
    # For very large graphs, use early exit to avoid processing all facts
    early_exit_threshold = limit * 3 if graph_size > 10000 else None  # Get 3x candidates, then sort and take top N
    
    # Extract meaningful keywords from question (remove stopwords)
    stopwords = {
        'the','a','an','and','or','but','in','on','at','to','for','of','with','by',
        'is','are','was','were','be','been','have','has','had','do','does','did',
        'will','would','could','should','may','might','can','what','how','when',
        'where','why','who','which','this','that','these','those','tell','me','about',
        'show','give','explain','describe','list','from','there','here'
    }
    
    # Extract keywords - keep words longer than 2 chars and not stopwords
    # Make everything case-insensitive
    qwords = [w.lower().strip() for w in question.split() 
              if w.lower().strip() not in stopwords and len(w.strip()) > 2]
    
    # Also extract potential entity names (capitalized words, acronyms) - but convert to lowercase for matching
    entities = [w.lower() for w in question.split() if (w[0].isupper() or w.isupper()) and len(w) > 1]
    qwords.extend(entities)
    
    # Add operational/strategic query keywords to help find insights
    insight_keywords = ['operational', 'strategic', 'analysis', 'insight', 'monitoring', 'tracking', 
                       'performance', 'engagement', 'absence', 'department', 'manager', 'team']
    for keyword in insight_keywords:
        if keyword in question.lower():
            qwords.append(keyword)
    
    # Remove duplicates
    qwords = list(set(qwords))
    
    if not qwords:
        # If no meaningful words, use the whole question (excluding very short words)
        qwords = [w.lower() for w in question.split() if len(w) > 2]
    
    scored_matches = []
    facts_processed = 0
    max_facts_to_check = None
    
    # For very large graphs, limit how many facts we check
    graph_size = len(graph) if graph else 0
    if graph_size > 10000:
        # Only check first 20,000 facts (enough to find top matches)
        max_facts_to_check = 20000
        print(f"üìä Large graph detected ({graph_size} facts), limiting check to first {max_facts_to_check} facts")
    elif graph_size > 5000:
        max_facts_to_check = 10000
        print(f"üìä Medium-large graph ({graph_size} facts), limiting check to first {max_facts_to_check} facts")
    
    for s, p, o in graph:
        # Early exit if we've checked enough facts
        if max_facts_to_check and facts_processed >= max_facts_to_check:
            break
        facts_processed += 1
        # Skip metadata triples
        predicate_str = str(p)
        if ('fact_subject' in predicate_str or 'fact_predicate' in predicate_str or 
            'fact_object' in predicate_str or 'has_details' in predicate_str or 
            'source_document' in predicate_str or 'uploaded_at' in predicate_str or
            'is_inferred' in predicate_str or 'confidence' in predicate_str):
            continue
        
        # Extract subject from URI (format: urn:entity:subject or urn:subject)
        subject_uri_str = str(s)
        if 'urn:entity:' in subject_uri_str:
            subject = subject_uri_str.split('urn:entity:')[-1]
        elif 'urn:' in subject_uri_str:
            subject = subject_uri_str.split('urn:')[-1]
        else:
            subject = subject_uri_str
        subject = unquote(subject).replace('_', ' ')
        
        # Extract predicate from URI (format: urn:predicate:predicate or urn:predicate)
        predicate_uri_str = str(p)
        if 'urn:predicate:' in predicate_uri_str:
            predicate = predicate_uri_str.split('urn:predicate:')[-1]
        elif 'urn:' in predicate_uri_str:
            predicate = predicate_uri_str.split('urn:')[-1]
        else:
            predicate = predicate_uri_str
        predicate = unquote(predicate).replace('_', ' ')
        
        # Object is already a literal
        object_val = str(o)
        
        # Build fact text for matching
        fact_text = f"{subject} {predicate} {object_val}".lower()
        
        # Boost score for operational/strategic insights - check source document
        source_doc = None
        try:
            # Try to get source document from metadata
            fact_id = f"urn:fact:{subject}|{predicate}|{object_val}"
            for s2, p2, o2 in graph:
                if str(s2) == fact_id and 'source_document' in str(p2).lower():
                    source_doc = str(o2).lower()
                    break
        except:
            pass
        
        # Check if this fact is from operational_insights source
        is_operational_insight = (source_doc and 'operational_insights' in source_doc)
        is_insight = ('operational' in fact_text or 'strategic' in fact_text or 
                     'operational_insights' in str(s).lower() or 'strategic_insights' in str(s).lower() or
                     'operational_insights' in str(p).lower() or 'strategic_insights' in str(p).lower() or
                     (source_doc and ('operational' in source_doc or 'strategic' in source_doc or 'insight' in source_doc)))
        
        # Calculate relevance score with improved matching
        # Strong boost for operational insights (especially when query contains "operational")
        if is_operational_insight and ('operational' in question.lower() or 'absence' in question.lower() or 'engagement' in question.lower() or 'top' in question.lower() or 'bottom' in question.lower()):
            score = 20  # Very strong boost for operational insights when query matches
        elif is_insight:
            score = 10  # Strong boost for insights
        else:
            score = 0
        
        # Exact word matches
        for word in qwords:
            word_lower = word.lower()
            
            # Check if word appears in any part of the fact
            if word_lower in fact_text:
                # Base score for any match
                score += 1
                
                # Higher scores for matches in important positions
                if word_lower in subject.lower():
                    score += 5  # Subject matches are most important
                if word_lower in predicate.lower():
                    score += 3  # Predicate matches are important
                if word_lower in object_val.lower():
                    score += 2  # Object matches are relevant
                
                # Bonus for exact word match (not substring)
                if f" {word_lower} " in f" {fact_text} ":
                    score += 2
        
        # Partial/substring matches (for abbreviations, partial words)
        for word in qwords:
            if len(word) > 3:  # Only for longer words
                if word in subject.lower():
                    score += 2
                if word in object_val.lower():
                    score += 1
        
        # Entity name matching (case-insensitive, handles acronyms)
        for entity in entities:
            # entity is already lowercase from extraction
            if entity in subject.lower() or entity in object_val.lower():
                score += 4  # Entity matches are very relevant
            # Also check if entity is part of a word (for acronyms)
            if len(entity) > 2:
                if entity in fact_text:
                    score += 2
        
        # Include facts with ANY word match (score > 0 means at least one word matched)
        if score > 0:
            # Track where matches were found (more detailed)
            match_locations = []
            matched_words = []
            for word in qwords:
                word_lower = word.lower()
                if word_lower in subject.lower():
                    if "subject" not in match_locations:
                        match_locations.append("subject")
                    matched_words.append(f"{word} (in subject)")
                if word_lower in predicate.lower():
                    if "predicate" not in match_locations:
                        match_locations.append("predicate")
                    matched_words.append(f"{word} (in predicate)")
                if word_lower in object_val.lower():
                    if "object" not in match_locations:
                        match_locations.append("object")
                    matched_words.append(f"{word} (in object)")
            
            # Create match location string
            if match_locations:
                match_str = f"[Matched in: {', '.join(match_locations)}]"
            else:
                match_str = "[Match found]"
            
            # Get details for richer context
            details = get_fact_details(subject, predicate, object_val)
            
            # Get ALL source documents (not just the first one)
            all_sources = get_fact_source_document(subject, predicate, object_val)
            
            # Build fact description with match location, details, and ALL sources
            fact_desc = f"{match_str} {subject} ‚Üí {predicate} ‚Üí {object_val}"
            if details:
                fact_desc += f" | Details: {details}"
            
            # Format all sources - ALWAYS include source information
            if all_sources:
                source_list = []
                for source_doc, uploaded_at in all_sources:
                    if source_doc and source_doc.strip():
                        if uploaded_at and uploaded_at.strip():
                            # Format timestamp nicely
                            try:
                                from datetime import datetime
                                # Handle different timestamp formats
                                timestamp_str = uploaded_at
                                if 'T' in uploaded_at or ' ' in uploaded_at:
                                    dt = datetime.fromisoformat(uploaded_at.replace('Z', '+00:00'))
                                    timestamp_str = dt.strftime('%Y-%m-%d %H:%M')
                            except Exception as e:
                                # If parsing fails, use original timestamp
                                timestamp_str = uploaded_at
                            source_list.append(f"{source_doc} (uploaded: {timestamp_str})")
                        else:
                            source_list.append(source_doc)
                if source_list:
                    if len(source_list) == 1:
                        fact_desc += f" | Source: {source_list[0]}"
                    else:
                        fact_desc += f" | Sources: {', '.join(source_list)}"
            else:
                # If no sources found, try to get from fact_id_uri metadata
                # This is a fallback for facts that might not have sources stored yet
                fact_desc += " | Source: unknown"
            
            scored_matches.append((score, fact_desc, subject, predicate, object_val, match_locations, matched_words))
            
            # Early exit optimization: if we have enough high-scoring matches, stop iterating
            # Check every 1000 facts if we can exit early
            if facts_processed % 1000 == 0 and len(scored_matches) >= limit * 2:
                # If we have 2x the limit in matches, we likely have enough good ones
                # Sort what we have so far and check if top N are high-scoring
                temp_sorted = sorted(scored_matches, key=lambda x: x[0], reverse=True)
                if len(temp_sorted) >= limit and temp_sorted[limit-1][0] >= 5:  # Top N have score >= 5
                    print(f"üìä Early exit: Found {len(scored_matches)} matches after checking {facts_processed} facts")
                    break
    
    # Sort by score (highest first) - but return ALL matches, not just top N
    scored_matches.sort(key=lambda x: x[0], reverse=True)
    
    # Remove duplicates (same fact with different scores) - but return ALL unique matches
    seen_facts = set()
    unique_matches = []
    # Group facts by document source for better organization
    facts_by_document = {}
    
    for match_data in scored_matches:
        # Handle both old format (5-7 items) and new format
        if len(match_data) >= 6:
            score, fact_desc, subj, pred, obj, match_locs = match_data[:6]
        else:
            score, fact_desc, subj, pred, obj = match_data[:5]
            match_locs = []
        
        fact_key = (subj.lower(), pred.lower(), obj.lower())
        if fact_key not in seen_facts:
            seen_facts.add(fact_key)
            unique_matches.append(fact_desc)
            
            # Extract document source from fact_desc for grouping
            if "Source:" in fact_desc or "Sources:" in fact_desc:
                # Extract source document name
                source_part = fact_desc.split("Source")[-1].split("|")[-1].strip()
                if source_part and source_part != "unknown":
                    # Extract document name (before "uploaded:" if present)
                    doc_name = source_part.split("(uploaded:")[0].strip()
                    if doc_name:
                        if doc_name not in facts_by_document:
                            facts_by_document[doc_name] = []
                        facts_by_document[doc_name].append(fact_desc)
            # Limit already determined at function start, no need to recalculate
    
    # Apply limit to unique_matches
    if limit and len(unique_matches) > limit:
        unique_matches = unique_matches[:limit]
        print(f"üìä Limiting context to top {limit} most relevant facts (out of {len(scored_matches)} matches) to prevent LLM timeout")
    
    if unique_matches:
        # If we have multiple documents, organize by document
        if len(facts_by_document) > 1:
            total_shown = 0
            result = f"**Relevant Knowledge from Your Documents ({len(unique_matches)} facts found across {len(facts_by_document)} documents):**\n\n"
            for doc_name, doc_facts in facts_by_document.items():
                if total_shown >= limit:
                    break
                result += f"**From {doc_name}:**\n"
                doc_limit = min(10, limit - total_shown)
                for i, fact in enumerate(doc_facts[:doc_limit], 1):
                    result += f"  {i}. {fact}\n"
                    total_shown += 1
                if len(doc_facts) > doc_limit:
                    result += f"  ... and {len(doc_facts) - doc_limit} more facts from this document\n"
                result += "\n"
            
            # Add remaining facts that weren't grouped
            if total_shown < limit:
                remaining = [f for f in unique_matches if f not in [fact for facts in facts_by_document.values() for fact in facts]]
                if remaining:
                    result += "**Other relevant facts:**\n"
                    remaining_limit = min(20, limit - total_shown)
                    for i, match in enumerate(remaining[:remaining_limit], 1):
                        result += f"{i}. {match}\n"
        else:
            # Single document or no grouping - show limited facts
            result = f"**Relevant Knowledge from Your Documents ({len(unique_matches)} facts found):**\n\n"
            for i, match in enumerate(unique_matches[:limit] if limit else unique_matches, 1):
                result += f"{i}. {match}\n"
        return result
    
    # If no matches at all, return helpful message
    return "**No directly relevant facts found in the knowledge base.**\n\nTry asking about topics that might be in your knowledge base, or add more knowledge by uploading documents or adding text."

def show_graph_contents():
    if len(graph) == 0:
        return "**Knowledge Graph Status: EMPTY**\n\n**How to build your knowledge base:**\n1. **Add text directly** - Paste any text in the 'Add Knowledge from Text' box above\n2. **Upload documents** - Use the file upload to process PDF, DOCX, TXT, CSV files\n3. **Extract facts** - The system will automatically extract knowledge from your content\n4. **Build knowledge** - Add more text or files to expand your knowledge base\n5. **Save knowledge** - Use 'Save Knowledge' to persist your data\n\n**Start by adding some text or uploading a document!**"
    facts_by_subject = {}
    all_facts = []
    for s, p, o in graph:
        subject = str(s).split(':')[-1] if ':' in str(s) else str(s)
        predicate = str(p).split(':')[-1] if ':' in str(p) else str(p)
        object_val = str(o)
        fact_text = f"{subject} {predicate} {object_val}"
        all_facts.append(fact_text)
        facts_by_subject.setdefault(subject, []).append(f"{predicate} {object_val}")
    result = f"**Knowledge Graph Overview**\n"
    result += f"**Total Facts:** {len(graph)}\n"
    result += f"**Unique Subjects:** {len(facts_by_subject)}\n\n"
    result += "## **Knowledge by Subject:**\n\n"
    for i, (subject, facts) in enumerate(facts_by_subject.items()):
        if i >= 10:
            remaining = len(facts_by_subject) - 10
            result += f"... and {remaining} more subjects\n"
            break
        result += f"**{subject}:**\n"
        for fact in facts:
            result += f"  ‚Ä¢ {fact}\n"
        result += "\n"
    result += "## **All Facts:**\n\n"
    for i, fact in enumerate(all_facts[:20]):
        result += f"{i+1}. {fact}\n"
    if len(all_facts) > 20:
        result += f"\n... and {len(all_facts) - 20} more facts"
    return result

def visualize_knowledge_graph():
    if len(graph) == 0:
        return "<p>No knowledge in graph. Add some text or upload a document first!</p>"
    try:
        G = nx.Graph()
        fact_data = {}
        for s, p, o in graph:
            subject = str(s).split(':')[-1] if ':' in str(s) else str(s)
            predicate = str(p).split(':')[-1] if ':' in str(p) else str(p)
            object_val = str(o)
            subject_short = (subject[:30] + "...") if len(subject) > 30 else subject
            object_short = (object_val[:30] + "...") if len(object_val) > 30 else object_val
            if subject not in G:
                G.add_node(subject, display=subject_short, node_type='subject')
            if object_val not in G:
                G.add_node(object_val, display=object_short, node_type='object')
            G.add_edge(subject, object_val, label=predicate)
            fact_data[(subject, object_val)] = f"{subject} {predicate} {object_val}"
        pos = nx.spring_layout(G, k=2, iterations=100, seed=42)
        import numpy as np
        x_positions = [pos[n][0] for n in G.nodes()]
        y_positions = [pos[n][1] for n in G.nodes()]
        x_min, x_max = min(x_positions), max(x_positions)
        y_min, y_max = min(y_positions), max(y_positions)
        scale = min(500 / (x_max - x_min), 400 / (y_max - y_min)) if (x_max - x_min) > 0 and (y_max - y_min) > 0 else 50
        offset_x = 350
        offset_y = 300
        svg_elements = []
        for edge in G.edges():
            x1 = pos[edge[0]][0] * scale + offset_x
            y1 = pos[edge[0]][1] * scale + offset_y
            x2 = pos[edge[1]][0] * scale + offset_x
            y2 = pos[edge[1]][1] * scale + offset_y
            edge_data = G[edge[0]][edge[1]]
            label = edge_data.get('label', 'has')
            svg_elements.append(f"""
            <line x1="{x1}" y1="{y1}" x2="{x2}" y2="{y2}" 
                  stroke="#999" stroke-width="2" opacity="0.5">
                <title>{label}</title>
            </line>
            """)
        node_info = []
        for i, node in enumerate(G.nodes()):
            x = pos[node][0] * scale + offset_x
            y = pos[node][1] * scale + offset_y
            display_name = G.nodes[node].get('display', node)
            node_type = G.nodes[node].get('node_type', 'unknown')
            color = '#4CAF50' if node_type == 'subject' else ('#2196F3' if node_type == 'object' else '#546E7A')
            neighbors = list(G.neighbors(node))
            neighbor_count = len(neighbors)
            node_info.append(f"""
            <circle cx="{x}" cy="{y}" r="{max(40, min(30, neighbor_count * 2 + 20))}" 
                    fill="{color}" stroke="#fff" stroke-width="2">
                <title>{display_name} ({neighbor_count} connections)</title>
            </circle>
            <text x="{x}" y="{y+6}" text-anchor="middle" font-size="15" font-weight="bold" fill="#000" 
                  pointer-events="none">{display_name[:15]}</text>
            """)
        svg_content = '\n'.join(svg_elements + node_info)
        html = f"""
        <div style="width: 100%; min-height: 700px; max-height: 800px; background: white; border: 2px solid #ddd; border-radius: 10px; padding: 20px; position: relative; overflow: auto;">
            <svg width="100%" height="550" style="border: 1px solid #ddd; border-radius: 5px; background: #f9f9f9; display: block;">
                {svg_content}
            </svg>
        </div>
        """
        return html
    except Exception as e:
        return f"<p style='color: red; padding: 20px;'>Error creating visualization: {e}</p>"

def delete_all_knowledge():
    global graph
    count = len(graph)
    # CRITICAL: Use remove() instead of creating a new graph object
    # This ensures all references to the graph (like kb_graph in api_server.py) stay in sync
    graph.remove((None, None, None))
    save_knowledge_graph()
    return f"üóëÔ∏è Deleted all {count} facts from the knowledge graph. Graph is now empty."

def delete_knowledge_by_keyword(keyword):
    global graph
    if not keyword or keyword.strip() == "":
        return "‚ö†Ô∏è Please enter a keyword to search for."
    keyword = keyword.strip().lower()
    deleted_count = 0
    facts_to_remove = []
    for s, p, o in graph:
        fact_text = f"{s} {p} {o}".lower()
        if keyword in fact_text:
            facts_to_remove.append((s, p, o))
    for fact in facts_to_remove:
        graph.remove(fact)
        deleted_count += 1
    if deleted_count > 0:
        save_knowledge_graph()
        return f"üóëÔ∏è Deleted {deleted_count} facts containing '{keyword}'"
    else:
        return f"‚ÑπÔ∏è No facts found containing '{keyword}'"

def delete_recent_knowledge(count=5):
    global graph
    if len(graph) == 0:
        return "‚ÑπÔ∏è Knowledge graph is already empty."
    facts = list(graph)
    facts_to_remove = facts[-count:] if count < len(facts) else facts
    for fact in facts_to_remove:
        graph.remove(fact)
    save_knowledge_graph()
    return f"üóëÔ∏è Deleted {len(facts_to_remove)} most recent facts"

def list_facts_for_editing():
    global fact_index
    fact_index = {}
    options = []
    for i, (s, p, o) in enumerate(list(graph), start=1):
        subject = str(s).split(':')[-1] if ':' in str(s) else str(s)
        predicate = str(p).split(':')[-1] if ':' in str(p) else str(p)
        object_val = str(o)
        label = f"{i}. {subject} {predicate} {object_val}"
        options.append(label)
        fact_index[i] = (s, p, o)
    return options

def load_fact_by_label(fact_label):
    if not fact_label:
        return None
    try:
        fact_id = int(fact_label.split('.', 1)[0].strip())
        return fact_index.get(fact_id)
    except Exception:
        return None

def add_fact_details(subject: str, predicate: str, object_val: str, details: str):
    """
    Add details/comment to an existing fact by storing it as a separate RDF triple.
    Uses a special predicate 'has_details' to link details to the main fact.
    
    Args:
        subject: The subject of the main fact
        predicate: The predicate of the main fact
        object_val: The object of the main fact
        details: The details/comment text to store
    """
    global graph
    import rdflib
    from urllib.parse import quote
    
    if not details or not details.strip():
        return
    
    # Create a unique identifier for this fact to link details to it
    # We'll use a combination of subject, predicate, and object as the identifier
    fact_id = f"{subject}|{predicate}|{object_val}"
    fact_id_clean = fact_id.strip().replace(' ', '_')
    fact_id_uri = rdflib.URIRef(f"urn:fact:{quote(fact_id_clean, safe='')}")
    
    # Store details with special predicate
    details_predicate = rdflib.URIRef("urn:has_details")
    details_literal = rdflib.Literal(details.strip())
    
    # Remove any existing details for this fact first
    remove_fact_details(subject, predicate, object_val)
    
    # Add the details triple
    graph.add((fact_id_uri, details_predicate, details_literal))
    
    # Also link the fact_id to the actual fact components for easier retrieval
    subject_clean = subject.strip().replace(' ', '_')
    predicate_clean = predicate.strip().replace(' ', '_')
    subject_uri = rdflib.URIRef(f"urn:{quote(subject_clean, safe='')}")
    predicate_uri = rdflib.URIRef(f"urn:{quote(predicate_clean, safe='')}")
    
    # Link fact_id to subject, predicate, object for retrieval
    graph.add((fact_id_uri, rdflib.URIRef("urn:fact_subject"), subject_uri))
    graph.add((fact_id_uri, rdflib.URIRef("urn:fact_predicate"), predicate_uri))
    graph.add((fact_id_uri, rdflib.URIRef("urn:fact_object"), rdflib.Literal(object_val)))

def remove_fact_details(subject: str, predicate: str, object_val: str):
    """
    Remove details for a specific fact.
    
    Args:
        subject: The subject of the main fact
        predicate: The predicate of the main fact
        object_val: The object of the main fact
    """
    global graph
    import rdflib
    from urllib.parse import quote
    
    # Create the fact identifier
    fact_id = f"{subject}|{predicate}|{object_val}"
    fact_id_clean = fact_id.strip().replace(' ', '_')
    fact_id_uri = rdflib.URIRef(f"urn:fact:{quote(fact_id_clean, safe='')}")
    
    # Find and remove all triples related to this fact's details
    triples_to_remove = []
    for s, p, o in graph:
        if str(s) == str(fact_id_uri):
            triples_to_remove.append((s, p, o))
    
    for triple in triples_to_remove:
        graph.remove(triple)

def get_fact_details(subject: str, predicate: str, object_val: str) -> str:
    """
    Retrieve details/comment for a specific fact.
    
    Args:
        subject: The subject of the main fact
        predicate: The predicate of the main fact
        object_val: The object of the main fact
    
    Returns:
        The details text if found, empty string otherwise
    """
    global graph
    import rdflib
    from urllib.parse import quote, unquote
    
    # Create the fact identifier
    fact_id = f"{subject}|{predicate}|{object_val}"
    fact_id_clean = fact_id.strip().replace(' ', '_')
    fact_id_uri = rdflib.URIRef(f"urn:fact:{quote(fact_id_clean, safe='')}")
    
    # Look for details triple
    details_predicate = rdflib.URIRef("urn:has_details")
    
    for s, p, o in graph:
        if str(s) == str(fact_id_uri) and str(p) == str(details_predicate):
            return str(o)
    
    return ""

def add_fact_agent_id(subject: str, predicate: str, object_val: str, agent_id: str):
    """
    Store agent_id for a specific fact (which worker agent extracted it).
    
    Args:
        subject: The subject of the main fact
        predicate: The predicate of the main fact
        object_val: The object of the main fact
        agent_id: ID of the worker agent that extracted this fact
    """
    global graph
    import rdflib
    from urllib.parse import quote
    
    if not agent_id:
        return
    
    # Create the fact identifier
    fact_id = f"{subject}|{predicate}|{object_val}"
    fact_id_clean = fact_id.strip().replace(' ', '_')
    fact_id_uri = rdflib.URIRef(f"urn:fact:{quote(fact_id_clean, safe='')}")
    
    # Store agent_id with special predicate
    agent_predicate = rdflib.URIRef("urn:agent_id")
    agent_literal = rdflib.Literal(agent_id.strip())
    
    # Remove any existing agent_id first (only one agent per fact)
    remove_fact_agent_id(subject, predicate, object_val)
    
    # Add the agent_id triple
    graph.add((fact_id_uri, agent_predicate, agent_literal))

def remove_fact_agent_id(subject: str, predicate: str, object_val: str):
    """Remove agent_id for a specific fact."""
    global graph
    import rdflib
    from urllib.parse import quote
    
    fact_id = f"{subject}|{predicate}|{object_val}"
    fact_id_clean = fact_id.strip().replace(' ', '_')
    fact_id_uri = rdflib.URIRef(f"urn:fact:{quote(fact_id_clean, safe='')}")
    agent_predicate = rdflib.URIRef("urn:agent_id")
    
    triples_to_remove = [(s, p, o) for s, p, o in graph 
                        if str(s) == str(fact_id_uri) and str(p) == str(agent_predicate)]
    for triple in triples_to_remove:
        graph.remove(triple)

def get_fact_agent_id(subject: str, predicate: str, object_val: str) -> Optional[str]:
    """Retrieve agent_id for a specific fact."""
    global graph
    import rdflib
    from urllib.parse import quote
    
    fact_id = f"{subject}|{predicate}|{object_val}"
    fact_id_clean = fact_id.strip().replace(' ', '_')
    fact_id_uri = rdflib.URIRef(f"urn:fact:{quote(fact_id_clean, safe='')}")
    agent_predicate = rdflib.URIRef("urn:agent_id")
    
    for s, p, o in graph:
        if str(s) == str(fact_id_uri) and str(p) == str(agent_predicate):
            return str(o)
    return None

def add_fact_source_document(subject: str, predicate: str, object_val: str, source_document: str, uploaded_at: str):
    """
    Store source document and upload timestamp for a specific fact.
    ENHANCED: Now supports multiple sources per fact (appends instead of replacing).
    
    Args:
        subject: The subject of the main fact
        predicate: The predicate of the main fact
        object_val: The object of the main fact
        source_document: Name of the source document (or "manual" for manually added)
        uploaded_at: ISO format timestamp when the fact was added
    """
    global graph
    import rdflib
    from urllib.parse import quote
    
    if not source_document or not uploaded_at:
        return
    
    # Create the fact identifier
    fact_id = f"{subject}|{predicate}|{object_val}"
    fact_id_clean = fact_id.strip().replace(' ', '_')
    fact_id_uri = rdflib.URIRef(f"urn:fact:{quote(fact_id_clean, safe='')}")
    
    # Store source document and timestamp with special predicates
    # Use a combined format: "source_document|uploaded_at" to allow multiple sources
    source_predicate = rdflib.URIRef("urn:source_document")
    timestamp_predicate = rdflib.URIRef("urn:uploaded_at")
    
    # Check if this source already exists for this fact
    source_doc_str = source_document.strip()
    timestamp_str = uploaded_at.strip()
    
    # Check for duplicates - don't add the same source twice
    existing_sources = get_fact_source_document(subject, predicate, object_val)
    source_exists = False
    for existing_source, existing_timestamp in existing_sources:
        if existing_source == source_doc_str and existing_timestamp == timestamp_str:
            source_exists = True
            break
    
    if not source_exists:
        # Add the source document and timestamp triples (multiple sources allowed)
        # Use a unique identifier for each source entry
        source_entry_id = f"{source_doc_str}|{timestamp_str}"
        graph.add((fact_id_uri, source_predicate, rdflib.Literal(source_entry_id)))
        graph.add((fact_id_uri, timestamp_predicate, rdflib.Literal(timestamp_str)))
    
    # Also link the fact_id to the actual fact components for easier retrieval (if not already linked)
    subject_clean = subject.strip().replace(' ', '_')
    predicate_clean = predicate.strip().replace(' ', '_')
    subject_uri = rdflib.URIRef(f"urn:{quote(subject_clean, safe='')}")
    predicate_uri = rdflib.URIRef(f"urn:{quote(predicate_clean, safe='')}")
    
    # Check if links already exist
    has_subject_link = any(str(s) == str(fact_id_uri) and 'fact_subject' in str(p) for s, p, o in graph)
    has_predicate_link = any(str(s) == str(fact_id_uri) and 'fact_predicate' in str(p) for s, p, o in graph)
    has_object_link = any(str(s) == str(fact_id_uri) and 'fact_object' in str(p) for s, p, o in graph)
    
    if not has_subject_link:
        graph.add((fact_id_uri, rdflib.URIRef("urn:fact_subject"), subject_uri))
    if not has_predicate_link:
        graph.add((fact_id_uri, rdflib.URIRef("urn:fact_predicate"), predicate_uri))
    if not has_object_link:
        graph.add((fact_id_uri, rdflib.URIRef("urn:fact_object"), rdflib.Literal(object_val)))

def remove_fact_source_document(subject: str, predicate: str, object_val: str):
    """
    Remove source document info for a specific fact.
    
    Args:
        subject: The subject of the main fact
        predicate: The predicate of the main fact
        object_val: The object of the main fact
    """
    global graph
    import rdflib
    from urllib.parse import quote
    
    # Create the fact identifier
    fact_id = f"{subject}|{predicate}|{object_val}"
    fact_id_clean = fact_id.strip().replace(' ', '_')
    fact_id_uri = rdflib.URIRef(f"urn:fact:{quote(fact_id_clean, safe='')}")
    
    # Find and remove source document and timestamp triples
    source_predicate = rdflib.URIRef("urn:source_document")
    timestamp_predicate = rdflib.URIRef("urn:uploaded_at")
    
    triples_to_remove = []
    for s, p, o in graph:
        if str(s) == str(fact_id_uri) and str(p) in [str(source_predicate), str(timestamp_predicate)]:
            triples_to_remove.append((s, p, o))
    
    for triple in triples_to_remove:
        graph.remove(triple)

def add_fact_is_inferred(subject: str, predicate: str, object_val: str, is_inferred: bool):
    """
    Store the inferred status of a fact.
    
    Args:
        subject: The subject of the main fact
        predicate: The predicate of the main fact
        object_val: The object of the main fact
        is_inferred: True if the fact was inferred, False if extracted from document
    """
    global graph
    import rdflib
    from urllib.parse import quote
    
    # Create the fact identifier
    fact_id = f"{subject}|{predicate}|{object_val}"
    fact_id_clean = fact_id.strip().replace(' ', '_')
    fact_id_uri = rdflib.URIRef(f"urn:fact:{quote(fact_id_clean, safe='')}")
    
    # Store inferred status with special predicate
    inferred_predicate = rdflib.URIRef("urn:is_inferred")
    inferred_literal = rdflib.Literal(str(is_inferred).lower())  # Store as "true" or "false"
    
    # Remove any existing status first
    remove_fact_is_inferred(subject, predicate, object_val)
    
    # Add the inferred status triple
    graph.add((fact_id_uri, inferred_predicate, inferred_literal))

def remove_fact_is_inferred(subject: str, predicate: str, object_val: str):
    """
    Remove the inferred status of a fact.
    
    Args:
        subject: The subject of the main fact
        predicate: The predicate of the main fact
        object_val: The object of the main fact
    """
    global graph
    import rdflib
    from urllib.parse import quote
    
    # Create the fact identifier
    fact_id = f"{subject}|{predicate}|{object_val}"
    fact_id_clean = fact_id.strip().replace(' ', '_')
    fact_id_uri = rdflib.URIRef(f"urn:fact:{quote(fact_id_clean, safe='')}")
    
    # Find and remove inferred status triple
    inferred_predicate = rdflib.URIRef("urn:is_inferred")
    
    triples_to_remove = []
    for s, p, o in graph:
        if str(s) == str(fact_id_uri) and str(p) == str(inferred_predicate):
            triples_to_remove.append((s, p, o))
    
    for triple in triples_to_remove:
        graph.remove(triple)

def get_fact_is_inferred(subject: str, predicate: str, object_val: str):
    """
    Retrieve the inferred status of a fact.
    
    Args:
        subject: The subject of the main fact
        predicate: The predicate of the main fact
        object_val: The object of the main fact
    
    Returns:
        True if the fact was inferred, False if extracted from document, None if not set
    """
    global graph
    import rdflib
    from urllib.parse import quote
    
    # Create the fact identifier
    fact_id = f"{subject}|{predicate}|{object_val}"
    fact_id_clean = fact_id.strip().replace(' ', '_')
    fact_id_uri = rdflib.URIRef(f"urn:fact:{quote(fact_id_clean, safe='')}")
    
    # Find inferred status triple
    inferred_predicate = rdflib.URIRef("urn:is_inferred")
    
    for s, p, o in graph:
        if str(s) == str(fact_id_uri) and str(p) == str(inferred_predicate):
            # Return True if the literal is "true", False otherwise
            return str(o).lower() == 'true'
    
    # Return None if not found (not set yet)
    return None

def add_fact_confidence(subject: str, predicate: str, object_val: str, confidence: float):
    """
    Store the confidence score of a fact.
    
    Args:
        subject: The subject of the main fact
        predicate: The predicate of the main fact
        object_val: The object of the main fact
        confidence: Confidence score between 0.0 and 1.0
    """
    global graph
    import rdflib
    from urllib.parse import quote
    
    # Create the fact identifier
    fact_id = f"{subject}|{predicate}|{object_val}"
    fact_id_clean = fact_id.strip().replace(' ', '_')
    fact_id_uri = rdflib.URIRef(f"urn:fact:{quote(fact_id_clean, safe='')}")
    
    # Store confidence with special predicate
    confidence_predicate = rdflib.URIRef("urn:confidence")
    confidence_literal = rdflib.Literal(str(confidence))  # Store as string
    
    # Remove any existing confidence first
    remove_fact_confidence(subject, predicate, object_val)
    
    # Add the confidence triple
    graph.add((fact_id_uri, confidence_predicate, confidence_literal))

def remove_fact_confidence(subject: str, predicate: str, object_val: str):
    """Remove confidence for a specific fact."""
    global graph
    import rdflib
    from urllib.parse import quote
    
    fact_id = f"{subject}|{predicate}|{object_val}"
    fact_id_clean = fact_id.strip().replace(' ', '_')
    fact_id_uri = rdflib.URIRef(f"urn:fact:{quote(fact_id_clean, safe='')}")
    
    confidence_predicate = rdflib.URIRef("urn:confidence")
    
    triples_to_remove = []
    for s, p, o in graph:
        if str(s) == str(fact_id_uri) and str(p) == str(confidence_predicate):
            triples_to_remove.append((s, p, o))
    
    for triple in triples_to_remove:
        graph.remove(triple)

def get_fact_confidence(subject: str, predicate: str, object_val: str) -> float:
    """
    Retrieve the confidence score for a specific fact.
    
    Args:
        subject: The subject of the main fact
        predicate: The predicate of the main fact
        object_val: The object of the main fact
    
    Returns:
        The confidence score (0.0 to 1.0), or 0.7 as default if not found
    """
    global graph
    import rdflib
    from urllib.parse import quote
    
    fact_id = f"{subject}|{predicate}|{object_val}"
    fact_id_clean = fact_id.strip().replace(' ', '_')
    fact_id_uri = rdflib.URIRef(f"urn:fact:{quote(fact_id_clean, safe='')}")
    
    confidence_predicate = rdflib.URIRef("urn:confidence")
    
    for s, p, o in graph:
        if str(s) == str(fact_id_uri) and str(p) == str(confidence_predicate):
            try:
                return float(str(o))
            except (ValueError, TypeError):
                return 0.7  # Default confidence
    
    return 0.7  # Default confidence if not found

def get_fact_source_document(subject: str, predicate: str, object_val: str) -> list[tuple[str, str]]:
    """
    Retrieve all source documents and upload timestamps for a specific fact.
    ENHANCED: Now returns a list of (source_document, uploaded_at) tuples to support multiple sources.
    
    Args:
        subject: The subject of the main fact
        predicate: The predicate of the main fact
        object_val: The object of the main fact
    
    Returns:
        List of tuples of (source_document, uploaded_at) for all sources found
    """
    global graph
    import rdflib
    from urllib.parse import quote
    
    # Create the fact identifier
    fact_id = f"{subject}|{predicate}|{object_val}"
    fact_id_clean = fact_id.strip().replace(' ', '_')
    fact_id_uri = rdflib.URIRef(f"urn:fact:{quote(fact_id_clean, safe='')}")
    
    # Look for source document and timestamp triples
    source_predicate = rdflib.URIRef("urn:source_document")
    timestamp_predicate = rdflib.URIRef("urn:uploaded_at")
    
    sources = []  # List of (source_document, uploaded_at) tuples
    
    # Collect all source entries
    source_entries = []
    timestamps = []
    
    for s, p, o in graph:
        if str(s) == str(fact_id_uri):
            if str(p) == str(source_predicate):
                # Source entry format: "source_document|uploaded_at" or just "source_document"
                source_entry = str(o)
                source_entries.append(source_entry)
            elif str(p) == str(timestamp_predicate):
                timestamps.append(str(o))
    
    # Match sources with timestamps
    # If source entries contain both source and timestamp (format: "source|timestamp")
    for source_entry in source_entries:
        if '|' in source_entry:
            parts = source_entry.split('|', 1)
            if len(parts) == 2:
                source_doc = parts[0]
                timestamp = parts[1]
                sources.append((source_doc, timestamp))
        else:
            # Legacy format: just source document, try to match with timestamp
            # For now, use the first timestamp or empty string
            timestamp = timestamps[0] if timestamps else ""
            sources.append((source_entry, timestamp))
    
    # Also handle case where we have separate source and timestamp triples
    # (for backward compatibility with existing data)
    if not sources and source_entries and timestamps:
        # Match each source with each timestamp (simple pairing)
        for i, source_entry in enumerate(source_entries):
            timestamp = timestamps[i] if i < len(timestamps) else (timestamps[0] if timestamps else "")
            sources.append((source_entry, timestamp))
    
    # Remove duplicates
    unique_sources = []
    seen = set()
    for source_doc, timestamp in sources:
        key = (source_doc, timestamp)
        if key not in seen:
            seen.add(key)
            unique_sources.append((source_doc, timestamp))
    
    return unique_sources if unique_sources else []

def import_knowledge_from_json_file(file):
    try:
        if file is None:
            return "‚ö†Ô∏è No file selected."
        file_path = file.name if hasattr(file, 'name') else str(file)
        if not os.path.exists(file_path):
            return f"‚ö†Ô∏è File not found: {file_path}"
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        if isinstance(data, dict) and 'facts' in data:
            facts = data['facts']
        elif isinstance(data, list):
            facts = data
        else:
            return "‚ùå Unsupported JSON structure. Expect an object with 'facts' or a list of facts."
        added = 0
        skipped = 0
        for fact in facts:
            try:
                subject = fact.get('subject') or fact.get('full_subject')
                predicate = fact.get('predicate') or fact.get('full_predicate')
                obj = fact.get('object') or fact.get('full_object')
                if not subject or not predicate or obj is None:
                    skipped += 1
                    continue
                
                # Extract subject/predicate from URI if needed (handle both formats)
                # If it's already a URI like "urn:subject", extract the subject part
                subject_str = str(subject)
                if subject_str.startswith('urn:'):
                    # Extract the actual subject text from URI
                    from urllib.parse import unquote
                    subject_str = unquote(subject_str.replace('urn:', '')).replace('_', ' ')
                else:
                    subject_str = str(subject)
                
                predicate_str = str(predicate)
                if predicate_str.startswith('urn:'):
                    from urllib.parse import unquote
                    predicate_str = unquote(predicate_str.replace('urn:', '')).replace('_', ' ')
                else:
                    predicate_str = str(predicate)
                
                obj_str = str(obj)
                
                # Check if fact already exists using fact_exists function
                if fact_exists(subject_str, predicate_str, obj_str):
                    skipped += 1
                    continue
                
                # Create URIs using the same encoding as other functions
                from urllib.parse import quote
                subject_clean = subject_str.strip().replace(' ', '_')
                predicate_clean = predicate_str.strip().replace(' ', '_')
                s_ref = rdflib.URIRef(f"urn:{quote(subject_clean, safe='')}")
                p_ref = rdflib.URIRef(f"urn:{quote(predicate_clean, safe='')}")
                o_lit = rdflib.Literal(obj_str)
                
                graph.add((s_ref, p_ref, o_lit))
                
                # Import details if present
                details = fact.get('details')
                if details:
                    add_fact_details(subject_str, predicate_str, obj_str, details)
                
                # Import source documents (support both single and multiple sources)
                source_documents = fact.get('sourceDocuments') or fact.get('source_documents')
                if source_documents and isinstance(source_documents, list):
                    # Multiple sources format
                    for source_info in source_documents:
                        if isinstance(source_info, dict):
                            source_doc = source_info.get('document') or source_info.get('sourceDocument')
                            uploaded_at = source_info.get('uploadedAt') or source_info.get('uploaded_at')
                            if source_doc:
                                if not uploaded_at:
                                    from datetime import datetime
                                    uploaded_at = datetime.now().isoformat()
                                add_fact_source_document(subject_str, predicate_str, obj_str, source_doc, uploaded_at)
                else:
                    # Legacy single source format
                    source_document = fact.get('sourceDocument') or fact.get('source_document')
                    uploaded_at = fact.get('uploadedAt') or fact.get('uploaded_at')
                    if source_document:
                        if not uploaded_at:
                            from datetime import datetime
                            uploaded_at = datetime.now().isoformat()
                        add_fact_source_document(subject_str, predicate_str, obj_str, source_document, uploaded_at)
                
                added += 1
            except Exception as e:
                skipped += 1
                print(f"‚ö†Ô∏è  Error importing fact: {e}")
        save_knowledge_graph()
        if skipped > 0:
            return f"‚úÖ Imported {added} new facts, skipped {skipped} duplicates. Total facts: {len(graph)}."
        return f"‚úÖ Imported {added} facts. Skipped {skipped}. Total facts: {len(graph)}."
    except Exception as e:
        return f"‚ùå Import failed: {e}"


