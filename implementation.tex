\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{geometry}

\geometry{margin=1in}

% Code listing style
\lstset{
    language=Python,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny\color{gray},
    stepnumber=1,
    numbersep=5pt,
    frame=single,
    breaklines=true,
    showstringspaces=false,
    tabsize=2
}

\title{Implementation Details\\
\large{Core Algorithmic Components and Computational Mechanisms}}
\author{}
\date{}

\begin{document}

\maketitle

\section{Implementation}

This document presents the core algorithmic components and theoretical foundations underlying the system's implementation, focusing on the computational mechanisms that enable transparency, traceability, and human-centred decision-making.

\subsection{Knowledge Graph Storage and Provenance}

The knowledge graph uses RDFLib's in-memory \texttt{Graph} structure, where facts are stored as RDF triples with \texttt{URIRef} objects for subjects/predicates and \texttt{Literal} objects for values. The theoretical choice of RDF as the representation formalism provides standardised semantics while enabling explicit provenance tracking through reification.

Each fact is assigned a unique identifier computed as \texttt{fact\_id = f"{subject}|{predicate}|{normalized\_object}"}, which is URI-encoded and stored separately. Provenance metadata (source document, timestamp, agent ID, confidence) is attached through additional RDF triples linking fact IDs to metadata values. This design enables multiple sources per fact and maintains complete auditability.

\begin{lstlisting}[language=Python, caption=Fact Storage with Provenance Metadata]
from rdflib import Graph, URIRef, Literal
from urllib.parse import quote

# Create knowledge graph
graph = Graph()

# Store a fact with provenance
subject_uri = URIRef("urn:entity:John_Smith")
predicate_uri = URIRef("urn:predicate:has_salary")
object_literal = Literal("75000")

# Add triple to knowledge graph
graph.add((subject_uri, predicate_uri, object_literal))

# Create fact ID for metadata linking
fact_id = f"{subject}|{predicate}|{normalized_object}"
fact_id_uri = URIRef(f"urn:fact:{quote(fact_id, safe='')}")

# Add provenance metadata
graph.add((fact_id_uri, URIRef("urn:source_document"), 
           Literal("employees.csv")))
graph.add((fact_id_uri, URIRef("urn:uploaded_at"), 
           Literal("2024-01-15T10:30:00")))
graph.add((fact_id_uri, URIRef("urn:agent_id"), 
           Literal("worker_001")))
graph.add((fact_id_uri, URIRef("urn:confidence"), 
           Literal(0.95)))
\end{lstlisting}

The graph persistence uses dual-format storage: binary \texttt{pickle} serialisation for efficient loading and JSON backup for portability. Entity normalisation addresses the representational consistency problem: semantically identical entities with different surface forms must map to a canonical representation. The algorithm applies sequential transformations: lowercasing, special character removal, and space normalisation, followed by lookup in a learned mapping dictionary.

\begin{lstlisting}[language=Python, caption=Entity Normalization Algorithm]
def normalize_entity(entity: str) -> str:
    """Normalize entity name for consistent storage"""
    # Step 1: Convert to lowercase and strip whitespace
    normalized = entity.lower().strip()
    
    # Step 2: Remove special characters (keep alphanumeric and spaces)
    normalized = re.sub(r'[^\w\s]', '', normalized)
    
    # Step 3: Replace multiple spaces with single space
    normalized = re.sub(r'\s+', ' ', normalized)
    
    # Step 4: Check normalization map for known variants
    if normalized in entity_normalization_map:
        normalized = entity_normalization_map[normalized]
    
    return normalized

def learn_normalizations_from_facts():
    """Learn normalization mappings from existing facts"""
    entity_groups = defaultdict(list)
    
    # Group entities by normalized form
    for s, p, o in graph:
        s_normalized = str(s).lower().strip()
        entity_groups[s_normalized].append(str(s))
    
    # Use most common variant as canonical
    for normalized, variants in entity_groups.items():
        if len(set(variants)) > 1:
            canonical = max(set(variants), key=variants.count)
            for variant in variants:
                if variant.lower() != canonical.lower():
                    entity_normalization_map[variant.lower()] = canonical.lower()
\end{lstlisting}

Duplicate prevention is critical for parallel processing correctness. The system maintains an in-memory hash set \texttt{\_fact\_lookup\_set} containing normalised fact tuples, enabling O(1) existence checks before graph insertion. This index is initialised at startup (O(n) complexity) and maintained incrementally, ensuring that concurrent worker threads can safely check for duplicates without graph traversal.

\subsection{Multi-Agent Orchestration}

The multi-agent system implements indirect communication through the knowledge graph, eliminating direct message passing between agents. Each agent is implemented as a Python \texttt{dataclass} with identity attributes, lifecycle status, and metadata. Agents interact by reading from and writing to the shared knowledge graph, creating a naturally decoupled architecture.

Worker agents extract facts from document chunks and write provenance-enriched triples to the graph. Analytical agents (Statistics and Operational) generate higher-level semantic structure by computing correlations, descriptive statistics, and operational insights, then storing these as facts in the same representational substrate. This unified approach enables both raw data and derived patterns to be queryable through the same interface.

\begin{lstlisting}[language=Python, caption=Statistics Agent Implementation]
def process_with_statistics_agent(df, document_name: str):
    """Compute correlations and descriptive statistics"""
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    
    # Compute pairwise correlations
    corr_matrix = df[numeric_cols].corr()
    
    # Filter significant correlations (|r| > 0.3)
    threshold = 0.3
    for col1 in numeric_cols:
        for col2 in numeric_cols:
            if col1 < col2:  # Avoid duplicates
                corr_value = corr_matrix.loc[col1, col2]
                if abs(corr_value) > threshold:
                    # Store as fact in knowledge graph
                    fact_subject = f"correlation_{col1}_{col2}"
                    add_to_graph(
                        f"{fact_subject} has_correlation_value {corr_value}",
                        source_document=document_name
                    )
    
    # Compute descriptive statistics
    for col in numeric_cols:
        stats = {
            'mean': df[col].mean(),
            'median': df[col].median(),
            'std': df[col].std(),
            'min': df[col].min(),
            'max': df[col].max()
        }
        for stat_name, stat_value in stats.items():
            add_to_graph(
                f"{col} has_{stat_name} {stat_value}",
                source_document=document_name
            )
\end{lstlisting}

The Operational Agent generates insights through pandas groupby operations, computing aggregations such as department performance metrics and manager-level statistics. These operational insights are stored as facts, enabling queries like ``What is the average salary by department?'' to be answered through direct graph retrieval.

\begin{lstlisting}[language=Python, caption=Operational Insights Generation]
def generate_operational_insights(df, document_name: str):
    """Generate operational insights using groupby operations"""
    # Department-level aggregations
    if 'department' in df.columns and 'salary' in df.columns:
        dept_stats = df.groupby('department')['salary'].agg([
            'mean', 'count', 'min', 'max'
        ]).reset_index()
        
        for _, row in dept_stats.iterrows():
            dept = row['department']
            add_to_graph(
                f"{dept} has_average_salary {row['mean']}",
                source_document=document_name
            )
            add_to_graph(
                f"{dept} has_employee_count {row['count']}",
                source_document=document_name
            )
    
    # Manager-level aggregations
    if 'manager' in df.columns:
        manager_stats = df.groupby('manager').agg({
            'salary': 'mean',
            'performance': 'mean'
        })
        
        for manager, stats in manager_stats.iterrows():
            add_to_graph(
                f"{manager} manages_avg_salary {stats['salary']}",
                source_document=document_name
            )
\end{lstlisting}

\subsection{Query Routing and Classification}

Query routing uses a hierarchical pattern-matching algorithm that processes queries through priority-ordered stages. The theoretical motivation is efficiency: employee-specific queries can be answered by querying only relevant document agents, reducing graph traversal overhead.

\begin{lstlisting}[language=Python, caption=Query Routing Algorithm]
def find_agents_for_query(query: str, query_type: str) -> Dict[str, Any]:
    """Route query to appropriate agents based on type and content"""
    routing_info = {
        "strategy": "all_agents",
        "target_agents": [],
        "reason": ""
    }
    
    query_lower = query.lower()
    
    # Stage 1: Employee-specific detection
    if query_type == "filter":
        name_pattern = r'([A-Z][a-z]+,\s*[A-Z][a-z]+)'
        match = re.search(name_pattern, query)
        if match:
            employee_name = match.group(1)
            matching_agents = find_agent_for_employee(employee_name)
            if matching_agents:
                routing_info["strategy"] = "specific_agents"
                routing_info["target_agents"] = matching_agents
                routing_info["reason"] = f"Found employee in {len(matching_agents)} agent(s)"
                return routing_info
    
    # Stage 2: Statistical query detection
    statistics_keywords = [
        "correlation", "distribution", "average", "mean", 
        "median", "standard deviation", "relationship between"
    ]
    if any(keyword in query_lower for keyword in statistics_keywords):
        routing_info["strategy"] = "statistics_agent"
        routing_info["target_agents"] = ["statistics_agent"]
        routing_info["reason"] = "Query requires statistical analysis"
        return routing_info
    
    # Stage 3: Operational query detection
    if query_type == "operational":
        routing_info["strategy"] = "operational_agent"
        routing_info["target_agents"] = ["operational_query_agent"]
        routing_info["reason"] = "Routed to operational agent"
        return routing_info
    
    # Stage 4: Structured pattern matching
    if query_type == "structured":
        # Handle max/min/filter queries with direct graph lookups
        routing_info["strategy"] = "direct_lookup"
        return routing_info
    
    # Stage 5: Conversational fallback
    routing_info["strategy"] = "llm_agent"
    routing_info["target_agents"] = ["llm_agent"]
    routing_info["reason"] = "Default: conversational query"
    return routing_info
\end{lstlisting}

The routing algorithm extracts query parameters (entities, attributes, operations) using regex patterns and keyword lists. It returns routing decisions with explicit reasoning, maintaining transparency about why each query was routed to specific agents. This design enables users to understand the system's decision-making process.

\subsection{Parallel Processing and Concurrency}

Parallel document processing uses Python's \texttt{ThreadPoolExecutor} with an adaptive chunking strategy. The chunk size calculation \texttt{chunk\_size = max(25, min(200, 15000 // c))} balances load distribution against thread overhead, targeting approximately 15,000 data points per chunk. This adaptive approach ensures efficient parallelisation regardless of dataset dimensions.

The theoretical foundation is that RDFLib's graph operations are thread-safe, enabling concurrent writes without explicit synchronisation. Worker threads use \texttt{threading.Lock()} only for updating shared counters and agent registries, not for graph access.

\begin{lstlisting}[language=Python, caption=Parallel Processing with ThreadPoolExecutor]
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

def extract_csv_facts_parallel(df, document_name: str, num_workers: int = 4):
    """Extract facts from CSV using parallel processing"""
    total_rows = len(df)
    total_cols = len(df.columns)
    
    # Calculate optimal chunk size
    data_complexity = total_rows * total_cols
    chunk_size = max(25, min(200, 15000 // total_cols))
    
    # Split into chunks
    chunks = []
    for start_idx in range(0, total_rows, chunk_size):
        end_idx = min(start_idx + chunk_size, total_rows)
        chunks.append((start_idx, end_idx))
    
    # Thread-safe counters
    facts_counter = {'added': 0, 'skipped': 0}
    facts_lock = threading.Lock()
    
    def process_chunk(chunk_idx: int, start_idx: int, end_idx: int):
        """Process a single chunk"""
        chunk_df = df.iloc[start_idx:end_idx].copy()
        chunk_facts = 0
        
        for _, row in chunk_df.iterrows():
            employee_name = row['Employee_Name']
            
            # Extract facts from row
            for col in chunk_df.columns:
                if col != 'Employee_Name':
                    fact = (employee_name, f"has_{col.lower()}", row[col])
                    
                    # Check duplicate (O(1) lookup)
                    if not fact_exists(*fact):
                        add_to_graph(
                            f"{fact[0]} {fact[1]} {fact[2]}",
                            source_document=document_name,
                            agent_id=f"worker_{chunk_idx}"
                        )
                        chunk_facts += 1
        
        # Update shared counter (thread-safe)
        with facts_lock:
            facts_counter['added'] += chunk_facts
        
        return chunk_facts
    
    # Process chunks in parallel
    with ThreadPoolExecutor(max_workers=num_workers) as executor:
        futures = {
            executor.submit(process_chunk, i, start, end): i
            for i, (start, end) in enumerate(chunks)
        }
        
        for future in as_completed(futures):
            try:
                future.result()
            except Exception as e:
                print(f"Chunk processing error: {e}")
    
    return facts_counter['added']
\end{lstlisting}

The chunking algorithm splits the DataFrame into non-overlapping ranges and verifies complete coverage by comparing covered row indices to the total row count. Each chunk is processed by a dedicated worker function that extracts facts, checks the in-memory fact index, and writes provenance metadata. The system achieves linear speedup for large documents while maintaining data consistency through thread-safe graph operations.

\subsection{Evidence Assembly and Fact Ranking}

Evidence assembly ranks facts by relevance to the query using a scoring function that considers keyword overlap, entity name matches, and predicate alignment. The top-\(k\) facts (typically \(k=5\)) are selected and formatted with source attribution, ensuring that every system response includes inspectable evidence.

\begin{lstlisting}[language=Python, caption=Evidence Assembly Algorithm]
def build_evidence_context(evidence_facts: List[Dict], query: str) -> str:
    """Assemble and rank evidence facts for response"""
    if not evidence_facts:
        return ""
    
    # Rank facts by relevance
    ranked_facts = rank_facts_by_relevance(evidence_facts, query)
    
    # Select top-k most relevant facts
    top_facts = ranked_facts[:5]
    
    # Format evidence display
    evidence_lines = ["**Evidence from Knowledge Graph:**"]
    
    for i, fact in enumerate(top_facts, 1):
        subj = fact["subject"]
        pred = fact["predicate"]
        obj = fact["object"]
        sources = fact.get("source", [])
        
        evidence_line = f"{i}. {subj} → {pred} → {obj}"
        
        # Add source attribution
        if sources:
            source_list = []
            for source_item in sources:
                if isinstance(source_item, tuple):
                    source_list.append(str(source_item[0]))
                else:
                    source_list.append(str(source_item))
            
            unique_sources = list(dict.fromkeys(source_list))[:2]
            evidence_line += f" [Source: {', '.join(unique_sources)}]"
        
        evidence_lines.append(evidence_line)
    
    return "\n".join(evidence_lines)

def rank_facts_by_relevance(facts: List[Dict], query: str) -> List[Dict]:
    """Rank facts by relevance to query"""
    query_terms = set(query.lower().split())
    
    def score_fact(fact):
        score = 0
        fact_text = f"{fact['subject']} {fact['predicate']} {fact['object']}".lower()
        fact_terms = set(fact_text.split())
        
        # Keyword overlap
        score += len(query_terms & fact_terms) * 2
        
        # Entity name matches
        if any(term in fact['subject'].lower() for term in query_terms):
            score += 3
        
        # Predicate alignment
        if any(term in fact['predicate'].lower() for term in query_terms):
            score += 1
        
        return score
    
    return sorted(facts, key=score_fact, reverse=True)
\end{lstlisting}

The theoretical foundation is that explainability requires not just evidence presence, but evidence relevance. By ranking facts by semantic overlap with the query, the system presents the most informative supporting evidence first, reducing cognitive load on users while maintaining complete traceability.

\end{document}

