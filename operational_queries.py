"""
Operational Query Processor - Pre-computes operational insights
Groups data by various columns and calculates aggregations (avg, min, max, top/bottom N)
Stores results as facts in knowledge base for LLM access and displays in insights page.
"""

from typing import List, Dict, Any, Optional, Tuple
import pandas as pd
import os
import math
from agent_system import document_agents
from strategic_query_agent import normalize_column_name
from strategic_queries import find_csv_file_path, load_csv_data


def sanitize_float(value: Any) -> Optional[float]:
    """
    Convert a value to a JSON-safe float.
    Returns None for NaN, Infinity, or -Infinity values.
    """
    if value is None:
        return None
    try:
        float_val = float(value)
        if math.isnan(float_val) or math.isinf(float_val):
            return None
        return float_val
    except (ValueError, TypeError):
        return None


def compute_operational_insights(csv_file_path: Optional[str] = None, df: Optional[pd.DataFrame] = None) -> Dict[str, Any]:
    """
    Compute operational insights by grouping data and calculating aggregations.
    Returns a dictionary with all computed insights organized by category.
    Focus: Manager-based insights and all departments.
    
    Can use either a pre-loaded DataFrame (preferred, faster) or load from CSV file.
    This ensures no data is missed and can run in parallel with fact extraction.
    
    Args:
        csv_file_path: Optional direct path to CSV file. Used only if df is not provided.
        df: Optional pre-loaded DataFrame. If provided, uses this instead of reading file.
    """
    insights = {}
    
    # Use pre-loaded DataFrame if available (preferred - no file I/O needed)
    if df is not None and len(df) > 0:
        print(f"âœ… Using pre-loaded DataFrame for operational insights ({len(df)} rows, {len(df.columns)} columns)")
    else:
        # Load DataFrame directly from CSV file (fallback)
        csv_path = csv_file_path
        if csv_path is None:
            csv_path = find_csv_file_path()
        
        if csv_path is None or not os.path.exists(csv_path):
            print(f"âš ï¸  No CSV file path available and no DataFrame provided")
            print(f"   Returning empty insights structure")
            # Return empty structure instead of empty dict
            return {
                'by_manager': [],
                'by_department': [],
                'by_recruitment_source': [],
                'top_performance': [],
                'bottom_performance': [],
                'top_absences': [],
                'bottom_engagement': [],
                'top_special_projects': [],
                'top_salary': []
            }
        
        print(f"ðŸ“Š Loading DataFrame from CSV file: {csv_path}")
        try:
            df = load_csv_data(csv_path)
        except Exception as load_error:
            print(f"âŒ Error loading CSV file: {load_error}")
            import traceback
            traceback.print_exc()
            # Return empty structure
            return {
                'by_manager': [],
                'by_department': [],
                'by_recruitment_source': [],
                'top_performance': [],
                'bottom_performance': [],
                'top_absences': [],
                'bottom_engagement': [],
                'top_special_projects': [],
                'top_salary': []
            }
        
        if df is None or len(df) == 0:
            print(f"âš ï¸  Failed to load DataFrame from CSV file or DataFrame is empty")
            # Return empty structure
            return {
                'by_manager': [],
                'by_department': [],
                'by_recruitment_source': [],
                'top_performance': [],
                'bottom_performance': [],
                'top_absences': [],
                'bottom_engagement': [],
                'top_special_projects': [],
                'top_salary': []
            }
    
    # Validate DataFrame
    if df is None:
        print(f"âŒ DataFrame is None after loading")
        return {
            'by_manager': [],
            'by_department': [],
            'by_recruitment_source': [],
            'top_performance': [],
            'bottom_performance': [],
            'top_absences': [],
            'bottom_engagement': [],
            'top_special_projects': [],
            'top_salary': []
        }
    
    print(f"ðŸ“Š DataFrame ready: {len(df)} rows, {len(df.columns)} columns: {list(df.columns)[:10]}")
    
    # Manager-based insights (consolidated into one table)
    try:
        insights['by_manager'] = group_by_manager(df)
    except Exception as e:
        print(f"âš ï¸  Error computing by_manager insights: {e}")
        insights['by_manager'] = []
    
    # Department insights (all departments)
    try:
        insights['by_department'] = group_by_department(df)
    except Exception as e:
        print(f"âš ï¸  Error computing by_department insights: {e}")
        insights['by_department'] = []
    
    # Recruitment source insights
    try:
        insights['by_recruitment_source'] = group_by_recruitment_source(df)
    except Exception as e:
        print(f"âš ï¸  Error computing by_recruitment_source insights: {e}")
        insights['by_recruitment_source'] = []
    
    # Employee-level insights (only for active employees)
    try:
        insights['top_performance'] = get_top_performance(df, top_n=5)
    except Exception as e:
        print(f"âš ï¸  Error computing top_performance insights: {e}")
        insights['top_performance'] = []
    
    try:
        insights['bottom_performance'] = get_bottom_performance(df, bottom_n=5)
    except Exception as e:
        print(f"âš ï¸  Error computing bottom_performance insights: {e}")
        insights['bottom_performance'] = []
    
    try:
        insights['top_absences'] = get_top_absences(df, top_n=5)
    except Exception as e:
        print(f"âš ï¸  Error computing top_absences insights: {e}")
        insights['top_absences'] = []
    
    try:
        insights['bottom_engagement'] = get_bottom_engagement(df, bottom_n=5)
    except Exception as e:
        print(f"âš ï¸  Error computing bottom_engagement insights: {e}")
        insights['bottom_engagement'] = []
    
    try:
        insights['top_special_projects'] = get_top_special_projects(df, top_n=5)
    except Exception as e:
        print(f"âš ï¸  Error computing top_special_projects insights: {e}")
        insights['top_special_projects'] = []
    
    try:
        insights['top_salary'] = get_top_salary(df, top_n=5)
    except Exception as e:
        print(f"âš ï¸  Error computing top_salary insights: {e}")
        insights['top_salary'] = []
    
    # Additional insights (by_employment_status, by_position, etc.)
    try:
        additional_insights = compute_additional_insights(df)
        insights.update(additional_insights)
    except Exception as e:
        print(f"âš ï¸  Error computing additional insights: {e}")
        # Continue without additional insights
    
    # Always return insights dict, even if some values are empty lists
    # This ensures the API can return the insights structure to the frontend
    print(f"âœ… Operational insights computation completed: {len(insights)} keys")
    for key, value in insights.items():
        if isinstance(value, list):
            print(f"   - {key}: {len(value)} items")
        elif isinstance(value, dict):
            print(f"   - {key}: {len(value)} items")
        else:
            print(f"   - {key}: {type(value).__name__}")
    
    # Store insights as facts for LLM access (so queries like "average salary in department 3" work)
    # Wrap in try-except to ensure computation doesn't fail if fact storage has issues
    try:
        store_operational_insights_as_facts(insights)
    except Exception as store_error:
        # Log error but don't fail - insights are still valid and can be used
        print(f"âš ï¸  Warning: Failed to store operational insights as facts: {store_error}")
        print(f"   Insights computed successfully but not stored in KG. This won't affect insight retrieval.")
        import traceback
        traceback.print_exc()
    
    # CRITICAL: Always return insights dict, never return empty dict
    # Even if all lists are empty, return the structure so frontend knows what to expect
    return insights


def find_manager_column(df: pd.DataFrame) -> Optional[str]:
    """Helper function to find manager column in DataFrame - prefers ManagerName over ManagerID"""
    # First try exact column name matching (case-insensitive) - prefer ManagerName
    manager_name_col = None
    manager_id_col = None
    
    for col in df.columns:
        col_lower = col.lower().strip()
        if col_lower == 'managername' or col_lower == 'manager_name':
            manager_name_col = col
        elif col_lower == 'managerid' or col_lower == 'manager_id':
            manager_id_col = col
    
    # Prefer ManagerName if both exist
    if manager_name_col:
        return manager_name_col
    elif manager_id_col:
        return manager_id_col
    
    # Try normalize_column_name
    mgr_col = normalize_column_name(df, "ManagerName")
    if mgr_col is None:
        mgr_col = normalize_column_name(df, "ManagerID")
    if mgr_col is None:
        mgr_col = normalize_column_name(df, "Manager")
    
    # If still None, try direct column matching with partial match
    if mgr_col is None:
        for col in df.columns:
            col_lower = col.lower()
            if 'manager' in col_lower and ('name' in col_lower or 'id' in col_lower or col_lower == 'manager'):
                mgr_col = col
                break
    
    if mgr_col and mgr_col in df.columns:
        return mgr_col
    
    return None


def group_by_department(df: pd.DataFrame) -> List[Dict[str, Any]]:
    """Group by Department and calculate avg PerformanceScoreID, avg Absences, avg Salary"""
    results = []
    
    try:
        if df is None or len(df) == 0:
            print(f"âš ï¸  group_by_department: DataFrame is None or empty")
            return results
        
        # Find department column
        dept_col = normalize_column_name(df, "Department")
        if dept_col is None:
            if "Department" in df.columns:
                dept_col = "Department"
            elif "DeptID" in df.columns:
                dept_col = "DeptID"
            else:
                print(f"âš ï¸  group_by_department: No department column found. Available columns: {list(df.columns)[:10]}")
                return results
        
        # Find performance score column - prefer PerfScoreID (numeric) over PerformanceScore (text)
        perf_col = normalize_column_name(df, "PerfScoreID")
        if perf_col is None:
            perf_col = normalize_column_name(df, "PerformanceScore")
        
        # Find absences column
        abs_col = normalize_column_name(df, "Absences")
        
        # Find salary column
        salary_col = normalize_column_name(df, "Salary")
        
        if dept_col not in df.columns:
            print(f"âš ï¸  group_by_department: Column '{dept_col}' not in DataFrame")
            return results
        
        # Group by department - ensure we get all unique departments
        dept_groups = df.groupby(dept_col)
        
        for dept, group_df in dept_groups:
            dept_name = str(dept).strip()
            
            # If department is numeric (DeptID), try to find actual department name
            if dept_name.replace('.', '').replace('-', '').isdigit():
                # Try to find a text Department column
                for col in df.columns:
                    if col != dept_col and ('department' in col.lower() or 'dept' in col.lower()):
                        dept_names = group_df[col].dropna().unique()
                        if len(dept_names) > 0:
                            text_name = str(dept_names[0]).strip()
                            if not text_name.replace('.', '').replace('-', '').isdigit():
                                dept_name = text_name
                                break
            
            dept_data = {
                "department": dept_name,
                "employee_count": len(group_df)
            }
            
            # Find satisfaction and engagement columns for department
            satisfaction_col = normalize_column_name(df, "Satisfaction")
            if satisfaction_col is None:
                satisfaction_col = normalize_column_name(df, "EngagementSurvey")
            
            engagement_col = normalize_column_name(df, "EngagementSurvey")
            
            # Average PerformanceScoreID - use PerfScoreID (numeric) not PerformanceScore (text)
            if perf_col and perf_col in group_df.columns:
                perf_series = pd.to_numeric(group_df[perf_col], errors='coerce')
                # Filter out NaN values for calculation
                perf_valid = perf_series.dropna()
                if len(perf_valid) > 0:
                    dept_data["avg_performance_score"] = sanitize_float(perf_valid.mean())
                else:
                    dept_data["avg_performance_score"] = None
            
            # Average Satisfaction
            if satisfaction_col and satisfaction_col in group_df.columns:
                sat_series = pd.to_numeric(group_df[satisfaction_col], errors='coerce')
                sat_valid = sat_series.dropna()
                if len(sat_valid) > 0:
                    dept_data["avg_satisfaction"] = sanitize_float(sat_valid.mean())
                else:
                    dept_data["avg_satisfaction"] = None
            
            # Average Engagement
            if engagement_col and engagement_col in group_df.columns:
                eng_series = pd.to_numeric(group_df[engagement_col], errors='coerce')
                eng_valid = eng_series.dropna()
                if len(eng_valid) > 0:
                    dept_data["avg_engagement"] = sanitize_float(eng_valid.mean())
                else:
                    dept_data["avg_engagement"] = None
            
            # Average Absences
            if abs_col and abs_col in group_df.columns:
                abs_series = pd.to_numeric(group_df[abs_col], errors='coerce')
                abs_valid = abs_series.dropna()
                if len(abs_valid) > 0:
                    dept_data["avg_absences"] = sanitize_float(abs_valid.mean())
                else:
                    dept_data["avg_absences"] = None
            
            # Average Salary - only average over valid numeric values in this group
            if salary_col and salary_col in group_df.columns:
                salary_series = pd.to_numeric(group_df[salary_col], errors='coerce')
                # Filter out NaN values for calculation - only average rows with valid salary in this department
                salary_valid = salary_series.dropna()
                if len(salary_valid) > 0:
                    dept_data["avg_salary"] = sanitize_float(salary_valid.mean())
                else:
                    dept_data["avg_salary"] = None
            
            results.append(dept_data)
        
        return results
    except Exception as e:
        print(f"âŒ Error in group_by_department: {e}")
        import traceback
        traceback.print_exc()
        return results


def group_by_manager(df: pd.DataFrame) -> List[Dict[str, Any]]:
    """Group by Manager (ID or name) and calculate comprehensive metrics: avg Performance, Satisfaction, Engagement, Absences, Salary"""
    results = []
    
    try:
        if df is None or len(df) == 0:
            print(f"âš ï¸  group_by_manager: DataFrame is None or empty")
            return results
        
        # Find manager column
        mgr_col = find_manager_column(df)
        if mgr_col is None:
            print(f"âš ï¸  group_by_manager: No manager column found. Available columns: {list(df.columns)[:10]}")
            return results
        
        # Find performance score column - prefer PerfScoreID (numeric) over PerformanceScore (text)
        perf_col = normalize_column_name(df, "PerfScoreID")
        if perf_col is None:
            perf_col = normalize_column_name(df, "PerformanceScore")
        
        # Find satisfaction column (might be EngagementSurvey or separate)
        satisfaction_col = normalize_column_name(df, "Satisfaction")
        if satisfaction_col is None:
            satisfaction_col = normalize_column_name(df, "EngagementSurvey")
        
        # Find engagement column
        engagement_col = normalize_column_name(df, "EngagementSurvey")
        
        # Find absences column
        abs_col = normalize_column_name(df, "Absences")
        
        # Find salary column
        salary_col = normalize_column_name(df, "Salary")
        
        if mgr_col not in df.columns:
            return results
        
        # Group by manager
        mgr_groups = df.groupby(mgr_col)
        
        for mgr, group_df in mgr_groups:
            mgr_name = str(mgr).strip()
            mgr_data = {
                "manager": mgr_name,
                "employee_count": len(group_df)
            }
            
            # Average PerformanceScoreID - filter out NaN values
            if perf_col and perf_col in group_df.columns:
                perf_series = pd.to_numeric(group_df[perf_col], errors='coerce')
                perf_valid = perf_series.dropna()
                if len(perf_valid) > 0:
                    mgr_data["avg_performance_score"] = sanitize_float(perf_valid.mean())
                else:
                    mgr_data["avg_performance_score"] = None
            
            # Average Satisfaction
            if satisfaction_col and satisfaction_col in group_df.columns:
                sat_series = pd.to_numeric(group_df[satisfaction_col], errors='coerce')
                sat_valid = sat_series.dropna()
                if len(sat_valid) > 0:
                    mgr_data["avg_satisfaction"] = sanitize_float(sat_valid.mean())
                else:
                    mgr_data["avg_satisfaction"] = None
            
            # Average Engagement
            if engagement_col and engagement_col in group_df.columns:
                eng_series = pd.to_numeric(group_df[engagement_col], errors='coerce')
                eng_valid = eng_series.dropna()
                if len(eng_valid) > 0:
                    mgr_data["avg_engagement"] = sanitize_float(eng_valid.mean())
                else:
                    mgr_data["avg_engagement"] = None
            
            # Average Absences
            if abs_col and abs_col in group_df.columns:
                abs_series = pd.to_numeric(group_df[abs_col], errors='coerce')
                abs_valid = abs_series.dropna()
                if len(abs_valid) > 0:
                    mgr_data["avg_absences"] = sanitize_float(abs_valid.mean())
                else:
                    mgr_data["avg_absences"] = None
            
            # Average Salary and Total Salary
            if salary_col and salary_col in group_df.columns:
                salary_series = pd.to_numeric(group_df[salary_col], errors='coerce')
                salary_valid = salary_series.dropna()
                if len(salary_valid) > 0:
                    mgr_data["avg_salary"] = sanitize_float(salary_valid.mean())
                    mgr_data["total_salary"] = sanitize_float(salary_valid.sum())
                else:
                    mgr_data["avg_salary"] = None
                    mgr_data["total_salary"] = None
            
            results.append(mgr_data)
        
        return results
    except Exception as e:
        print(f"âŒ Error in group_by_manager: {e}")
        import traceback
        traceback.print_exc()
        return results


def get_top_absences(df: pd.DataFrame, top_n: int = 5) -> List[Dict[str, Any]]:
    """Get top N employees with max Absences (only active employees)"""
    results = []
    
    # Filter for active employees only
    status_col = normalize_column_name(df, "EmploymentStatus")
    if status_col and status_col in df.columns:
        active_mask = df[status_col].astype(str).str.lower().str.contains('active', na=False)
        df_active = df[active_mask].copy()
    else:
        df_active = df.copy()
    
    if len(df_active) == 0:
        return results
    
    # Find absences column
    abs_col = normalize_column_name(df, "Absences")
    if abs_col is None or abs_col not in df_active.columns:
        return results
    
    # Find employee name column
    emp_name_col = "Employee_Name" if "Employee_Name" in df_active.columns else None
    if emp_name_col is None:
        for col in df_active.columns:
            if 'name' in col.lower() and 'employee' in col.lower():
                emp_name_col = col
                break
    
    # Convert to numeric
    abs_series = pd.to_numeric(df_active[abs_col], errors='coerce')
    
    # Get top N
    top_absences = abs_series.nlargest(top_n)
    
    # Find additional columns
    dept_col = normalize_column_name(df_active, "Department")
    mgr_col = find_manager_column(df_active)
    salary_col = normalize_column_name(df_active, "Salary")
    position_col = normalize_column_name(df_active, "Position")
    perf_col = normalize_column_name(df_active, "PerfScoreID")
    if perf_col is None:
        perf_col = normalize_column_name(df_active, "PerformanceScore")
    eng_col = normalize_column_name(df_active, "EngagementSurvey")
    sat_col = normalize_column_name(df_active, "EmpSatisfaction")
    dayslate_col = normalize_column_name(df_active, "DaysLateLast30")
    
    for idx, value in top_absences.items():
        emp_data = {
            "absences": sanitize_float(value) if not pd.isna(value) else None,
            "rank": len(results) + 1
        }
        
        if emp_name_col and emp_name_col in df_active.columns:
            emp_data["employee_name"] = str(df_active.loc[idx, emp_name_col])
        else:
            emp_data["employee_name"] = f"Employee {idx}"
        
        # Add position (actual position name, not ID)
        if position_col and position_col in df_active.columns:
            pos_val = df_active.loc[idx, position_col]
            if pd.notna(pos_val):
                emp_data["position"] = str(pos_val).strip()
        
        # Add department
        if dept_col and dept_col in df_active.columns:
            emp_data["department"] = str(df_active.loc[idx, dept_col])
        
        # Add manager
        if mgr_col and mgr_col in df_active.columns:
            emp_data["manager"] = str(df_active.loc[idx, mgr_col])
        
        # Add salary
        if salary_col and salary_col in df_active.columns:
            salary_val = pd.to_numeric(df_active.loc[idx, salary_col], errors='coerce')
            emp_data["salary"] = sanitize_float(salary_val) if not pd.isna(salary_val) else None
        
        # Add performance
        if perf_col and perf_col in df_active.columns:
            perf_val = pd.to_numeric(df_active.loc[idx, perf_col], errors='coerce')
            emp_data["performance_score"] = sanitize_float(perf_val) if not pd.isna(perf_val) else None
        
        # Add engagement
        if eng_col and eng_col in df_active.columns:
            eng_val = pd.to_numeric(df_active.loc[idx, eng_col], errors='coerce')
            emp_data["engagement_score"] = sanitize_float(eng_val) if not pd.isna(eng_val) else None
        
        # Add satisfaction
        if sat_col and sat_col in df_active.columns:
            sat_val = pd.to_numeric(df_active.loc[idx, sat_col], errors='coerce')
            emp_data["satisfaction_score"] = sanitize_float(sat_val) if not pd.isna(sat_val) else None
        
        # Add DaysLateLast30
        if dayslate_col and dayslate_col in df_active.columns:
            dayslate_val = pd.to_numeric(df_active.loc[idx, dayslate_col], errors='coerce')
            emp_data["days_late_last30"] = sanitize_float(dayslate_val) if not pd.isna(dayslate_val) else None
        
        results.append(emp_data)
    
    return results


def get_bottom_engagement(df: pd.DataFrame, bottom_n: int = 5) -> List[Dict[str, Any]]:
    """Get bottom N employees with min EngagementSurvey (only active employees)"""
    results = []
    
    # Filter for active employees only
    status_col = normalize_column_name(df, "EmploymentStatus")
    if status_col and status_col in df.columns:
        active_mask = df[status_col].astype(str).str.lower().str.contains('active', na=False)
        df_active = df[active_mask].copy()
    else:
        df_active = df.copy()
    
    if len(df_active) == 0:
        return results
    
    # Find engagement column
    eng_col = normalize_column_name(df_active, "EngagementSurvey")
    if eng_col is None or eng_col not in df_active.columns:
        return results
    
    # Find employee name column
    emp_name_col = "Employee_Name" if "Employee_Name" in df_active.columns else None
    if emp_name_col is None:
        for col in df_active.columns:
            if 'name' in col.lower() and 'employee' in col.lower():
                emp_name_col = col
                break
    
    # Convert to numeric
    eng_series = pd.to_numeric(df_active[eng_col], errors='coerce')
    
    # Get bottom N
    bottom_engagement = eng_series.nsmallest(bottom_n)
    
    # Find additional columns
    dept_col = normalize_column_name(df_active, "Department")
    mgr_col = find_manager_column(df_active)
    salary_col = normalize_column_name(df_active, "Salary")
    abs_col = normalize_column_name(df_active, "Absences")
    perf_col = normalize_column_name(df_active, "PerfScoreID")
    if perf_col is None:
        perf_col = normalize_column_name(df_active, "PerformanceScore")
    sat_col = normalize_column_name(df_active, "EmpSatisfaction")
    
    for idx, value in bottom_engagement.items():
        emp_data = {
            "engagement_score": sanitize_float(value) if not pd.isna(value) else None,
            "rank": len(results) + 1
        }
        
        if emp_name_col and emp_name_col in df_active.columns:
            emp_data["employee_name"] = str(df_active.loc[idx, emp_name_col])
        else:
            emp_data["employee_name"] = f"Employee {idx}"
        
        # Add department
        if dept_col and dept_col in df_active.columns:
            emp_data["department"] = str(df_active.loc[idx, dept_col])
        
        # Add manager
        if mgr_col and mgr_col in df_active.columns:
            emp_data["manager"] = str(df_active.loc[idx, mgr_col])
        
        # Add salary
        if salary_col and salary_col in df_active.columns:
            salary_val = pd.to_numeric(df_active.loc[idx, salary_col], errors='coerce')
            emp_data["salary"] = sanitize_float(salary_val) if not pd.isna(salary_val) else None
        
        # Add absences
        if abs_col and abs_col in df_active.columns:
            abs_val = pd.to_numeric(df_active.loc[idx, abs_col], errors='coerce')
            emp_data["absences"] = sanitize_float(abs_val) if not pd.isna(abs_val) else None
        
        # Add performance
        if perf_col and perf_col in df_active.columns:
            perf_val = pd.to_numeric(df_active.loc[idx, perf_col], errors='coerce')
            emp_data["performance_score"] = sanitize_float(perf_val) if not pd.isna(perf_val) else None
        
        # Add satisfaction
        if sat_col and sat_col in df_active.columns:
            sat_val = pd.to_numeric(df_active.loc[idx, sat_col], errors='coerce')
            emp_data["satisfaction_score"] = sanitize_float(sat_val) if not pd.isna(sat_val) else None
        
        results.append(emp_data)
    
    return results


def get_top_performance(df: pd.DataFrame, top_n: int = 5) -> List[Dict[str, Any]]:
    """Get top N employees with max PerformanceScore (only active employees)"""
    results = []
    
    # Filter for active employees only
    status_col = normalize_column_name(df, "EmploymentStatus")
    if status_col and status_col in df.columns:
        active_mask = df[status_col].astype(str).str.lower().str.contains('active', na=False)
        df_active = df[active_mask].copy()
    else:
        df_active = df.copy()
    
    if len(df_active) == 0:
        return results
    
    # Find performance column - prefer PerfScoreID (numeric) over PerformanceScore (text)
    perf_col = normalize_column_name(df_active, "PerfScoreID")
    if perf_col is None:
        perf_col = normalize_column_name(df_active, "PerformanceScore")
    
    if perf_col is None or perf_col not in df_active.columns:
        return results
    
    # Find employee name column
    emp_name_col = "Employee_Name" if "Employee_Name" in df_active.columns else None
    if emp_name_col is None:
        for col in df_active.columns:
            if 'name' in col.lower() and 'employee' in col.lower():
                emp_name_col = col
                break
    
    # Convert to numeric
    perf_series = pd.to_numeric(df_active[perf_col], errors='coerce')
    
    # Get top N
    top_performance = perf_series.nlargest(top_n)
    
    # Find additional columns
    dept_col = normalize_column_name(df_active, "Department")
    mgr_col = find_manager_column(df_active)
    salary_col = normalize_column_name(df_active, "Salary")
    abs_col = normalize_column_name(df_active, "Absences")
    
    for idx, value in top_performance.items():
        emp_data = {
            "performance_score": sanitize_float(value) if not pd.isna(value) else None,
            "rank": len(results) + 1
        }
        
        if emp_name_col and emp_name_col in df_active.columns:
            emp_data["employee_name"] = str(df_active.loc[idx, emp_name_col])
        else:
            emp_data["employee_name"] = f"Employee {idx}"
        
        # Add department
        if dept_col and dept_col in df_active.columns:
            emp_data["department"] = str(df_active.loc[idx, dept_col])
        
        # Add manager
        if mgr_col and mgr_col in df_active.columns:
            emp_data["manager"] = str(df_active.loc[idx, mgr_col])
        
        # Add salary
        if salary_col and salary_col in df_active.columns:
            salary_val = pd.to_numeric(df_active.loc[idx, salary_col], errors='coerce')
            emp_data["salary"] = sanitize_float(salary_val) if not pd.isna(salary_val) else None
        
        # Add absences
        if abs_col and abs_col in df_active.columns:
            abs_val = pd.to_numeric(df_active.loc[idx, abs_col], errors='coerce')
            emp_data["absences"] = sanitize_float(abs_val) if not pd.isna(abs_val) else None
        
        results.append(emp_data)
    
    return results


def get_bottom_performance(df: pd.DataFrame, bottom_n: int = 5) -> List[Dict[str, Any]]:
    """Get bottom N employees with min PerformanceScore (only active employees)"""
    results = []
    
    # Filter for active employees only
    status_col = normalize_column_name(df, "EmploymentStatus")
    if status_col and status_col in df.columns:
        active_mask = df[status_col].astype(str).str.lower().str.contains('active', na=False)
        df_active = df[active_mask].copy()
    else:
        df_active = df.copy()
    
    if len(df_active) == 0:
        return results
    
    # Find performance column - prefer PerfScoreID (numeric) over PerformanceScore (text)
    perf_col = normalize_column_name(df_active, "PerfScoreID")
    if perf_col is None:
        perf_col = normalize_column_name(df_active, "PerformanceScore")
    
    if perf_col is None or perf_col not in df_active.columns:
        return results
    
    # Find employee name column
    emp_name_col = "Employee_Name" if "Employee_Name" in df_active.columns else None
    if emp_name_col is None:
        for col in df_active.columns:
            if 'name' in col.lower() and 'employee' in col.lower():
                emp_name_col = col
                break
    
    # Convert to numeric
    perf_series = pd.to_numeric(df_active[perf_col], errors='coerce')
    
    # Get bottom N
    bottom_performance = perf_series.nsmallest(bottom_n)
    
    # Find additional columns
    dept_col = normalize_column_name(df_active, "Department")
    mgr_col = find_manager_column(df_active)
    salary_col = normalize_column_name(df_active, "Salary")
    abs_col = normalize_column_name(df_active, "Absences")
    
    for idx, value in bottom_performance.items():
        emp_data = {
            "performance_score": sanitize_float(value) if not pd.isna(value) else None,
            "rank": len(results) + 1
        }
        
        if emp_name_col and emp_name_col in df_active.columns:
            emp_data["employee_name"] = str(df_active.loc[idx, emp_name_col])
        else:
            emp_data["employee_name"] = f"Employee {idx}"
        
        # Add department
        if dept_col and dept_col in df_active.columns:
            emp_data["department"] = str(df_active.loc[idx, dept_col])
        
        # Add manager
        if mgr_col and mgr_col in df_active.columns:
            emp_data["manager"] = str(df_active.loc[idx, mgr_col])
        
        # Add salary
        if salary_col and salary_col in df_active.columns:
            salary_val = pd.to_numeric(df_active.loc[idx, salary_col], errors='coerce')
            emp_data["salary"] = sanitize_float(salary_val) if not pd.isna(salary_val) else None
        
        # Add absences
        if abs_col and abs_col in df_active.columns:
            abs_val = pd.to_numeric(df_active.loc[idx, abs_col], errors='coerce')
            emp_data["absences"] = sanitize_float(abs_val) if not pd.isna(abs_val) else None
        
        results.append(emp_data)
    
    return results


def get_top_special_projects(df: pd.DataFrame, top_n: int = 5) -> List[Dict[str, Any]]:
    """Get top N employees with max SpecialProjectsCount (only active employees)"""
    results = []
    
    # Filter for active employees only
    status_col = normalize_column_name(df, "EmploymentStatus")
    if status_col and status_col in df.columns:
        active_mask = df[status_col].astype(str).str.lower().str.contains('active', na=False)
        df_active = df[active_mask].copy()
    else:
        df_active = df.copy()
    
    if len(df_active) == 0:
        return results
    
    # Find special projects column
    sp_col = normalize_column_name(df_active, "SpecialProjectsCount")
    if sp_col is None or sp_col not in df_active.columns:
        return results
    
    # Find employee name column
    emp_name_col = "Employee_Name" if "Employee_Name" in df_active.columns else None
    if emp_name_col is None:
        for col in df_active.columns:
            if 'name' in col.lower() and 'employee' in col.lower():
                emp_name_col = col
                break
    
    # Convert to numeric
    sp_series = pd.to_numeric(df_active[sp_col], errors='coerce')
    
    # Get top N
    top_special_projects = sp_series.nlargest(top_n)
    
    # Find additional columns
    dept_col = normalize_column_name(df_active, "Department")
    mgr_col = find_manager_column(df_active)
    salary_col = normalize_column_name(df_active, "Salary")
    perf_col = normalize_column_name(df_active, "PerfScoreID")
    if perf_col is None:
        perf_col = normalize_column_name(df_active, "PerformanceScore")
    eng_col = normalize_column_name(df_active, "EngagementSurvey")
    sat_col = normalize_column_name(df_active, "EmpSatisfaction")
    
    for idx, value in top_special_projects.items():
        emp_data = {
            "special_projects_count": sanitize_float(value) if not pd.isna(value) else None,
            "rank": len(results) + 1
        }
        
        if emp_name_col and emp_name_col in df_active.columns:
            emp_data["employee_name"] = str(df_active.loc[idx, emp_name_col])
        else:
            emp_data["employee_name"] = f"Employee {idx}"
        
        # Add department
        if dept_col and dept_col in df_active.columns:
            emp_data["department"] = str(df_active.loc[idx, dept_col])
        
        # Add manager
        if mgr_col and mgr_col in df_active.columns:
            emp_data["manager"] = str(df_active.loc[idx, mgr_col])
        
        # Add salary
        if salary_col and salary_col in df_active.columns:
            salary_val = pd.to_numeric(df_active.loc[idx, salary_col], errors='coerce')
            emp_data["salary"] = sanitize_float(salary_val) if not pd.isna(salary_val) else None
        
        # Add performance
        if perf_col and perf_col in df_active.columns:
            perf_val = pd.to_numeric(df_active.loc[idx, perf_col], errors='coerce')
            emp_data["performance_score"] = sanitize_float(perf_val) if not pd.isna(perf_val) else None
        
        # Add engagement
        if eng_col and eng_col in df_active.columns:
            eng_val = pd.to_numeric(df_active.loc[idx, eng_col], errors='coerce')
            emp_data["engagement_score"] = sanitize_float(eng_val) if not pd.isna(eng_val) else None
        
        # Add satisfaction
        if sat_col and sat_col in df_active.columns:
            sat_val = pd.to_numeric(df_active.loc[idx, sat_col], errors='coerce')
            emp_data["satisfaction_score"] = sanitize_float(sat_val) if not pd.isna(sat_val) else None
        
        results.append(emp_data)
    
    return results


def get_top_salary(df: pd.DataFrame, top_n: int = 5) -> List[Dict[str, Any]]:
    """Get top N employees with max Salary (only active employees)"""
    results = []
    
    # Filter for active employees only
    status_col = normalize_column_name(df, "EmploymentStatus")
    if status_col and status_col in df.columns:
        active_mask = df[status_col].astype(str).str.lower().str.contains('active', na=False)
        df_active = df[active_mask].copy()
    else:
        df_active = df.copy()
    
    if len(df_active) == 0:
        return results
    
    # Find salary column
    salary_col = normalize_column_name(df_active, "Salary")
    if salary_col is None or salary_col not in df_active.columns:
        return results
    
    # Find employee name column
    emp_name_col = "Employee_Name" if "Employee_Name" in df_active.columns else None
    if emp_name_col is None:
        for col in df_active.columns:
            if 'name' in col.lower() and 'employee' in col.lower():
                emp_name_col = col
                break
    
    # Convert to numeric
    salary_series = pd.to_numeric(df_active[salary_col], errors='coerce')
    
    # Get top N
    top_salary = salary_series.nlargest(top_n)
    
    # Find additional columns
    dept_col = normalize_column_name(df_active, "Department")
    mgr_col = find_manager_column(df_active)
    perf_col = normalize_column_name(df_active, "PerfScoreID")
    if perf_col is None:
        perf_col = normalize_column_name(df_active, "PerformanceScore")
    eng_col = normalize_column_name(df_active, "EngagementSurvey")
    sat_col = normalize_column_name(df_active, "EmpSatisfaction")
    
    for idx, value in top_salary.items():
        emp_data = {
            "salary": sanitize_float(value) if not pd.isna(value) else None,
            "rank": len(results) + 1
        }
        
        if emp_name_col and emp_name_col in df_active.columns:
            emp_data["employee_name"] = str(df_active.loc[idx, emp_name_col])
        else:
            emp_data["employee_name"] = f"Employee {idx}"
        
        # Add department
        if dept_col and dept_col in df_active.columns:
            emp_data["department"] = str(df_active.loc[idx, dept_col])
        
        # Add manager
        if mgr_col and mgr_col in df_active.columns:
            emp_data["manager"] = str(df_active.loc[idx, mgr_col])
        
        # Add performance
        if perf_col and perf_col in df_active.columns:
            perf_val = pd.to_numeric(df_active.loc[idx, perf_col], errors='coerce')
            emp_data["performance_score"] = sanitize_float(perf_val) if not pd.isna(perf_val) else None
        
        # Add engagement
        if eng_col and eng_col in df_active.columns:
            eng_val = pd.to_numeric(df_active.loc[idx, eng_col], errors='coerce')
            emp_data["engagement_score"] = sanitize_float(eng_val) if not pd.isna(eng_val) else None
        
        # Add satisfaction
        if sat_col and sat_col in df_active.columns:
            sat_val = pd.to_numeric(df_active.loc[idx, sat_col], errors='coerce')
            emp_data["satisfaction_score"] = sanitize_float(sat_val) if not pd.isna(sat_val) else None
        
        results.append(emp_data)
    
    return results


def group_by_recruitment_source(df: pd.DataFrame) -> List[Dict[str, Any]]:
    """Group by RecruitmentSource and calculate avg PerformanceScoreID, avg Salary, etc."""
    results = []
    
    try:
        if df is None or len(df) == 0:
            print(f"âš ï¸  group_by_recruitment_source: DataFrame is None or empty")
            return results
        
        # Find recruitment source column
        source_col = normalize_column_name(df, "RecruitmentSource")
        if source_col is None:
            if "RecruitmentSource" in df.columns:
                source_col = "RecruitmentSource"
            else:
                print(f"âš ï¸  group_by_recruitment_source: No recruitment source column found. Available columns: {list(df.columns)[:10]}")
                return results
        
        # Find performance score column - prefer PerfScoreID (numeric) over PerformanceScore (text)
        perf_col = normalize_column_name(df, "PerfScoreID")
        if perf_col is None:
            perf_col = normalize_column_name(df, "PerformanceScore")
        
        # Find salary column
        salary_col = normalize_column_name(df, "Salary")
        
        # Find absences column
        abs_col = normalize_column_name(df, "Absences")
        
        # Find employment status column
        status_col = normalize_column_name(df, "EmploymentStatus")
        
        if source_col not in df.columns:
            print(f"âš ï¸  group_by_recruitment_source: Column '{source_col}' not in DataFrame")
            return results
        
        # Group by recruitment source
        source_groups = df.groupby(source_col)
        
        for source, group_df in source_groups:
            source_name = str(source).strip()
            employee_count = len(group_df)
            source_data = {
                "recruitment_source": source_name,
                "employee_count": employee_count
            }
            
            # Average PerformanceScoreID - filter out NaN values
            if perf_col and perf_col in group_df.columns:
                perf_series = pd.to_numeric(group_df[perf_col], errors='coerce')
                perf_valid = perf_series.dropna()
                if len(perf_valid) > 0:
                    source_data["avg_performance_score"] = sanitize_float(perf_valid.mean())
                else:
                    source_data["avg_performance_score"] = None
            
            # Average Salary
            if salary_col and salary_col in group_df.columns:
                salary_series = pd.to_numeric(group_df[salary_col], errors='coerce')
                salary_valid = salary_series.dropna()
                if len(salary_valid) > 0:
                    source_data["avg_salary"] = sanitize_float(salary_valid.mean())
                else:
                    source_data["avg_salary"] = None
            
            # Average Absences
            if abs_col and abs_col in group_df.columns:
                abs_series = pd.to_numeric(group_df[abs_col], errors='coerce')
                abs_valid = abs_series.dropna()
                if len(abs_valid) > 0:
                    source_data["avg_absences"] = sanitize_float(abs_valid.mean())
                else:
                    source_data["avg_absences"] = None
            
            # Average Satisfaction
            satisfaction_col = normalize_column_name(df, "Satisfaction")
            if satisfaction_col is None:
                satisfaction_col = normalize_column_name(df, "EmpSatisfaction")
            if satisfaction_col and satisfaction_col in group_df.columns:
                sat_series = pd.to_numeric(group_df[satisfaction_col], errors='coerce')
                sat_valid = sat_series.dropna()
                if len(sat_valid) > 0:
                    source_data["avg_satisfaction"] = sanitize_float(sat_valid.mean())
                else:
                    source_data["avg_satisfaction"] = None
            
            # Average Engagement
            engagement_col = normalize_column_name(df, "EngagementSurvey")
            if engagement_col and engagement_col in group_df.columns:
                eng_series = pd.to_numeric(group_df[engagement_col], errors='coerce')
                eng_valid = eng_series.dropna()
                if len(eng_valid) > 0:
                    source_data["avg_engagement"] = sanitize_float(eng_valid.mean())
                else:
                    source_data["avg_engagement"] = None
            
            # Total number of active employees
            if status_col and status_col in group_df.columns:
                active_count = len(group_df[group_df[status_col].astype(str).str.lower().str.contains('active', na=False)])
                source_data["active_employees"] = active_count
                # Calculate percentage of active employees
                if employee_count > 0:
                    source_data["active_percentage"] = sanitize_float((active_count / employee_count) * 100)
                else:
                    source_data["active_percentage"] = None
            else:
                source_data["active_employees"] = None
                source_data["active_percentage"] = None
            
            results.append(source_data)
        
        return results
    except Exception as e:
        print(f"âŒ Error in group_by_recruitment_source: {e}")
        import traceback
        traceback.print_exc()
        return results


def compute_additional_insights(df: pd.DataFrame) -> Dict[str, Any]:
    """Compute additional insights for other columns"""
    additional = {}
    
    # Group by Position/Role if available
    pos_col = normalize_column_name(df, "Position")
    if pos_col is None:
        pos_col = normalize_column_name(df, "Role")
    if pos_col and pos_col in df.columns:
        perf_col = normalize_column_name(df, "PerfScoreID")
        if perf_col is None:
            perf_col = normalize_column_name(df, "PerformanceScore")
        
        # Find other columns
        salary_col = normalize_column_name(df, "Salary")
        satisfaction_col = normalize_column_name(df, "Satisfaction")
        if satisfaction_col is None:
            satisfaction_col = normalize_column_name(df, "EngagementSurvey")
        engagement_col = normalize_column_name(df, "EngagementSurvey")
        abs_col = normalize_column_name(df, "Absences")
        status_col = normalize_column_name(df, "EmploymentStatus")
        
        if perf_col and perf_col in df.columns:
            pos_groups = df.groupby(pos_col)
            position_insights = []
            
            for pos, group_df in pos_groups:
                pos_data = {
                    "position": str(pos).strip(),
                    "employee_count": len(group_df)
                }
                
                # Average performance score
                perf_series = pd.to_numeric(group_df[perf_col], errors='coerce')
                perf_valid = perf_series.dropna()
                pos_data["avg_performance_score"] = sanitize_float(perf_valid.mean()) if len(perf_valid) > 0 else None
                
                # Average salary
                if salary_col and salary_col in group_df.columns:
                    salary_series = pd.to_numeric(group_df[salary_col], errors='coerce')
                    salary_valid = salary_series.dropna()
                    if len(salary_valid) > 0:
                        pos_data["avg_salary"] = sanitize_float(salary_valid.mean())
                    else:
                        pos_data["avg_salary"] = None
                
                # Total number with employment status active
                if status_col and status_col in group_df.columns:
                    active_count = len(group_df[group_df[status_col].astype(str).str.lower().str.contains('active', na=False)])
                    pos_data["active_employees"] = active_count
                else:
                    pos_data["active_employees"] = None
                
                # Average satisfaction
                if satisfaction_col and satisfaction_col in group_df.columns:
                    sat_series = pd.to_numeric(group_df[satisfaction_col], errors='coerce')
                    sat_valid = sat_series.dropna()
                    if len(sat_valid) > 0:
                        pos_data["avg_satisfaction"] = sanitize_float(sat_valid.mean())
                    else:
                        pos_data["avg_satisfaction"] = None
                
                # Average engagement
                if engagement_col and engagement_col in group_df.columns:
                    eng_series = pd.to_numeric(group_df[engagement_col], errors='coerce')
                    eng_valid = eng_series.dropna()
                    if len(eng_valid) > 0:
                        pos_data["avg_engagement"] = sanitize_float(eng_valid.mean())
                    else:
                        pos_data["avg_engagement"] = None
                
                # Average absences
                if abs_col and abs_col in group_df.columns:
                    abs_series = pd.to_numeric(group_df[abs_col], errors='coerce')
                    abs_valid = abs_series.dropna()
                    if len(abs_valid) > 0:
                        pos_data["avg_absences"] = sanitize_float(abs_valid.mean())
                    else:
                        pos_data["avg_absences"] = None
                
                position_insights.append(pos_data)
            
            additional["by_position"] = position_insights
    
    # Group by EmploymentStatus if available
    status_col = normalize_column_name(df, "EmploymentStatus")
    if status_col and status_col in df.columns:
        perf_col = normalize_column_name(df, "PerfScoreID")
        if perf_col is None:
            perf_col = normalize_column_name(df, "PerformanceScore")
        
        # Find other columns
        satisfaction_col = normalize_column_name(df, "Satisfaction")
        if satisfaction_col is None:
            satisfaction_col = normalize_column_name(df, "EngagementSurvey")
        engagement_col = normalize_column_name(df, "EngagementSurvey")
        abs_col = normalize_column_name(df, "Absences")
        
        if perf_col and perf_col in df.columns:
            status_groups = df.groupby(status_col)
            status_insights = []
            
            for status, group_df in status_groups:
                status_data = {
                    "employment_status": str(status).strip(),
                    "employee_count": len(group_df)
                }
                
                # Average performance score
                perf_series = pd.to_numeric(group_df[perf_col], errors='coerce')
                perf_valid = perf_series.dropna()
                status_data["avg_performance_score"] = sanitize_float(perf_valid.mean()) if len(perf_valid) > 0 else None
                
                # Average absences
                if abs_col and abs_col in group_df.columns:
                    abs_series = pd.to_numeric(group_df[abs_col], errors='coerce')
                    abs_valid = abs_series.dropna()
                    if len(abs_valid) > 0:
                        status_data["avg_absences"] = sanitize_float(abs_valid.mean())
                    else:
                        status_data["avg_absences"] = None
                else:
                    status_data["avg_absences"] = None
                
                # Average engagement survey
                if engagement_col and engagement_col in group_df.columns:
                    eng_series = pd.to_numeric(group_df[engagement_col], errors='coerce')
                    eng_valid = eng_series.dropna()
                    if len(eng_valid) > 0:
                        status_data["avg_engagement"] = sanitize_float(eng_valid.mean())
                    else:
                        status_data["avg_engagement"] = None
                
                # Average satisfaction
                if satisfaction_col and satisfaction_col in group_df.columns:
                    sat_series = pd.to_numeric(group_df[satisfaction_col], errors='coerce')
                    sat_valid = sat_series.dropna()
                    if len(sat_valid) > 0:
                        status_data["avg_satisfaction"] = sanitize_float(sat_valid.mean())
                    else:
                        status_data["avg_satisfaction"] = None
                
                status_insights.append(status_data)
            
            additional["by_employment_status"] = status_insights
    
    return additional


def group_by_manager_performance(df: pd.DataFrame) -> List[Dict[str, Any]]:
    """Extended manager analysis: Group by Manager and calculate comprehensive performance metrics"""
    results = []
    
    # Find manager column
    mgr_col = find_manager_column(df)
    if mgr_col is None:
        return results
    
    # Find performance score column - prefer PerfScoreID (numeric) over PerformanceScore (text)
    perf_col = normalize_column_name(df, "PerfScoreID")
    if perf_col is None:
        perf_col = normalize_column_name(df, "PerformanceScore")
    
    if mgr_col not in df.columns:
        return results
    
    # Group by manager
    mgr_groups = df.groupby(mgr_col)
    
    for mgr, group_df in mgr_groups:
        mgr_name = str(mgr).strip()
        mgr_data = {
            "manager": mgr_name,
            "employee_count": len(group_df)
        }
        
        # Performance metrics - filter out NaN values
        if perf_col and perf_col in group_df.columns:
            perf_series = pd.to_numeric(group_df[perf_col], errors='coerce')
            perf_valid = perf_series.dropna()
            if len(perf_valid) > 0:
                mgr_data["avg_performance_score"] = sanitize_float(perf_valid.mean())
                mgr_data["min_performance_score"] = sanitize_float(perf_valid.min())
                mgr_data["max_performance_score"] = sanitize_float(perf_valid.max())
                mgr_data["median_performance_score"] = sanitize_float(perf_valid.median())
                mgr_data["std_performance_score"] = sanitize_float(perf_valid.std())
            else:
                mgr_data["avg_performance_score"] = None
                mgr_data["min_performance_score"] = None
                mgr_data["max_performance_score"] = None
                mgr_data["median_performance_score"] = None
                mgr_data["std_performance_score"] = None
        
        # Department distribution for this manager
        if "Department" in group_df.columns:
            dept_counts = group_df["Department"].value_counts().to_dict()
            mgr_data["departments"] = dept_counts
        
        results.append(mgr_data)
    
    return results


def group_by_manager_absences(df: pd.DataFrame) -> List[Dict[str, Any]]:
    """Group by Manager and calculate absence metrics"""
    results = []
    
    # Find manager column
    mgr_col = find_manager_column(df)
    if mgr_col is None:
        return results
    
    # Find absences column
    abs_col = normalize_column_name(df, "Absences")
    
    # Group by manager
    mgr_groups = df.groupby(mgr_col)
    
    for mgr, group_df in mgr_groups:
        mgr_name = str(mgr).strip()
        mgr_data = {
            "manager": mgr_name,
            "employee_count": len(group_df)
        }
        
        # Absence metrics
        if abs_col and abs_col in group_df.columns:
            abs_series = pd.to_numeric(group_df[abs_col], errors='coerce')
            abs_valid = abs_series.dropna()
            if len(abs_valid) > 0:
                mgr_data["avg_absences"] = sanitize_float(abs_valid.mean())
                mgr_data["min_absences"] = sanitize_float(abs_valid.min())
                mgr_data["max_absences"] = sanitize_float(abs_valid.max())
            else:
                mgr_data["avg_absences"] = None
                mgr_data["min_absences"] = None
                mgr_data["max_absences"] = None
            abs_valid = abs_series.dropna()
            mgr_data["total_absences"] = sanitize_float(abs_valid.sum()) if len(abs_valid) > 0 else None
        
        results.append(mgr_data)
    
    return results


def group_by_manager_salary(df: pd.DataFrame) -> List[Dict[str, Any]]:
    """Group by Manager and calculate salary metrics"""
    results = []
    
    # Find manager column
    mgr_col = find_manager_column(df)
    if mgr_col is None:
        return results
    
    # Find salary column
    salary_col = normalize_column_name(df, "Salary")
    
    # Group by manager
    mgr_groups = df.groupby(mgr_col)
    
    for mgr, group_df in mgr_groups:
        mgr_name = str(mgr).strip()
        mgr_data = {
            "manager": mgr_name,
            "employee_count": len(group_df)
        }
        
        # Salary metrics
        if salary_col and salary_col in group_df.columns:
            salary_series = pd.to_numeric(group_df[salary_col], errors='coerce')
            salary_valid = salary_series.dropna()
            if len(salary_valid) > 0:
                mgr_data["avg_salary"] = sanitize_float(salary_valid.mean())
                mgr_data["min_salary"] = sanitize_float(salary_valid.min())
                mgr_data["max_salary"] = sanitize_float(salary_valid.max())
            else:
                mgr_data["avg_salary"] = None
                mgr_data["min_salary"] = None
                mgr_data["max_salary"] = None
            salary_valid = salary_series.dropna()
            mgr_data["total_salary"] = sanitize_float(salary_valid.sum()) if len(salary_valid) > 0 else None
        
        results.append(mgr_data)
    
    return results


def group_by_manager_department(df: pd.DataFrame) -> List[Dict[str, Any]]:
    """Group by Manager and Department combination"""
    results = []
    
    # Find manager column
    mgr_col = find_manager_column(df)
    if mgr_col is None:
        return results
    
    # Find department column
    dept_col = normalize_column_name(df, "Department")
    if dept_col is None:
        if "Department" in df.columns:
            dept_col = "Department"
        elif "DeptID" in df.columns:
            dept_col = "DeptID"
        else:
            return results
    
    # Find performance score column - prefer PerfScoreID (numeric) over PerformanceScore (text)
    perf_col = normalize_column_name(df, "PerfScoreID")
    if perf_col is None:
        perf_col = normalize_column_name(df, "PerformanceScore")
    
    if mgr_col not in df.columns or dept_col not in df.columns:
        return results
    
    # Group by manager and department
    mgr_dept_groups = df.groupby([mgr_col, dept_col])
    
    for (mgr, dept), group_df in mgr_dept_groups:
        mgr_name = str(mgr).strip()
        dept_name = str(dept).strip()
        
        mgr_dept_data = {
            "manager": mgr_name,
            "department": dept_name,
            "employee_count": len(group_df)
        }
        
        # Performance metrics - filter out NaN values
        if perf_col and perf_col in group_df.columns:
            perf_series = pd.to_numeric(group_df[perf_col], errors='coerce')
            perf_valid = perf_series.dropna()
            mgr_dept_data["avg_performance_score"] = sanitize_float(perf_valid.mean()) if len(perf_valid) > 0 else None
        
        # Absence metrics
        abs_col = normalize_column_name(df, "Absences")
        if abs_col and abs_col in group_df.columns:
            abs_series = pd.to_numeric(group_df[abs_col], errors='coerce')
            abs_valid = abs_series.dropna()
            mgr_dept_data["avg_absences"] = sanitize_float(abs_valid.mean()) if len(abs_valid) > 0 else None
        
        # Engagement metrics
        eng_col = normalize_column_name(df, "EngagementSurvey")
        if eng_col and eng_col in group_df.columns:
            eng_series = pd.to_numeric(group_df[eng_col], errors='coerce')
            eng_valid = eng_series.dropna()
            mgr_dept_data["avg_engagement"] = sanitize_float(eng_valid.mean()) if len(eng_valid) > 0 else None
        
        results.append(mgr_dept_data)
    
    return results


def get_top_managers_by_performance(df: pd.DataFrame, top_n: int = 10) -> List[Dict[str, Any]]:
    """Get top N managers by average team performance"""
    results = []
    
    # Find manager column
    mgr_col = find_manager_column(df)
    if mgr_col is None:
        return results
    
    # Find performance score column - prefer PerfScoreID (numeric) over PerformanceScore (text)
    perf_col = normalize_column_name(df, "PerfScoreID")
    if perf_col is None:
        perf_col = normalize_column_name(df, "PerformanceScore")
    
    if mgr_col not in df.columns or not perf_col or perf_col not in df.columns:
        return results
    
    # Group by manager and calculate average performance - filter out NaN
    mgr_perf = df.groupby(mgr_col)[perf_col].apply(
        lambda x: pd.to_numeric(x, errors='coerce').dropna().mean() if len(pd.to_numeric(x, errors='coerce').dropna()) > 0 else None
    ).dropna().sort_values(ascending=False)
    
    # Get top N
    top_managers = mgr_perf.head(top_n)
    
    for rank, (mgr, avg_perf) in enumerate(top_managers.items(), 1):
        mgr_name = str(mgr).strip()
        mgr_df = df[df[mgr_col] == mgr]
        
        mgr_data = {
            "manager": mgr_name,
            "rank": rank,
            "avg_performance_score": sanitize_float(avg_perf) if not pd.isna(avg_perf) else None,
            "employee_count": len(mgr_df)
        }
        
        # Add department info
        if "Department" in mgr_df.columns:
            dept_counts = mgr_df["Department"].value_counts().to_dict()
            mgr_data["departments"] = dept_counts
        
        results.append(mgr_data)
    
    return results


def get_bottom_managers_by_engagement(df: pd.DataFrame, bottom_n: int = 10) -> List[Dict[str, Any]]:
    """Get bottom N managers by average team engagement"""
    results = []
    
    # Find manager column
    mgr_col = find_manager_column(df)
    if mgr_col is None:
        return results
    
    # Find engagement column
    eng_col = normalize_column_name(df, "EngagementSurvey")
    
    if mgr_col not in df.columns or not eng_col or eng_col not in df.columns:
        return results
    
    # Group by manager and calculate average engagement - filter out NaN
    mgr_eng = df.groupby(mgr_col)[eng_col].apply(
        lambda x: pd.to_numeric(x, errors='coerce').dropna().mean() if len(pd.to_numeric(x, errors='coerce').dropna()) > 0 else None
    ).dropna().sort_values(ascending=True)
    
    # Get bottom N
    bottom_managers = mgr_eng.head(bottom_n)
    
    for rank, (mgr, avg_eng) in enumerate(bottom_managers.items(), 1):
        mgr_name = str(mgr).strip()
        mgr_df = df[df[mgr_col] == mgr]
        
        mgr_data = {
            "manager": mgr_name,
            "rank": rank,
            "avg_engagement": sanitize_float(avg_eng) if not pd.isna(avg_eng) else None,
            "employee_count": len(mgr_df)
        }
        
        # Add department info
        if "Department" in mgr_df.columns:
            dept_counts = mgr_df["Department"].value_counts().to_dict()
            mgr_data["departments"] = dept_counts
        
        results.append(mgr_data)
    
    return results


def store_operational_insights_as_facts(insights: Dict[str, Any]) -> None:
    """
    Store operational insights as facts in the knowledge base.
    This allows the LLM to access these insights when answering questions.
    
    This function is designed to handle large datasets gracefully by:
    - Catching and logging errors for individual fact storage operations
    - Continuing even if some facts fail to store
    - Limiting the number of fact formats stored for very large datasets
    - Removing old rounded facts before storing new precise ones
    """
    try:
        from knowledge import add_to_graph, graph, fact_exists
        from datetime import datetime
        import rdflib
        from urllib.parse import quote
        
        # CRITICAL: Remove old operational insights facts with rounded values
        # This ensures we don't have duplicate/conflicting facts with "4" vs "4.43"
        print(f"ðŸ§¹ Cleaning up old operational insights facts with rounded values...")
        try:
            # Find and remove facts about manager engagement that might have rounded values
            # We'll remove facts that match the pattern but have integer values
            triples_to_remove = []
            for s, p, o in graph:
                try:
                    subject_str = str(s)
                    predicate_str = str(p)
                    object_str = str(o)
                    
                    # Check if this is an operational insights fact about manager engagement
                    if ("manager" in subject_str.lower() or "amy dunn" in subject_str.lower() or 
                        "michael albert" in subject_str.lower() or "simon roup" in subject_str.lower()):
                        if ("engagement" in predicate_str.lower() or "engagement" in object_str.lower()):
                            # Check if object is a rounded integer (like "4" instead of "4.43")
                            try:
                                obj_val = float(object_str)
                                # If it's a whole number (like 4.0), it's likely a rounded old fact
                                if obj_val == int(obj_val) and obj_val < 5:
                                    triples_to_remove.append((s, p, o))
                                    print(f"   ðŸ—‘ï¸  Removing old rounded fact: {subject_str[:50]}... {predicate_str[:50]}... {object_str}")
                            except (ValueError, TypeError):
                                pass
                except Exception:
                    pass
            
            # Remove the old facts
            for triple in triples_to_remove:
                try:
                    graph.remove(triple)
                except Exception:
                    pass
            
            if triples_to_remove:
                print(f"âœ… Removed {len(triples_to_remove)} old rounded operational insights facts")
                from knowledge import save_knowledge_graph
                save_knowledge_graph()
        except Exception as cleanup_error:
            print(f"âš ï¸  Warning: Error cleaning up old facts: {cleanup_error}")
            # Continue anyway - we'll still store new facts
        
        # Count total items to store (for large dataset handling)
        total_items = 0
        if 'by_department' in insights:
            total_items += len(insights['by_department'])
        if 'by_manager' in insights:
            total_items += len(insights['by_manager'])
        if 'by_recruitment_source' in insights:
            total_items += len(insights['by_recruitment_source'])
        
        # For very large datasets (>100 departments/managers), limit fact formats to avoid overwhelming the KG
        use_limited_formats = total_items > 100
        if use_limited_formats:
            print(f"ðŸ“Š Large dataset detected ({total_items} items). Using optimized fact storage.")
        
        # Store department insights - make them queryable for questions like "average salary in department 3"
        if 'by_department' in insights:
            for dept_data in insights['by_department']:
                dept_name = str(dept_data.get('department', 'Unknown')).strip()
                # Normalize department name (handle "3.0" -> "3" for better matching)
                dept_name_normalized = dept_name
                try:
                    if dept_name.replace('.', '').replace('-', '').isdigit():
                        dept_num = float(dept_name)
                        if dept_num.is_integer():
                            dept_name_normalized = str(int(dept_num))
                except:
                    pass
                
                # Store multiple fact formats for better queryability
                dept_employee_count = dept_data.get('employee_count', 0)
                
                # Average performance score
                if dept_data.get('avg_performance_score') is not None:
                    avg_perf = dept_data['avg_performance_score']
                    # Store in multiple formats for better queryability (limit formats for large datasets)
                    if use_limited_formats:
                        fact_texts = [
                            f"Department {dept_name_normalized} has average performance score of {avg_perf:.2f}",
                            f"The average performance score in department {dept_name_normalized} is {avg_perf:.2f}",
                        ]
                    else:
                        fact_texts = [
                        f"Department {dept_name} has average performance score of {avg_perf:.2f}",
                        f"Department {dept_name_normalized} has average performance score of {avg_perf:.2f}",
                        f"The average performance score in department {dept_name} is {avg_perf:.2f}",
                        f"The average performance score in department {dept_name_normalized} is {avg_perf:.2f}",
                    ]
                    for fact_text in fact_texts:
                        try:
                            add_to_graph(
                            fact_text,
                            source_document="operational_insights",
                            uploaded_at=datetime.now().isoformat(),
                            agent_id="operational_query_agent"
                        )
                        except Exception as fact_error:
                            # Log but continue - don't fail entire operation for one fact
                            print(f"âš ï¸  Warning: Failed to store fact '{fact_text[:50]}...': {fact_error}")
                            continue
                
                # Average absences
                if dept_data.get('avg_absences') is not None:
                    avg_abs = dept_data['avg_absences']
                    if use_limited_formats:
                        fact_texts = [
                            f"Department {dept_name_normalized} has average absences of {avg_abs:.2f} days",
                            f"The average absences in department {dept_name_normalized} is {avg_abs:.2f} days",
                        ]
                    else:
                        fact_texts = [
                        f"Department {dept_name} has average absences of {avg_abs:.2f} days",
                        f"Department {dept_name_normalized} has average absences of {avg_abs:.2f} days",
                        f"The average absences in department {dept_name} is {avg_abs:.2f} days",
                        f"The average absences in department {dept_name_normalized} is {avg_abs:.2f} days",
                    ]
                    for fact_text in fact_texts:
                        try:
                            add_to_graph(
                            fact_text,
                            source_document="operational_insights",
                            uploaded_at=datetime.now().isoformat(),
                            agent_id="operational_query_agent"
                        )
                        except Exception as fact_error:
                            print(f"âš ï¸  Warning: Failed to store fact '{fact_text[:50]}...': {fact_error}")
                            continue
                
                # Average salary - CRITICAL for queries like "average salary in department 3"
                if dept_data.get('avg_salary') is not None:
                    avg_sal = dept_data['avg_salary']
                    if use_limited_formats:
                        fact_texts = [
                            f"Department {dept_name_normalized} has average salary of {avg_sal:.2f}",
                            f"The average salary in department {dept_name_normalized} is {avg_sal:.2f}",
                            f"Average salary for department {dept_name_normalized} is {avg_sal:.2f}",
                        ]
                    else:
                        fact_texts = [
                        f"Department {dept_name} has average salary of {avg_sal:.2f}",
                        f"Department {dept_name_normalized} has average salary of {avg_sal:.2f}",
                        f"The average salary in department {dept_name} is {avg_sal:.2f}",
                        f"The average salary in department {dept_name_normalized} is {avg_sal:.2f}",
                        f"Average salary for department {dept_name} is {avg_sal:.2f}",
                        f"Average salary for department {dept_name_normalized} is {avg_sal:.2f}",
                    ]
                    for fact_text in fact_texts:
                        try:
                            add_to_graph(
                            fact_text,
                            source_document="operational_insights",
                            uploaded_at=datetime.now().isoformat(),
                            agent_id="operational_query_agent"
                        )
                        except Exception as fact_error:
                            print(f"âš ï¸  Warning: Failed to store fact '{fact_text[:50]}...': {fact_error}")
                            continue
                
                # Store employee count
                if dept_employee_count > 0:
                    fact_text = f"Department {dept_name_normalized} has {dept_employee_count} employees"
                    try:
                        add_to_graph(
                        fact_text,
                        source_document="operational_insights",
                        uploaded_at=datetime.now().isoformat(),
                        agent_id="operational_query_agent"
                    )
                    except Exception as fact_error:
                        print(f"âš ï¸  Warning: Failed to store fact '{fact_text[:50]}...': {fact_error}")
                        continue
        
        # Store manager insights
        if 'by_manager' in insights:
            for mgr_data in insights['by_manager']:
                mgr_name = str(mgr_data.get('manager', 'Unknown')).strip()
                mgr_team_size = mgr_data.get('employee_count', 0)
                
                # Average performance score
                if mgr_data.get('avg_performance_score') is not None:
                    avg_perf = mgr_data['avg_performance_score']
                    if use_limited_formats:
                        fact_texts = [
                            f"Manager {mgr_name} has average team performance score of {avg_perf:.2f}",
                            f"The average performance score for manager {mgr_name}'s team is {avg_perf:.2f}",
                        ]
                    else:
                        fact_texts = [
                        f"Manager {mgr_name} has average team performance score of {avg_perf:.2f}",
                        f"Manager {mgr_name} manages a team with average performance score of {avg_perf:.2f}",
                        f"The average performance score for manager {mgr_name}'s team is {avg_perf:.2f}",
                    ]
                    for fact_text in fact_texts:
                        try:
                            add_to_graph(
                            fact_text,
                            source_document="operational_insights",
                            uploaded_at=datetime.now().isoformat(),
                            agent_id="operational_query_agent"
                        )
                        except Exception as fact_error:
                            print(f"âš ï¸  Warning: Failed to store fact '{fact_text[:50]}...': {fact_error}")
                            continue
                
                # Average engagement - Store with multiple formats for better queryability
                if mgr_data.get('avg_engagement') is not None:
                    avg_eng = mgr_data['avg_engagement']
                    # Ensure we use precise value (2 decimal places)
                    avg_eng_precise = round(float(avg_eng), 2)
                    if use_limited_formats:
                        fact_texts = [
                            f"Manager {mgr_name} has average engagement survey value of {avg_eng_precise:.2f}",
                            f"Manager {mgr_name} has average team engagement score of {avg_eng_precise:.2f}",
                        ]
                    else:
                        fact_texts = [
                            f"Manager {mgr_name} has average engagement survey value of {avg_eng_precise:.2f}",
                            f"Manager {mgr_name} has average engagement survey score of {avg_eng_precise:.2f}",
                            f"Manager {mgr_name} has average team engagement score of {avg_eng_precise:.2f}",
                            f"Manager {mgr_name} manages a team with average engagement score of {avg_eng_precise:.2f}",
                            f"The average engagement survey value for manager {mgr_name} is {avg_eng_precise:.2f}",
                            f"The average engagement score for manager {mgr_name}'s team is {avg_eng_precise:.2f}",
                            f"Manager {mgr_name}'s team has an average engagement survey value of {avg_eng_precise:.2f}",
                        ]
                    for fact_text in fact_texts:
                        try:
                            add_to_graph(
                                fact_text,
                                source_document="operational_insights",
                                uploaded_at=datetime.now().isoformat(),
                                agent_id="operational_query_agent"
                            )
                        except Exception as fact_error:
                            print(f"âš ï¸  Warning: Failed to store fact '{fact_text[:50]}...': {fact_error}")
                            continue
                
                # Average satisfaction
                if mgr_data.get('avg_satisfaction') is not None:
                    avg_sat = mgr_data['avg_satisfaction']
                    fact_text = f"Manager {mgr_name} has average team satisfaction score of {avg_sat:.2f}"
                    try:
                        add_to_graph(
                        fact_text,
                        source_document="operational_insights",
                        uploaded_at=datetime.now().isoformat(),
                        agent_id="operational_query_agent"
                    )
                    except Exception as fact_error:
                        print(f"âš ï¸  Warning: Failed to store fact '{fact_text[:50]}...': {fact_error}")
                        continue
                
                # Team size
                if mgr_team_size > 0:
                    fact_text = f"Manager {mgr_name} manages {mgr_team_size} employees"
                    try:
                        add_to_graph(
                        fact_text,
                        source_document="operational_insights",
                        uploaded_at=datetime.now().isoformat(),
                        agent_id="operational_query_agent"
                    )
                    except Exception as fact_error:
                        print(f"âš ï¸  Warning: Failed to store fact '{fact_text[:50]}...': {fact_error}")
                        continue
        
        # Store top absences - multiple formats for better queryability
        if 'top_absences' in insights:
            for i, emp_data in enumerate(insights['top_absences'][:5], 1):
                emp_name = emp_data.get('employee_name', 'Unknown')
                absences = emp_data.get('absences')
                dept = emp_data.get('department', '')
                position = emp_data.get('position', '')
                
                if absences is not None:
                    # Store in multiple formats for better queryability
                    fact_texts = [
                        f"Employee {emp_name} has {absences:.0f} absences (rank {i} highest)",
                        f"{emp_name} has {absences:.0f} absences and is ranked {i} in top employees by absences",
                        f"Top {i} employee by absences is {emp_name} with {absences:.0f} absences",
                        f"The employee with rank {i} highest absences is {emp_name} with {absences:.0f} absences",
                    ]
                    if dept:
                        fact_texts.append(f"{emp_name} from {dept} department has {absences:.0f} absences (rank {i})")
                    if position:
                        fact_texts.append(f"{emp_name} ({position}) has {absences:.0f} absences (rank {i})")
                    
                    for fact_text in fact_texts:
                        try:
                            add_to_graph(
                            fact_text,
                            source_document="operational_insights",
                            uploaded_at=datetime.now().isoformat(),
                            agent_id="operational_query_agent"
                        )
                        except Exception as fact_error:
                            print(f"âš ï¸  Warning: Failed to store fact '{fact_text[:50]}...': {fact_error}")
                            continue
            
            # Store summary fact for "top 5 employees by absences" queries
            if len(insights['top_absences']) >= 5:
                top5_list = []
                for i, emp_data in enumerate(insights['top_absences'][:5], 1):
                    emp_name = emp_data.get('employee_name', 'Unknown')
                    absences = emp_data.get('absences', 0)
                    top5_list.append(f"{i}. {emp_name} ({absences:.0f} absences)")
                
                summary_fact = f"Top 5 employees by absences: {', '.join(top5_list)}"
                try:
                    add_to_graph(
                    summary_fact,
                    source_document="operational_insights",
                    uploaded_at=datetime.now().isoformat(),
                    agent_id="operational_query_agent"
                )
                except Exception as fact_error:
                    print(f"âš ï¸  Warning: Failed to store summary fact: {fact_error}")
                    pass
        
        # Store bottom engagement - multiple formats for better queryability
        if 'bottom_engagement' in insights:
            for i, emp_data in enumerate(insights['bottom_engagement'][:5], 1):
                emp_name = emp_data.get('employee_name', 'Unknown')
                engagement = emp_data.get('engagement_score')
                dept = emp_data.get('department', '')
                manager = emp_data.get('manager', '')
                
                if engagement is not None:
                    fact_texts = [
                        f"Employee {emp_name} has engagement score of {engagement:.2f} (rank {i} lowest)",
                        f"{emp_name} has engagement score of {engagement:.2f} and is ranked {i} in bottom employees by engagement",
                        f"Bottom {i} employee by engagement is {emp_name} with engagement score of {engagement:.2f}",
                        f"The employee with rank {i} lowest engagement is {emp_name} with engagement score of {engagement:.2f}",
                    ]
                    if dept:
                        fact_texts.append(f"{emp_name} from {dept} department has engagement score of {engagement:.2f} (rank {i} lowest)")
                    if manager:
                        fact_texts.append(f"{emp_name} (managed by {manager}) has engagement score of {engagement:.2f} (rank {i} lowest)")
                    
                    for fact_text in fact_texts:
                        try:
                            add_to_graph(
                            fact_text,
                            source_document="operational_insights",
                            uploaded_at=datetime.now().isoformat(),
                            agent_id="operational_query_agent"
                        )
                        except Exception as fact_error:
                            print(f"âš ï¸  Warning: Failed to store fact '{fact_text[:50]}...': {fact_error}")
                            continue
            
            # Store summary fact for "bottom 5 employees by engagement" queries
            if len(insights['bottom_engagement']) >= 5:
                bottom5_list = []
                for i, emp_data in enumerate(insights['bottom_engagement'][:5], 1):
                    emp_name = emp_data.get('employee_name', 'Unknown')
                    engagement = emp_data.get('engagement_score', 0)
                    bottom5_list.append(f"{i}. {emp_name} ({engagement:.2f})")
                
                summary_fact = f"Bottom 5 employees by engagement: {', '.join(bottom5_list)}"
                try:
                    add_to_graph(
                    summary_fact,
                    source_document="operational_insights",
                    uploaded_at=datetime.now().isoformat(),
                    agent_id="operational_query_agent"
                )
                except Exception as fact_error:
                    print(f"âš ï¸  Warning: Failed to store summary fact: {fact_error}")
                    pass
        
        # Store recruitment source insights
        if 'by_recruitment_source' in insights:
            for source_data in insights['by_recruitment_source']:
                source_name = source_data.get('recruitment_source', 'Unknown')
                employee_count = source_data.get('employee_count', 0)
                
                if source_data.get('avg_performance_score') is not None:
                    avg_perf = source_data['avg_performance_score']
                    fact_texts = [
                        f"Recruitment source {source_name} has average performance score of {avg_perf:.2f}",
                        f"Recruitment source {source_name} recruited {employee_count} employees with average performance score of {avg_perf:.2f}",
                        f"The average performance score for recruitment source {source_name} is {avg_perf:.2f}",
                    ]
                    for fact_text in fact_texts:
                        try:
                            add_to_graph(
                            fact_text,
                            source_document="operational_insights",
                            uploaded_at=datetime.now().isoformat(),
                            agent_id="operational_query_agent"
                        )
                        except Exception as fact_error:
                            print(f"âš ï¸  Warning: Failed to store fact '{fact_text[:50]}...': {fact_error}")
                            continue
                
                if source_data.get('avg_salary') is not None:
                    avg_sal = source_data['avg_salary']
                    fact_texts = [
                        f"Recruitment source {source_name} has average salary of {avg_sal:.2f}",
                        f"Employees from recruitment source {source_name} have average salary of {avg_sal:.2f}",
                    ]
                    for fact_text in fact_texts:
                        try:
                            add_to_graph(
                            fact_text,
                            source_document="operational_insights",
                            uploaded_at=datetime.now().isoformat(),
                            agent_id="operational_query_agent"
                        )
                        except Exception as fact_error:
                            print(f"âš ï¸  Warning: Failed to store fact '{fact_text[:50]}...': {fact_error}")
                            continue
                
                if source_data.get('avg_absences') is not None:
                    avg_abs = source_data['avg_absences']
                    fact_text = f"Recruitment source {source_name} has average absences of {avg_abs:.2f} days"
                    try:
                        add_to_graph(
                        fact_text,
                        source_document="operational_insights",
                        uploaded_at=datetime.now().isoformat(),
                        agent_id="operational_query_agent"
                    )
                    except Exception as fact_error:
                        print(f"âš ï¸  Warning: Failed to store fact '{fact_text[:50]}...': {fact_error}")
                        continue
        
    except Exception as e:
        print(f"âš ï¸  Error storing operational insights: {e}")
        import traceback
        traceback.print_exc()
        # Don't re-raise - allow insights to be used even if fact storage fails


def process_operational_query(query_info: Dict[str, Any], question: str) -> Tuple[Optional[str], List[Dict[str, Any]], Dict[str, Any]]:
    """
    Process operational query by retrieving facts from knowledge graph and answering the question.
    First checks for existing insights, then ensures they're stored as facts.
    """
    # First, try to get existing insights from document agents or documents_store
    insights = None
    
    try:
        from agent_system import document_agents
        # Check document agents for cached insights
        for agent_id, agent in document_agents.items():
            if hasattr(agent, 'document_type') and agent.document_type.lower() == '.csv':
                metadata = getattr(agent, 'metadata', {})
                if 'operational_insights' in metadata:
                    cached_insights = metadata.get('operational_insights')
                    if cached_insights and isinstance(cached_insights, dict) and len(cached_insights) > 0:
                        insights = cached_insights
                        break
    except Exception:
        pass
    
    # If no cached insights, try documents_store
    if not insights:
        try:
            from documents_store import get_all_documents
            documents = get_all_documents()
            csv_docs = [d for d in documents if d.get('type', '').lower() == 'csv']
            if csv_docs:
                latest_csv = csv_docs[-1]
                if 'operational_insights' in latest_csv:
                    cached_insights = latest_csv.get('operational_insights')
                    if cached_insights and isinstance(cached_insights, dict) and len(cached_insights) > 0:
                        insights = cached_insights
        except Exception:
            pass
    
    # If still no insights, try to compute from CSV file
    if not insights:
        try:
            insights = compute_operational_insights()
        except Exception as compute_error:
            print(f"âŒ Error computing insights: {compute_error}")
            return (
                f"I couldn't compute the required data. Error: {str(compute_error)}. Please ensure a CSV file has been uploaded and processed.",
                [],
                {"strategy": "operational_query_agent", "reason": f"Computation error: {str(compute_error)}"}
            )
    
    if not insights or (isinstance(insights, dict) and len(insights) == 0):
        return (
            "I couldn't find the required data. Please ensure a CSV file has been uploaded and processed. If you just uploaded a file, try refreshing the page or waiting a moment for processing to complete.",
            [],
            {"strategy": "operational_query_agent", "reason": "No data available"}
        )
    
    # Store insights as facts for LLM access (if not already stored)
    # This is idempotent - won't duplicate facts if already stored
    store_operational_insights_as_facts(insights)
    
    # FIRST: Check if we can answer directly from insights (fast path, avoids slow context retrieval)
    question_lower = question.lower()
    evidence_facts = []
    
    # Direct answer for "top 5 employees by absences"
    if "top" in question_lower and ("absence" in question_lower or "absent" in question_lower):
        if 'top_absences' in insights and insights['top_absences']:
            answer_parts = ["Based on available data, here's the list of top 5 employees with highest absences:\n"]
            for i, emp_data in enumerate(insights['top_absences'][:5], 1):
                emp_name = emp_data.get('employee_name', 'Unknown')
                absences = emp_data.get('absences', 0)
                dept = emp_data.get('department', '')
                answer_parts.append(f"{i}. **{emp_name}**: {absences:.0f} absences" + (f" ({dept})" if dept else ""))
            answer = "\n".join(answer_parts)
            answer += "\n\n**Explanation:**\nThese are the employees with the highest number of absences based on the operational insights computed from the uploaded data."
            
            # Create evidence facts
            for emp_data in insights['top_absences'][:5]:
                evidence_facts.append({
                    "subject": emp_data.get('employee_name', 'Unknown'),
                    "predicate": "has_absences",
                    "object": str(emp_data.get('absences', 0)),
                    "source": ["operational_insights"]
                })
            
            return answer, evidence_facts, {
                "strategy": "operational_query_agent",
                "reason": "Retrieved from operational insights",
                "insights": insights
            }
    
    # Direct answer for "best recruitment source" or "recruitment source" queries
    if "recruitment" in question_lower and ("best" in question_lower or "most" in question_lower or "highest" in question_lower or "number" in question_lower or "hires" in question_lower):
        if 'by_recruitment_source' in insights and insights['by_recruitment_source']:
            # Find the recruitment source with highest employee_count
            sources = insights['by_recruitment_source']
            if sources:
                # Sort by employee_count (hires)
                sorted_sources = sorted(sources, key=lambda x: x.get('employee_count', 0), reverse=True)
                top_source = sorted_sources[0]
                source_name = top_source.get('recruitment_source', 'Unknown')
                hire_count = top_source.get('employee_count', 0)
                
                answer_parts = [f"Based on the operational insights data, **{source_name}** has been the most common recruitment source with {hire_count} hires."]
                if len(sorted_sources) > 1:
                    answer_parts.append(f"\n\nOther recruitment sources:")
                    for i, src in enumerate(sorted_sources[1:6], 2):  # Show top 5
                        answer_parts.append(f"{i}. {src.get('recruitment_source', 'Unknown')}: {src.get('employee_count', 0)} hires")
                
                answer = "\n".join(answer_parts)
                answer += "\n\n**Explanation:**\nThis is based on the total number of employees hired from each recruitment source in the dataset."
                
                # Create evidence facts
                for src in sorted_sources[:5]:
                    evidence_facts.append({
                        "subject": src.get('recruitment_source', 'Unknown'),
                        "predicate": "has_hires",
                        "object": str(src.get('employee_count', 0)),
                        "source": ["operational_insights"]
                    })
                
                return answer, evidence_facts, {
                    "strategy": "operational_query_agent",
                    "reason": "Retrieved from operational insights",
                    "insights": insights
                }
    
    # Direct answer for "bottom 5 employees by engagement"
    if "bottom" in question_lower and "engagement" in question_lower:
        if 'bottom_engagement' in insights and insights['bottom_engagement']:
            answer_parts = ["Based on available data, here's the list of bottom 5 employees with lowest engagement:\n"]
            for i, emp_data in enumerate(insights['bottom_engagement'][:5], 1):
                emp_name = emp_data.get('employee_name', 'Unknown')
                engagement = emp_data.get('engagement_score', 0)
                dept = emp_data.get('department', '')
                answer_parts.append(f"{i}. **{emp_name}**: {engagement:.2f} engagement score" + (f" ({dept})" if dept else ""))
            answer = "\n".join(answer_parts)
            answer += "\n\n**Explanation:**\nThese are the employees with the lowest engagement scores based on the operational insights computed from the uploaded data."
            
            # Create evidence facts
            for emp_data in insights['bottom_engagement'][:5]:
                evidence_facts.append({
                    "subject": emp_data.get('employee_name', 'Unknown'),
                    "predicate": "has_engagement_score",
                    "object": str(emp_data.get('engagement_score', 0)),
                    "source": ["operational_insights"]
                })
            
            return answer, evidence_facts, {
                "strategy": "operational_query_agent",
                "reason": "Retrieved from operational insights",
                "insights": insights
            }
            
    # Direct answer for "active vs terminated" or "active and terminated" queries about absences
    if ("absence" in question_lower or "absent" in question_lower) and (
        ("active" in question_lower and "terminated" in question_lower) or
        ("active" in question_lower and "vs" in question_lower) or
        ("compare" in question_lower and ("active" in question_lower or "terminated" in question_lower))
    ):
        if 'by_employment_status' in insights and insights['by_employment_status']:
            # Find active and terminated statuses
            active_status = None
            terminated_status = None
            
            for status_data in insights['by_employment_status']:
                status_str = str(status_data.get('employment_status', '')).lower()
                if 'active' in status_str and 'terminated' not in status_str:
                    active_status = status_data
                elif 'terminated' in status_str or 'term' in status_str:
                    terminated_status = status_data
            
            # If not found by exact match, try to identify by common patterns
            if not active_status or not terminated_status:
                for status_data in insights['by_employment_status']:
                    status_str = str(status_data.get('employment_status', '')).lower()
                    if not active_status and ('active' in status_str or status_str in ['a', 'act']):
                        active_status = status_data
                    if not terminated_status and ('terminated' in status_str or 'term' in status_str or status_str in ['t', 'termd']):
                        terminated_status = status_data
            
            if active_status and terminated_status:
                active_abs = active_status.get('avg_absences', 0) or 0
                terminated_abs = terminated_status.get('avg_absences', 0) or 0
                active_count = active_status.get('employee_count', 0)
                terminated_count = terminated_status.get('employee_count', 0)
                
                difference = terminated_abs - active_abs
                diff_percent = (difference / active_abs * 100) if active_abs > 0 else 0
                
                answer_parts = [
                    "Based on operational insights, here's how absences differ between active and terminated employees:\n"
                ]
                answer_parts.append(f"**Active Employees:**")
                answer_parts.append(f"- Average absences: {active_abs:.2f}")
                answer_parts.append(f"- Employee count: {active_count}")
                answer_parts.append(f"\n**Terminated Employees:**")
                answer_parts.append(f"- Average absences: {terminated_abs:.2f}")
                answer_parts.append(f"- Employee count: {terminated_count}")
                answer_parts.append(f"\n**Difference:**")
                if difference > 0:
                    answer_parts.append(f"Terminated employees have {difference:.2f} more absences on average ({diff_percent:.1f}% higher)")
                elif difference < 0:
                    answer_parts.append(f"Active employees have {abs(difference):.2f} more absences on average ({abs(diff_percent):.1f}% higher)")
                else:
                    answer_parts.append(f"No significant difference in average absences")
                
                answer = "\n".join(answer_parts)
                answer += "\n\n**Explanation:**\nThis comparison is based on the average number of absences for employees in each employment status category."
                
                # Create evidence facts
                evidence_facts.append({
                    "subject": "Active Employees",
                    "predicate": "has_avg_absences",
                    "object": str(active_abs),
                    "source": ["operational_insights"]
                })
                evidence_facts.append({
                    "subject": "Terminated Employees",
                    "predicate": "has_avg_absences",
                    "object": str(terminated_abs),
                    "source": ["operational_insights"]
                })
                
                return answer, evidence_facts, {
                    "strategy": "operational_query_agent",
                    "reason": "Retrieved from operational insights (by_employment_status)",
                    "insights": insights
                }
    
    # If no direct answer found, retrieve context from knowledge graph (slower path)
    try:
        from knowledge import retrieve_context
        
        # Retrieve context with focus on operational insights
        # The retrieve_context function will boost facts from "operational_insights" source
        context = retrieve_context(question, limit=50)
        
        # If we have context, use it to answer the question
        if context and "No directly relevant facts found" not in context:
            # For other queries, return context to be used by LLM
            # The context contains relevant facts from operational_insights
            answer = f"Based on operational insights from the uploaded data:\n\n{context}"
            
            # Create evidence facts from context
            evidence_facts = [{
                "subject": "operational_insights",
                "predicate": "contains_data",
                "object": question,
                "source": ["operational_insights"]
            }]
            
            return answer, evidence_facts, {
                "strategy": "operational_query_agent",
                "reason": "Retrieved from operational insights knowledge base",
                "insights": insights
            }
        else:
            # No relevant facts found, but we have insights - try to answer from insights directly
            return (
                "I couldn't find specific information to answer your question in the operational insights. Please ensure the data has been processed and try rephrasing your question.",
                [],
                {"strategy": "operational_query_agent", "reason": "No matching facts found", "insights": insights}
            )
    
    except Exception as e:
        print(f"âš ï¸  Error retrieving context for operational query: {e}")
        
        # Fallback: return generic message
        return (
            f"I encountered an error processing this operational query: {str(e)}",
            [],
            {"strategy": "operational_query_agent", "reason": f"Error: {str(e)}", "insights": insights}
        )
