{
  "per_query_metrics": [
    {
      "query_id": 1,
      "query": "What is the distribution of performance scores by department?",
      "scenario_type": "operational",
      "k": 2,
      "correct": false,
      "response_time": 0.15,
      "fact_retrieval_canonical": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 6,
        "gold_count": 6,
        "intersection_count": 6,
        "note": "Used evidence facts as gold proxy"
      },
      "fact_retrieval_raw": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 6,
        "gold_count": 6,
        "intersection_count": 6,
        "note": "Used evidence facts as gold proxy"
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 6,
      "gold_facts_count": 6
    },
    {
      "query_id": 2,
      "query": "How do performance scores vary across departments?",
      "scenario_type": "operational",
      "k": 2,
      "correct": true,
      "response_time": 0.15,
      "fact_retrieval_canonical": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 6,
        "gold_count": 6,
        "intersection_count": 6,
        "note": "Used evidence facts as gold proxy"
      },
      "fact_retrieval_raw": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 6,
        "gold_count": 6,
        "intersection_count": 6,
        "note": "Used evidence facts as gold proxy"
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 6,
      "gold_facts_count": 6
    },
    {
      "query_id": 3,
      "query": "Which department has the highest average performance score?",
      "scenario_type": "operational",
      "k": 2,
      "correct": false,
      "response_time": 0.17,
      "fact_retrieval_canonical": {
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "retrieved_count": 6,
        "gold_count": 1,
        "intersection_count": 0
      },
      "fact_retrieval_raw": {
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "retrieved_count": 6,
        "gold_count": 1,
        "intersection_count": 0
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 0.0,
      "hallucination_rate": 1.0,
      "evidence_count": 6,
      "gold_facts_count": 1
    },
    {
      "query_id": 4,
      "query": "Show me performance metrics by department",
      "scenario_type": "operational",
      "k": 2,
      "correct": true,
      "response_time": 0.17,
      "fact_retrieval_canonical": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 6,
        "gold_count": 6,
        "intersection_count": 6,
        "note": "Used evidence facts as gold proxy"
      },
      "fact_retrieval_raw": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 6,
        "gold_count": 6,
        "intersection_count": 6,
        "note": "Used evidence facts as gold proxy"
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 6,
      "gold_facts_count": 6
    },
    {
      "query_id": 5,
      "query": "What is the average special projects count by department?",
      "scenario_type": "operational",
      "k": 2,
      "correct": false,
      "response_time": 0.15,
      "fact_retrieval_canonical": {
        "precision": null,
        "recall": null,
        "f1": null,
        "retrieved_count": 0,
        "gold_count": 0,
        "intersection_count": 0,
        "note": "No evidence retrieved - metrics not applicable"
      },
      "fact_retrieval_raw": {
        "precision": null,
        "recall": null,
        "f1": null,
        "retrieved_count": 0,
        "gold_count": 0,
        "intersection_count": 0,
        "note": "No evidence retrieved - metrics not applicable"
      },
      "traceability_completeness": 0.0,
      "hallucination_resistance": 0.0,
      "hallucination_rate": 1.0,
      "evidence_count": 0,
      "gold_facts_count": 0
    },
    {
      "query_id": 6,
      "query": "How do special projects vary across departments?",
      "scenario_type": "operational",
      "k": 2,
      "correct": false,
      "response_time": 0.15,
      "fact_retrieval_canonical": {
        "precision": null,
        "recall": null,
        "f1": null,
        "retrieved_count": 0,
        "gold_count": 0,
        "intersection_count": 0,
        "note": "No evidence retrieved - metrics not applicable"
      },
      "fact_retrieval_raw": {
        "precision": null,
        "recall": null,
        "f1": null,
        "retrieved_count": 0,
        "gold_count": 0,
        "intersection_count": 0,
        "note": "No evidence retrieved - metrics not applicable"
      },
      "traceability_completeness": 0.0,
      "hallucination_resistance": 0.0,
      "hallucination_rate": 1.0,
      "evidence_count": 0,
      "gold_facts_count": 0
    },
    {
      "query_id": 7,
      "query": "Which department has the highest average special projects count?",
      "scenario_type": "operational",
      "k": 2,
      "correct": false,
      "response_time": 0.66,
      "fact_retrieval_canonical": {
        "precision": null,
        "recall": null,
        "f1": null,
        "retrieved_count": 0,
        "gold_count": 0,
        "intersection_count": 0,
        "note": "No evidence retrieved - metrics not applicable"
      },
      "fact_retrieval_raw": {
        "precision": null,
        "recall": null,
        "f1": null,
        "retrieved_count": 0,
        "gold_count": 0,
        "intersection_count": 0,
        "note": "No evidence retrieved - metrics not applicable"
      },
      "traceability_completeness": 0.0,
      "hallucination_resistance": 0.0,
      "hallucination_rate": 1.0,
      "evidence_count": 0,
      "gold_facts_count": 0
    },
    {
      "query_id": 8,
      "query": "Show me special projects distribution by department",
      "scenario_type": "operational",
      "k": 2,
      "correct": false,
      "response_time": 0.28,
      "fact_retrieval_canonical": {
        "precision": null,
        "recall": null,
        "f1": null,
        "retrieved_count": 0,
        "gold_count": 0,
        "intersection_count": 0,
        "note": "No evidence retrieved - metrics not applicable"
      },
      "fact_retrieval_raw": {
        "precision": null,
        "recall": null,
        "f1": null,
        "retrieved_count": 0,
        "gold_count": 0,
        "intersection_count": 0,
        "note": "No evidence retrieved - metrics not applicable"
      },
      "traceability_completeness": 0.0,
      "hallucination_resistance": 0.0,
      "hallucination_rate": 1.0,
      "evidence_count": 0,
      "gold_facts_count": 0
    },
    {
      "query_id": 9,
      "query": "What is the team-level engagement by manager?",
      "scenario_type": "operational",
      "k": 2,
      "correct": true,
      "response_time": 0.23,
      "fact_retrieval_canonical": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 50,
        "gold_count": 50,
        "intersection_count": 50,
        "note": "Used evidence facts as gold proxy"
      },
      "fact_retrieval_raw": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 50,
        "gold_count": 50,
        "intersection_count": 50,
        "note": "Used evidence facts as gold proxy"
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 50,
      "gold_facts_count": 50
    },
    {
      "query_id": 10,
      "query": "How does engagement vary by manager?",
      "scenario_type": "operational",
      "k": 2,
      "correct": true,
      "response_time": 0.12,
      "fact_retrieval_canonical": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 50,
        "gold_count": 50,
        "intersection_count": 50,
        "note": "Used evidence facts as gold proxy"
      },
      "fact_retrieval_raw": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 50,
        "gold_count": 50,
        "intersection_count": 50,
        "note": "Used evidence facts as gold proxy"
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 50,
      "gold_facts_count": 50
    },
    {
      "query_id": 11,
      "query": "Which manager has the highest team engagement?",
      "scenario_type": "operational",
      "k": 2,
      "correct": true,
      "response_time": 0.08,
      "fact_retrieval_canonical": {
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "retrieved_count": 50,
        "gold_count": 1,
        "intersection_count": 0
      },
      "fact_retrieval_raw": {
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "retrieved_count": 50,
        "gold_count": 1,
        "intersection_count": 0
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 50,
      "gold_facts_count": 1
    },
    {
      "query_id": 12,
      "query": "What is the average salary by department?",
      "scenario_type": "operational",
      "k": 2,
      "correct": true,
      "response_time": 0.15,
      "fact_retrieval_canonical": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 18,
        "gold_count": 18,
        "intersection_count": 18,
        "note": "Used evidence facts as gold proxy"
      },
      "fact_retrieval_raw": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 28,
        "gold_count": 28,
        "intersection_count": 28,
        "note": "Used evidence facts as gold proxy"
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 28,
      "gold_facts_count": 28
    },
    {
      "query_id": 13,
      "query": "How does salary distribution vary across departments?",
      "scenario_type": "operational",
      "k": 2,
      "correct": true,
      "response_time": 0.15,
      "fact_retrieval_canonical": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 18,
        "gold_count": 18,
        "intersection_count": 18,
        "note": "Used evidence facts as gold proxy"
      },
      "fact_retrieval_raw": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 28,
        "gold_count": 28,
        "intersection_count": 28,
        "note": "Used evidence facts as gold proxy"
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 28,
      "gold_facts_count": 28
    },
    {
      "query_id": 14,
      "query": "Which department has the highest average salary?",
      "scenario_type": "operational",
      "k": 2,
      "correct": true,
      "response_time": 0.15,
      "fact_retrieval_canonical": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 18,
        "gold_count": 18,
        "intersection_count": 18,
        "note": "Used evidence facts as gold proxy"
      },
      "fact_retrieval_raw": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 28,
        "gold_count": 28,
        "intersection_count": 28,
        "note": "Used evidence facts as gold proxy"
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 28,
      "gold_facts_count": 28
    },
    {
      "query_id": 15,
      "query": "How does performance vary by recruitment source?",
      "scenario_type": "operational",
      "k": 2,
      "correct": true,
      "response_time": 0.16,
      "fact_retrieval_canonical": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 10,
        "gold_count": 10,
        "intersection_count": 10,
        "note": "Used evidence facts as gold proxy"
      },
      "fact_retrieval_raw": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 10,
        "gold_count": 10,
        "intersection_count": 10,
        "note": "Used evidence facts as gold proxy"
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 10,
      "gold_facts_count": 10
    },
    {
      "query_id": 16,
      "query": "Which recruitment sources have the employees with highest performance score?",
      "scenario_type": "operational",
      "k": 2,
      "correct": false,
      "response_time": 0.16,
      "fact_retrieval_canonical": {
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "retrieved_count": 10,
        "gold_count": 1,
        "intersection_count": 0
      },
      "fact_retrieval_raw": {
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "retrieved_count": 10,
        "gold_count": 1,
        "intersection_count": 0
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 0.0,
      "hallucination_rate": 1.0,
      "evidence_count": 10,
      "gold_facts_count": 1
    },
    {
      "query_id": 17,
      "query": "What is the performance distribution by recruitment source?",
      "scenario_type": "operational",
      "k": 2,
      "correct": false,
      "response_time": 0.17,
      "fact_retrieval_canonical": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 10,
        "gold_count": 10,
        "intersection_count": 10,
        "note": "Used evidence facts as gold proxy"
      },
      "fact_retrieval_raw": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 10,
        "gold_count": 10,
        "intersection_count": 10,
        "note": "Used evidence facts as gold proxy"
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 0.0,
      "hallucination_rate": 1.0,
      "evidence_count": 10,
      "gold_facts_count": 10
    },
    {
      "query_id": 18,
      "query": "Identify employees with high performance, low engagement and many special projects",
      "scenario_type": "operational",
      "k": 2,
      "correct": true,
      "response_time": 2.89,
      "fact_retrieval_canonical": {
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "retrieved_count": 50,
        "gold_count": 1,
        "intersection_count": 0
      },
      "fact_retrieval_raw": {
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "retrieved_count": 50,
        "gold_count": 1,
        "intersection_count": 0
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 95,
      "gold_facts_count": 1
    },
    {
      "query_id": 19,
      "query": "Find employees with high performance, low engagement and low satisfaction",
      "scenario_type": "operational",
      "k": 2,
      "correct": true,
      "response_time": 2.99,
      "fact_retrieval_canonical": {
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "retrieved_count": 50,
        "gold_count": 1,
        "intersection_count": 0
      },
      "fact_retrieval_raw": {
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "retrieved_count": 50,
        "gold_count": 1,
        "intersection_count": 0
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 100,
      "gold_facts_count": 1
    },
    {
      "query_id": 20,
      "query": "Find employees with high performance, low engagement and low satisfaction and many special projects",
      "scenario_type": "strategic",
      "k": 3,
      "correct": true,
      "response_time": 1.26,
      "fact_retrieval_canonical": {
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "retrieved_count": 50,
        "gold_count": 1,
        "intersection_count": 0
      },
      "fact_retrieval_raw": {
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "retrieved_count": 50,
        "gold_count": 1,
        "intersection_count": 0
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 50,
      "gold_facts_count": 1
    },
    {
      "query_id": 21,
      "query": "Find employees with low engagement and low satisfaction and many special projects and many absences",
      "scenario_type": "strategic",
      "k": 3,
      "correct": true,
      "response_time": 1.02,
      "fact_retrieval_canonical": {
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "retrieved_count": 40,
        "gold_count": 1,
        "intersection_count": 0
      },
      "fact_retrieval_raw": {
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "retrieved_count": 40,
        "gold_count": 1,
        "intersection_count": 0
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 40,
      "gold_facts_count": 1
    },
    {
      "query_id": 22,
      "query": "Which departments have high salaries but low performance?",
      "scenario_type": "operational",
      "k": 2,
      "correct": false,
      "response_time": 0.01,
      "fact_retrieval_canonical": {
        "precision": null,
        "recall": null,
        "f1": null,
        "retrieved_count": 0,
        "gold_count": 0,
        "intersection_count": 0,
        "note": "No evidence retrieved - metrics not applicable"
      },
      "fact_retrieval_raw": {
        "precision": null,
        "recall": null,
        "f1": null,
        "retrieved_count": 0,
        "gold_count": 0,
        "intersection_count": 0,
        "note": "No evidence retrieved - metrics not applicable"
      },
      "traceability_completeness": 0.0,
      "hallucination_resistance": 0.0,
      "hallucination_rate": 1.0,
      "evidence_count": 0,
      "gold_facts_count": 0
    },
    {
      "query_id": 23,
      "query": "Analyze the relationship between salary, performance, and department",
      "scenario_type": "strategic",
      "k": 3,
      "correct": true,
      "response_time": 0.0,
      "fact_retrieval_canonical": {
        "precision": null,
        "recall": null,
        "f1": null,
        "retrieved_count": 0,
        "gold_count": 0,
        "intersection_count": 0,
        "note": "No evidence retrieved - metrics not applicable"
      },
      "fact_retrieval_raw": {
        "precision": null,
        "recall": null,
        "f1": null,
        "retrieved_count": 0,
        "gold_count": 0,
        "intersection_count": 0,
        "note": "No evidence retrieved - metrics not applicable"
      },
      "traceability_completeness": 0.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 0,
      "gold_facts_count": 0
    },
    {
      "query_id": 24,
      "query": "Identify departments with low salary and high performance",
      "scenario_type": "operational",
      "k": 2,
      "correct": false,
      "response_time": 0.37,
      "fact_retrieval_canonical": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 9,
        "gold_count": 9,
        "intersection_count": 9,
        "note": "Used evidence facts as gold proxy"
      },
      "fact_retrieval_raw": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 9,
        "gold_count": 9,
        "intersection_count": 9,
        "note": "Used evidence facts as gold proxy"
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 0.0,
      "hallucination_rate": 1.0,
      "evidence_count": 9,
      "gold_facts_count": 9
    },
    {
      "query_id": 25,
      "query": "Retrieve facts related with employee Becker, Scott",
      "scenario_type": "evidence",
      "k": 1,
      "correct": true,
      "response_time": 0.31,
      "fact_retrieval_canonical": {
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "retrieved_count": 50,
        "gold_count": 4,
        "intersection_count": 0
      },
      "fact_retrieval_raw": {
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "retrieved_count": 50,
        "gold_count": 4,
        "intersection_count": 0
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 50,
      "gold_facts_count": 4
    },
    {
      "query_id": 26,
      "query": "What information do we have about employee Becker, Scott?",
      "scenario_type": "operational",
      "k": 2,
      "correct": true,
      "response_time": 0.32,
      "fact_retrieval_canonical": {
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "retrieved_count": 50,
        "gold_count": 1,
        "intersection_count": 0
      },
      "fact_retrieval_raw": {
        "precision": 0.0,
        "recall": 0.0,
        "f1": 0.0,
        "retrieved_count": 50,
        "gold_count": 1,
        "intersection_count": 0
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 50,
      "gold_facts_count": 1
    },
    {
      "query_id": 27,
      "query": "Give me facts about the employee with the highest salary",
      "scenario_type": "evidence",
      "k": 1,
      "correct": true,
      "response_time": 0.0,
      "fact_retrieval_canonical": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 32,
        "gold_count": 32,
        "intersection_count": 32,
        "note": "Used evidence facts as gold proxy"
      },
      "fact_retrieval_raw": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 32,
        "gold_count": 32,
        "intersection_count": 32,
        "note": "Used evidence facts as gold proxy"
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 32,
      "gold_facts_count": 32
    },
    {
      "query_id": 28,
      "query": "Retrieve facts about the employee who has the highest salary",
      "scenario_type": "evidence",
      "k": 1,
      "correct": true,
      "response_time": 0.0,
      "fact_retrieval_canonical": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 32,
        "gold_count": 32,
        "intersection_count": 32,
        "note": "Used evidence facts as gold proxy"
      },
      "fact_retrieval_raw": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 32,
        "gold_count": 32,
        "intersection_count": 32,
        "note": "Used evidence facts as gold proxy"
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 32,
      "gold_facts_count": 32
    },
    {
      "query_id": 29,
      "query": "Show me information about the highest paid employee",
      "scenario_type": "evidence",
      "k": 1,
      "correct": true,
      "response_time": 0.06,
      "fact_retrieval_canonical": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 30,
        "gold_count": 30,
        "intersection_count": 30,
        "note": "Used evidence facts as gold proxy"
      },
      "fact_retrieval_raw": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 30,
        "gold_count": 30,
        "intersection_count": 30,
        "note": "Used evidence facts as gold proxy"
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 30,
      "gold_facts_count": 30
    },
    {
      "query_id": 30,
      "query": "Give me facts about the employee with the lowest performance",
      "scenario_type": "evidence",
      "k": 1,
      "correct": true,
      "response_time": 0.0,
      "fact_retrieval_canonical": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 34,
        "gold_count": 34,
        "intersection_count": 34,
        "note": "Used evidence facts as gold proxy"
      },
      "fact_retrieval_raw": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 34,
        "gold_count": 34,
        "intersection_count": 34,
        "note": "Used evidence facts as gold proxy"
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 34,
      "gold_facts_count": 34
    },
    {
      "query_id": 31,
      "query": "Retrieve facts about the employee who has the lowest performance score",
      "scenario_type": "evidence",
      "k": 1,
      "correct": true,
      "response_time": 0.0,
      "fact_retrieval_canonical": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 34,
        "gold_count": 34,
        "intersection_count": 34,
        "note": "Used evidence facts as gold proxy"
      },
      "fact_retrieval_raw": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 34,
        "gold_count": 34,
        "intersection_count": 34,
        "note": "Used evidence facts as gold proxy"
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 34,
      "gold_facts_count": 34
    },
    {
      "query_id": 32,
      "query": "Show me information about the employee with worst performance",
      "scenario_type": "evidence",
      "k": 1,
      "correct": true,
      "response_time": 0.0,
      "fact_retrieval_canonical": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 34,
        "gold_count": 34,
        "intersection_count": 34,
        "note": "Used evidence facts as gold proxy"
      },
      "fact_retrieval_raw": {
        "precision": 1.0,
        "recall": 1.0,
        "f1": 1.0,
        "retrieved_count": 34,
        "gold_count": 34,
        "intersection_count": 34,
        "note": "Used evidence facts as gold proxy"
      },
      "traceability_completeness": 1.0,
      "hallucination_resistance": 1.0,
      "hallucination_rate": 0.0,
      "evidence_count": 34,
      "gold_facts_count": 34
    }
  ],
  "aggregated_metrics": {
    "operational": {
      "f1": {
        "mean": 0.6470588235294118,
        "ci_lower": 0.37705401772827546,
        "ci_upper": 0.9170636293305481,
        "std": 0.492592183071889,
        "n": 17
      },
      "precision": {
        "mean": 0.6470588235294118,
        "ci_lower": 0.37705401772827546,
        "ci_upper": 0.9170636293305481,
        "std": 0.492592183071889,
        "n": 17
      },
      "recall": {
        "mean": 0.6470588235294118,
        "ci_lower": 0.37705401772827546,
        "ci_upper": 0.9170636293305481,
        "std": 0.492592183071889,
        "n": 17
      },
      "traceability_completeness": {
        "mean": 0.7727272727272727,
        "ci_lower": 0.5660534017581498,
        "ci_upper": 0.9794011436963956,
        "std": 0.4289320272288885,
        "n": 22
      },
      "hallucination_resistance": {
        "mean": 0.5909090909090909,
        "ci_lower": 0.34843293304805345,
        "ci_upper": 0.8333852487701284,
        "std": 0.5032362797401965,
        "n": 22
      },
      "latency": {
        "mean": 0.44681818181818184,
        "ci_lower": 0.05308698789311622,
        "ci_upper": 0.8405493757432474,
        "std": 0.8171517686372668,
        "n": 22
      },
      "accuracy": {
        "mean": 0.5454545454545454,
        "ci_lower": 0.2998893948094792,
        "ci_upper": 0.7910196960996116,
        "std": 0.5096471914376255,
        "n": 22
      },
      "n_queries": 22
    },
    "strategic": {
      "f1": {
        "mean": 0.0,
        "ci_lower": 0.0,
        "ci_upper": 0.0,
        "std": 0.0,
        "n": 2
      },
      "precision": {
        "mean": 0.0,
        "ci_lower": 0.0,
        "ci_upper": 0.0,
        "std": 0.0,
        "n": 2
      },
      "recall": {
        "mean": 0.0,
        "ci_lower": 0.0,
        "ci_upper": 0.0,
        "std": 0.0,
        "n": 2
      },
      "traceability_completeness": {
        "mean": 0.6666666666666666,
        "ci_lower": -0.3933333333333334,
        "ci_upper": 1.7266666666666666,
        "std": 0.5773502691896257,
        "n": 3
      },
      "hallucination_resistance": {
        "mean": 1.0,
        "ci_lower": 1.0,
        "ci_upper": 1.0,
        "std": 0.0,
        "n": 3
      },
      "latency": {
        "mean": 0.76,
        "ci_lower": -0.4683200234466589,
        "ci_upper": 1.988320023446659,
        "std": 0.6690291473471093,
        "n": 3
      },
      "accuracy": {
        "mean": 1.0,
        "ci_lower": 1.0,
        "ci_upper": 1.0,
        "std": 0.0,
        "n": 3
      },
      "n_queries": 3
    },
    "evidence": {
      "f1": {
        "mean": 0.8571428571428571,
        "ci_lower": 0.46,
        "ci_upper": 1.2542857142857142,
        "std": 0.3779644730092272,
        "n": 7
      },
      "precision": {
        "mean": 0.8571428571428571,
        "ci_lower": 0.46,
        "ci_upper": 1.2542857142857142,
        "std": 0.3779644730092272,
        "n": 7
      },
      "recall": {
        "mean": 0.8571428571428571,
        "ci_lower": 0.46,
        "ci_upper": 1.2542857142857142,
        "std": 0.3779644730092272,
        "n": 7
      },
      "traceability_completeness": {
        "mean": 1.0,
        "ci_lower": 1.0,
        "ci_upper": 1.0,
        "std": 0.0,
        "n": 7
      },
      "hallucination_resistance": {
        "mean": 1.0,
        "ci_lower": 1.0,
        "ci_upper": 1.0,
        "std": 0.0,
        "n": 7
      },
      "latency": {
        "mean": 0.05285714285714286,
        "ci_lower": -0.06858028539128405,
        "ci_upper": 0.17429457110556976,
        "std": 0.1155731061153193,
        "n": 7
      },
      "accuracy": {
        "mean": 1.0,
        "ci_lower": 1.0,
        "ci_upper": 1.0,
        "std": 0.0,
        "n": 7
      },
      "n_queries": 7
    }
  },
  "summary": {
    "total_queries": 32,
    "grouping": "scenario_type",
    "groups": [
      "operational",
      "strategic",
      "evidence"
    ]
  }
}